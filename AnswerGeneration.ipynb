{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "855177f2-f73c-4adb-8e85-a3584688bbab",
   "metadata": {},
   "source": [
    "### In this notebook, I will try to generate the response using the Mistral 7B model as my chatbot and one of the vector database created in the chunking and vectorisation notebook. Will also try to evaluate each strategy and choose the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "208b6056-6c5d-4277-be9c-cadbbd1be267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\shri\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "import pickle\n",
    "import json\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "from nltk.translate.meteor_score import single_meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa3b04d-ae72-4305-af4e-d413ea5b79f4",
   "metadata": {},
   "source": [
    "Loading the Mistral 7B model to use as our chatboat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a432d284-6ed6-4f32-862b-f36a40857840",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from C:\\Users\\shri\\Data_Science\\Text Mining\\mistral-7b-instruct-v0.1.Q2_K.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 10\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q2_K:   65 tensors\n",
      "llama_model_loader: - type q3_K:  160 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q2_K - Medium\n",
      "print_info: file size   = 2.87 GiB (3.41 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: printing all EOG tokens:\n",
      "load:   - 2 ('</s>')\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q2_K) (and 226 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "load_tensors:   CPU_REPACK model buffer size =   210.00 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =  2939.57 MiB\n",
      "repack: repack tensor blk.0.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.0.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.1.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.1.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.2.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.2.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.3.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.3.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.4.attn_q.weight with q2_K_8x8\n",
      ".repack: repack tensor blk.4.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.5.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.5.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.6.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.6.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.7.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.7.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.8.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.8.attn_k.weight with q2_K_8x8\n",
      ".repack: repack tensor blk.9.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.9.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.10.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.10.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.11.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.11.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.12.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.12.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.13.attn_q.weight with q2_K_8x8\n",
      ".repack: repack tensor blk.13.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.14.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.14.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.15.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.15.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.16.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.16.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.17.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.17.attn_k.weight with q2_K_8x8\n",
      ".repack: repack tensor blk.18.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.18.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.19.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.19.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.20.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.20.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.21.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.21.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.22.attn_q.weight with q2_K_8x8\n",
      ".repack: repack tensor blk.22.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.23.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.23.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.24.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.24.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.25.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.25.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.26.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.26.attn_k.weight with q2_K_8x8\n",
      ".repack: repack tensor blk.27.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.27.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.28.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.28.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.29.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.29.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.30.attn_q.weight with q2_K_8x8\n",
      "repack: repack tensor blk.30.attn_k.weight with q2_K_8x8\n",
      "repack: repack tensor blk.31.attn_q.weight with q2_K_8x8\n",
      ".repack: repack tensor blk.31.attn_k.weight with q2_K_8x8\n",
      "..........................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 8192\n",
      "llama_context: n_ctx_per_seq = 8192\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (8192) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.12 MiB\n",
      "create_memory: n_ctx = 8192 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified: layer  28: dev = CPU\n",
      "llama_kv_cache_unified: layer  29: dev = CPU\n",
      "llama_kv_cache_unified: layer  30: dev = CPU\n",
      "llama_kv_cache_unified: layer  31: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =  1024.00 MiB\n",
      "llama_kv_cache_unified: size = 1024.00 MiB (  8192 cells,  32 layers,  1/1 seqs), K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 2328\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:        CPU compute buffer size =   564.01 MiB\n",
      "llama_context: graph nodes  = 1126\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '10', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path = r'C:\\Users\\shri\\Data_Science\\Text Mining\\mistral-7b-instruct-v0.1.Q2_K.gguf'\n",
    "llm = Llama(\n",
    "    model_path=path,\n",
    "    n_ctx=8192,\n",
    "    verbose=True \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d028f8bd-a830-47d5-bd57-766b2158e948",
   "metadata": {},
   "source": [
    "Loading the first faiss index and all_chunks we created in chunking and vectorisation notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "910e963d-aff3-4860-baf1-97c27dba21e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.read_index(\"chunk_index.faiss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3882bad7-fa08-4f9c-a64d-2d3cd57745e7",
   "metadata": {},
   "source": [
    "Loading the chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0850b7d5-32e0-4170-b447-6f01c374a980",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"semantic_chunks.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "all_chunks = data[\"chunks\"]\n",
    "chunk_index_map = data[\"index_map\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6b8d54-cdb6-483d-816e-047d7f092811",
   "metadata": {},
   "source": [
    "Loading the Embedding model for the query vectorisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ccf8cfd-df31-46ec-a2d0-70e273884dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b702c06-5636-4e54-8574-838046942ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## to retrieve the top 5 most similar chunks to the querry\n",
    "def retrieve_relevant_chunks(query,model,index, chunks, top_k=5):\n",
    "    query_embedding = model.encode([query]).astype(\"float32\")\n",
    "    D, I = index.search(query_embedding, top_k)\n",
    "    return [chunks[i] for i in I[0]],D[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab0a42ea-8e33-4875-818e-2014a8f9c91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## to genrate prompt with questions and retrieved contex \n",
    "def build_prompt(query, retrieved_chunks):\n",
    "    context = \"\\n\".join(retrieved_chunks)\n",
    "\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "Answer the following question using the provided context.\n",
    "\n",
    "Context:\n",
    "{context.strip()}\n",
    "\n",
    "Question:\n",
    "{query.strip()}\n",
    "\n",
    "### Response:\"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28097210-32d7-43fc-8246-2ea85fd265fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   13709.23 ms\n",
      "llama_perf_context_print: prompt eval time =   13707.89 ms /   138 tokens (   99.33 ms per token,    10.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =   32825.70 ms /   156 runs   (  210.42 ms per token,     4.75 tokens per second)\n",
      "llama_perf_context_print:       total time =   46637.13 ms /   294 tokens\n",
      "llama_perf_context_print:    graphs reused =        150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " \n",
      "\n",
      "Reverse engineering studies can help in auditing algorithms on online platforms by allowing researchers to understand how the algorithms work and identify any potential biases or vulnerabilities. By examining the code and data used by the algorithms, researchers can identify patterns and inconsistencies that could be exploited or used to manipulate the results. This information can then be used to develop more accurate and reliable algorithms, as well as to identify potential risks and mitigation strategies. Reverse engineering studies can also provide valuable insights into the algorithms' decision-making processes, which can help to ensure that they are aligned with ethical and legal standards. Overall, reverse engineering studies can play an important role in ensuring the transparency, accountability, and fairness of algorithms used in online platforms.\n"
     ]
    }
   ],
   "source": [
    "query = \"How do reverse engineering studies help in auditing algorithms on online platforms\"\n",
    "\n",
    "top_chunks,i = retrieve_relevant_chunks(query,model = model,index = index, chunks = all_chunks, top_k=5)\n",
    "\n",
    "# prompt\n",
    "prompt = build_prompt(query, top_chunks)\n",
    "\n",
    "token_ids = llm.tokenize(prompt.encode(\"utf-8\"))\n",
    "print(\"Token count:\", len(token_ids))\n",
    "\n",
    "# \n",
    "response = llm(prompt, max_tokens=256, temperature=0.7, stop=[\"###\"])\n",
    "print(\"Answer:\\n\", response[\"choices\"][0][\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f8ca0d-f10f-44ad-882a-0ab64488a038",
   "metadata": {},
   "source": [
    "Remove the below cell when sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93b639a4-cbe8-419f-94aa-1c418f91fc9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The database covers 27 EU countries, Iceland, Lichtenstein, Norway, Switzerland and the United Kingdom.',\n",
       " 'emergency phone numbers',\n",
       " 'By 2040, the quantum sector is expected to create thousands of highly skilled jobs across the EU and exceed a global value of €155 billion.',\n",
       " 'treatments that are covered and their costs',\n",
       " 'Specific actions have been identified to meet the strategy’s objectives, such as:']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cfe201e-b424-4b9d-a83f-ff59aeb1b2a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5281775 , 0.59934396, 0.6755141 , 0.7139563 , 0.78965354],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529c75b4-6cc8-4360-a948-a9b7290ce9ad",
   "metadata": {},
   "source": [
    "It seems that the answer generated by the LLM is not coming from the context. Because the retrived chunks does not matches with the question or answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe38a23-073c-4f69-9b29-c614cdcfc235",
   "metadata": {},
   "source": [
    "Let's try changing the prompt a little bit and see if the response changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3e199ac-8f8c-44dc-821a-7d272c6e8971",
   "metadata": {},
   "outputs": [],
   "source": [
    "## A little different prompt\n",
    "def build_prompt_2(query, retrieved_chunks):\n",
    "    context = \"\\n\".join(retrieved_chunks)\n",
    "\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "Answer the following question only using the provided context. Do not use any external knowledge or assumptions. \n",
    "If the answer is not in the context, respond with \"The answer is not found in the provided context.\"\n",
    "\n",
    "Context:\n",
    "{context.strip()}\n",
    "\n",
    "Question:\n",
    "{query.strip()}\n",
    "\n",
    "### Response:\"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96ae1ed-26c9-48f8-8d3d-325733fdab82",
   "metadata": {},
   "source": [
    "Again passing the same querry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afc7c04f-7a9a-4956-9c73-c25bf93e62ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 11 prefix-match hit, remaining 161 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   13709.23 ms\n",
      "llama_perf_context_print: prompt eval time =   11884.62 ms /   161 tokens (   73.82 ms per token,    13.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2545.64 ms /    12 runs   (  212.14 ms per token,     4.71 tokens per second)\n",
      "llama_perf_context_print:       total time =   14435.46 ms /   173 tokens\n",
      "llama_perf_context_print:    graphs reused =         11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " \n",
      "\n",
      "The answer is not found in the provided context.\n"
     ]
    }
   ],
   "source": [
    "# Build prompt\n",
    "prompt = build_prompt_2(query, top_chunks)\n",
    "\n",
    "# Optional: check token count\n",
    "token_ids = llm.tokenize(prompt.encode(\"utf-8\"))\n",
    "print(\"Token count:\", len(token_ids))\n",
    "\n",
    "# Run inference\n",
    "response = llm(prompt, max_tokens=256, temperature=0.7, stop=[\"###\"])\n",
    "print(\"Answer:\\n\", response[\"choices\"][0][\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8899dfd-ddda-4fbf-ac89-e38cf35207e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'### Instruction:\\nAnswer the following question only using the provided context. Do not use any external knowledge or assumptions. \\nIf the answer is not in the context, respond with \"The answer is not found in the provided context.\"\\n\\nContext:\\nThe database covers 27 EU countries, Iceland, Lichtenstein, Norway, Switzerland and the United Kingdom.\\nemergency phone numbers\\nBy 2040, the quantum sector is expected to create thousands of highly skilled jobs across the EU and exceed a global value of €155 billion.\\ntreatments that are covered and their costs\\nSpecific actions have been identified to meet the strategy’s objectives, such as:\\n\\nQuestion:\\nHow do reverse engineering studies help in auditing algorithms on online platforms\\n\\n### Response:'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d23b59-3826-40ab-a376-e92a1e86e5b3",
   "metadata": {},
   "source": [
    "Let's try the cosine index and see if the matching improves "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9645a95-1370-48a6-beea-f446e578aa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_cosine = faiss.read_index(\"chunk_index_cosine.faiss\")\n",
    "query = \"When is the deadline for submitting applications to join the Platform on Sustainable Finance?\"\n",
    "query_embedding = model.encode([query])  \n",
    "query_embedding = query_embedding / np.linalg.norm(query_embedding, axis=1, keepdims=True)\n",
    "query_embedding = query_embedding.astype(\"float32\") \n",
    "D, I = index_cosine.search(query_embedding, k=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fdb1ef99-8e29-4d36-a30a-332a3a4d4d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. cos=0.6602 | The new platform will be composed of up to 35 members, of which up to 28 will be selected through today's call for applications....\n",
      "2. cos=0.8008 | By sharing the experiences and successes of this initiative, the CEB and its partners hope to create a ripple effect, encouraging more innovative and effective approaches to migran...\n",
      "3. cos=0.8998 | TheRoadmapaims to developclear standards and reliable certification for these nature-positive actionsto make nature credits effective and trustworthy, while avoiding administrative...\n",
      "4. cos=0.9088 | InApril 2025, the Commission proposed to amend the EGF regulation to support workers at risk of imminent job loss, allowing earlier intervention by swiftly mobilising support befor...\n",
      "5. cos=0.9249 | The European Commission has today launched acall for applicationsfor members of the thirdPlatform on Sustainable Finance....\n"
     ]
    }
   ],
   "source": [
    "index = faiss.read_index(\"chunk_index_cosine.faiss\")\n",
    "with open(\"filtered_chunks.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    all_chunks = [ln.strip() for ln in f if ln.strip()]\n",
    "\n",
    "# Build the query (E5 example)\n",
    "query = \"When is the deadline for submitting applications to join the Platform on Sustainable Finance?\"\n",
    "q = model.encode([f\"query: {query}\"], normalize_embeddings=True).astype(\"float32\")\n",
    "\n",
    "# Search\n",
    "k = 5\n",
    "D, I = index.search(q, k)\n",
    "\n",
    "# Show\n",
    "for rank, (idx, score) in enumerate(zip(I[0], D[0]), 1):\n",
    "    print(f\"{rank}. cos={score:.4f} | {all_chunks_2[idx][:180]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8fafd4e7-9742-4976-9e1a-0fffcb7e9247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.6602 | Chunk: The new platform will be composed of up to 35 members, of which up to 28 will be selected through today's call for applications.\n",
      "Score: 0.8008 | Chunk: By sharing the experiences and successes of this initiative, the CEB and its partners hope to create a ripple effect, encouraging more innovative and effective approaches to migrant integration across Europe.\n",
      "Score: 0.8998 | Chunk: TheRoadmapaims to developclear standards and reliable certification for these nature-positive actionsto make nature credits effective and trustworthy, while avoiding administrative burden when joining such a scheme.\n",
      "Score: 0.9088 | Chunk: InApril 2025, the Commission proposed to amend the EGF regulation to support workers at risk of imminent job loss, allowing earlier intervention by swiftly mobilising support before job losses occur.\n",
      "Score: 0.9249 | Chunk: The European Commission has today launched acall for applicationsfor members of the thirdPlatform on Sustainable Finance.\n"
     ]
    }
   ],
   "source": [
    "for idx, score in zip(I[0], D[0]):\n",
    "    print(f\"Score: {score:.4f} | Chunk: {all_chunks_2[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aab714f-2fb9-4460-8733-d3f8424322f5",
   "metadata": {},
   "source": [
    "Even after the clear prompt the LLM still generated the answer from its own base knowledge. This could be a limitation of the LLM. Also the chunking method we have used in this case seems not very good because the retrieved chunks lacks local contex which results in incomplete chunks. We should think of a better chunking strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7409546-afd7-470e-99fc-6c515a65d05b",
   "metadata": {},
   "source": [
    "Explaination of what I did so far:-\n",
    "1. Break the scrapped text into chunks of maximum length 300.\n",
    "2. Converted these text to a vector embedding of size 384 using \"all-MiniLM-L6-v2\".\n",
    "3. Created an faiss index usinig euclidean distance.\n",
    "4. The querry is converted to embedding then searched for top 5 similar embeddings from the index.\n",
    "5. Therse 5 similar chunks are then joined into one and then passed to our LLMA model for answer generation with a prompt having Question and context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f32392-9b39-4783-ba64-6b0fce233561",
   "metadata": {},
   "source": [
    "#### Now for the evaluation part, I will create a set of 200 QA using chatgpt and then measure the answering ability considering the answer of the chatgpt as base. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff53f74-94e1-4427-93cb-896b28a37d45",
   "metadata": {},
   "source": [
    "I have generated 297 high quality QA pair using chatgpt. Now I will take the answers of the chatgpt as the baseline and compare with the generated answers for all the questions using our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2024c909-1b65-4536-8586-8651d1a70daf",
   "metadata": {},
   "source": [
    "Loading the QA file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3c2400e4-a4d1-409a-a881-f624356d494d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total QA pairs found: 297\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"C:\\Users\\shri\\Data_Science\\Text Mining\\QA_Evaluation\\QA_text_mining.txt\"\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "num_pairs = len(data)\n",
    "\n",
    "print(f\"Total QA pairs found: {num_pairs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34459402-c6a9-4a80-b8d1-cd7d0e6256f3",
   "metadata": {},
   "source": [
    "Let's generate the answers of the Questions using our RAG model and evaluate against the chatgpt baseline answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e40a27c1-bf20-426e-b48b-f8524823a4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_qa_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f0a0fb61-ec3e-4a1d-b17e-16ff4c77f65a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/297 [00:00<?, ?it/s]Llama.generate: 21 prefix-match hit, remaining 246 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   66603.67 ms /   246 tokens (  270.75 ms per token,     3.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =   53061.34 ms /   111 runs   (  478.03 ms per token,     2.09 tokens per second)\n",
      "llama_perf_context_print:       total time =  119850.10 ms /   357 tokens\n",
      "  0%|▎                                                                              | 1/297 [01:59<9:51:41, 119.94s/it]Llama.generate: 22 prefix-match hit, remaining 361 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   96265.87 ms /   361 tokens (  266.66 ms per token,     3.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =   51647.64 ms /   121 runs   (  426.84 ms per token,     2.34 tokens per second)\n",
      "llama_perf_context_print:       total time =  148075.30 ms /   482 tokens\n",
      "  1%|▌                                                                             | 2/297 [04:28<11:11:22, 136.55s/it]Llama.generate: 21 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   46232.38 ms /   197 tokens (  234.68 ms per token,     4.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =   92089.71 ms /   199 runs   (  462.76 ms per token,     2.16 tokens per second)\n",
      "llama_perf_context_print:       total time =  138683.87 ms /   396 tokens\n",
      "  1%|▊                                                                             | 3/297 [06:46<11:14:00, 137.55s/it]Llama.generate: 21 prefix-match hit, remaining 316 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   81599.42 ms /   316 tokens (  258.23 ms per token,     3.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =   52779.02 ms /   114 runs   (  462.97 ms per token,     2.16 tokens per second)\n",
      "llama_perf_context_print:       total time =  134535.60 ms /   430 tokens\n",
      "  1%|█                                                                             | 4/297 [09:01<11:06:04, 136.40s/it]Llama.generate: 21 prefix-match hit, remaining 356 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   93141.27 ms /   356 tokens (  261.63 ms per token,     3.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =   70189.79 ms /   149 runs   (  471.07 ms per token,     2.12 tokens per second)\n",
      "llama_perf_context_print:       total time =  163556.96 ms /   505 tokens\n",
      "  2%|█▎                                                                            | 5/297 [11:45<11:51:35, 146.22s/it]Llama.generate: 21 prefix-match hit, remaining 352 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   74210.16 ms /   352 tokens (  210.82 ms per token,     4.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =   67861.56 ms /   148 runs   (  458.52 ms per token,     2.18 tokens per second)\n",
      "llama_perf_context_print:       total time =  142271.57 ms /   500 tokens\n",
      "  2%|█▌                                                                            | 6/297 [14:07<11:42:47, 144.90s/it]Llama.generate: 21 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   41141.26 ms /   200 tokens (  205.71 ms per token,     4.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =   44291.94 ms /    98 runs   (  451.96 ms per token,     2.21 tokens per second)\n",
      "llama_perf_context_print:       total time =   85553.93 ms /   298 tokens\n",
      "  2%|█▊                                                                            | 7/297 [15:33<10:06:42, 125.52s/it]Llama.generate: 21 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   42580.78 ms /   197 tokens (  216.15 ms per token,     4.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =  115248.90 ms /   234 runs   (  492.52 ms per token,     2.03 tokens per second)\n",
      "llama_perf_context_print:       total time =  158247.74 ms /   431 tokens\n",
      "  3%|██                                                                            | 8/297 [18:11<10:54:53, 135.96s/it]Llama.generate: 22 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   44142.65 ms /   207 tokens (  213.25 ms per token,     4.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =   80999.87 ms /   177 runs   (  457.63 ms per token,     2.19 tokens per second)\n",
      "llama_perf_context_print:       total time =  125421.25 ms /   384 tokens\n",
      "  3%|██▎                                                                           | 9/297 [20:16<10:36:55, 132.69s/it]Llama.generate: 21 prefix-match hit, remaining 249 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   52750.71 ms /   249 tokens (  211.85 ms per token,     4.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =  117750.41 ms /   255 runs   (  461.77 ms per token,     2.17 tokens per second)\n",
      "llama_perf_context_print:       total time =  170984.52 ms /   504 tokens\n",
      "  3%|██▌                                                                          | 10/297 [23:07<11:31:22, 144.54s/it]Llama.generate: 21 prefix-match hit, remaining 629 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  146603.34 ms /   629 tokens (  233.07 ms per token,     4.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =  118998.03 ms /   255 runs   (  466.66 ms per token,     2.14 tokens per second)\n",
      "llama_perf_context_print:       total time =  266066.95 ms /   884 tokens\n",
      "  4%|██▊                                                                          | 11/297 [27:34<14:26:21, 181.75s/it]Llama.generate: 21 prefix-match hit, remaining 498 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   90469.93 ms /   498 tokens (  181.67 ms per token,     5.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =   22900.16 ms /   118 runs   (  194.07 ms per token,     5.15 tokens per second)\n",
      "llama_perf_context_print:       total time =  113433.56 ms /   616 tokens\n",
      "  4%|███                                                                          | 12/297 [29:27<12:44:43, 160.99s/it]Llama.generate: 21 prefix-match hit, remaining 261 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   22701.68 ms /   261 tokens (   86.98 ms per token,    11.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =   26956.26 ms /   145 runs   (  185.91 ms per token,     5.38 tokens per second)\n",
      "llama_perf_context_print:       total time =   49737.53 ms /   406 tokens\n",
      "  4%|███▎                                                                         | 13/297 [30:17<10:02:33, 127.30s/it]Llama.generate: 21 prefix-match hit, remaining 255 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   23272.30 ms /   255 tokens (   91.26 ms per token,    10.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =   34568.23 ms /   179 runs   (  193.12 ms per token,     5.18 tokens per second)\n",
      "llama_perf_context_print:       total time =   57945.02 ms /   434 tokens\n",
      "  5%|███▋                                                                          | 14/297 [31:15<8:21:40, 106.36s/it]Llama.generate: 21 prefix-match hit, remaining 218 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   17494.72 ms /   218 tokens (   80.25 ms per token,    12.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =   18166.49 ms /    96 runs   (  189.23 ms per token,     5.28 tokens per second)\n",
      "llama_perf_context_print:       total time =   35711.73 ms /   314 tokens\n",
      "  5%|███▉                                                                           | 15/297 [31:51<6:39:51, 85.08s/it]Llama.generate: 21 prefix-match hit, remaining 237 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   20470.48 ms /   237 tokens (   86.37 ms per token,    11.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =   21818.19 ms /   109 runs   (  200.17 ms per token,     5.00 tokens per second)\n",
      "llama_perf_context_print:       total time =   42349.27 ms /   346 tokens\n",
      "  5%|████▎                                                                          | 16/297 [32:33<5:38:16, 72.23s/it]Llama.generate: 21 prefix-match hit, remaining 238 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   19913.81 ms /   238 tokens (   83.67 ms per token,    11.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =   29509.27 ms /   149 runs   (  198.05 ms per token,     5.05 tokens per second)\n",
      "llama_perf_context_print:       total time =   49509.94 ms /   387 tokens\n",
      "  6%|████▌                                                                          | 17/297 [33:23<5:05:13, 65.41s/it]Llama.generate: 21 prefix-match hit, remaining 269 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   22571.47 ms /   269 tokens (   83.91 ms per token,    11.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =   21366.30 ms /   112 runs   (  190.77 ms per token,     5.24 tokens per second)\n",
      "llama_perf_context_print:       total time =   43996.74 ms /   381 tokens\n",
      "  6%|████▊                                                                          | 18/297 [34:07<4:34:16, 58.98s/it]Llama.generate: 21 prefix-match hit, remaining 300 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   27827.46 ms /   300 tokens (   92.76 ms per token,    10.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =   47243.18 ms /   255 runs   (  185.27 ms per token,     5.40 tokens per second)\n",
      "llama_perf_context_print:       total time =   75251.94 ms /   555 tokens\n",
      "  6%|█████                                                                          | 19/297 [35:22<4:55:58, 63.88s/it]Llama.generate: 21 prefix-match hit, remaining 236 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   20482.62 ms /   236 tokens (   86.79 ms per token,    11.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =   22993.50 ms /   117 runs   (  196.53 ms per token,     5.09 tokens per second)\n",
      "llama_perf_context_print:       total time =   43546.84 ms /   353 tokens\n",
      "  7%|█████▎                                                                         | 20/297 [36:05<4:26:46, 57.78s/it]Llama.generate: 21 prefix-match hit, remaining 837 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   80666.07 ms /   837 tokens (   96.38 ms per token,    10.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =   33834.45 ms /   140 runs   (  241.67 ms per token,     4.14 tokens per second)\n",
      "llama_perf_context_print:       total time =  114599.70 ms /   977 tokens\n",
      "  7%|█████▌                                                                         | 21/297 [38:00<5:44:19, 74.85s/it]Llama.generate: 21 prefix-match hit, remaining 275 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   22104.79 ms /   275 tokens (   80.38 ms per token,    12.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =   43244.57 ms /   185 runs   (  233.75 ms per token,     4.28 tokens per second)\n",
      "llama_perf_context_print:       total time =   65492.38 ms /   460 tokens\n",
      "  7%|█████▊                                                                         | 22/297 [39:06<5:30:14, 72.05s/it]Llama.generate: 21 prefix-match hit, remaining 270 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   21530.37 ms /   270 tokens (   79.74 ms per token,    12.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =   43517.58 ms /   188 runs   (  231.48 ms per token,     4.32 tokens per second)\n",
      "llama_perf_context_print:       total time =   65186.49 ms /   458 tokens\n",
      "  8%|██████                                                                         | 23/297 [40:11<5:19:40, 70.00s/it]Llama.generate: 21 prefix-match hit, remaining 205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   16377.49 ms /   205 tokens (   79.89 ms per token,    12.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =   18092.67 ms /    84 runs   (  215.39 ms per token,     4.64 tokens per second)\n",
      "llama_perf_context_print:       total time =   34516.19 ms /   289 tokens\n",
      "  8%|██████▍                                                                        | 24/297 [40:45<4:30:06, 59.36s/it]Llama.generate: 21 prefix-match hit, remaining 177 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   13808.31 ms /   177 tokens (   78.01 ms per token,    12.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =   51668.70 ms /   229 runs   (  225.63 ms per token,     4.43 tokens per second)\n",
      "llama_perf_context_print:       total time =   65668.25 ms /   406 tokens\n",
      "  8%|██████▋                                                                        | 25/297 [41:51<4:37:44, 61.27s/it]Llama.generate: 21 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   14856.96 ms /   191 tokens (   77.79 ms per token,    12.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =   26531.69 ms /   116 runs   (  228.72 ms per token,     4.37 tokens per second)\n",
      "llama_perf_context_print:       total time =   41461.17 ms /   307 tokens\n",
      "  9%|██████▉                                                                        | 26/297 [42:33<4:09:55, 55.33s/it]Llama.generate: 21 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   39404.80 ms /   458 tokens (   86.04 ms per token,    11.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =   41356.92 ms /   190 runs   (  217.67 ms per token,     4.59 tokens per second)\n",
      "llama_perf_context_print:       total time =   80888.13 ms /   648 tokens\n",
      "  9%|███████▏                                                                       | 27/297 [43:54<4:43:32, 63.01s/it]Llama.generate: 21 prefix-match hit, remaining 295 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   23263.06 ms /   295 tokens (   78.86 ms per token,    12.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =   38469.70 ms /   206 runs   (  186.75 ms per token,     5.35 tokens per second)\n",
      "llama_perf_context_print:       total time =   61865.29 ms /   501 tokens\n",
      "  9%|███████▍                                                                       | 28/297 [44:55<4:41:01, 62.68s/it]Llama.generate: 21 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   15004.53 ms /   203 tokens (   73.91 ms per token,    13.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =   41410.50 ms /   218 runs   (  189.96 ms per token,     5.26 tokens per second)\n",
      "llama_perf_context_print:       total time =   56552.48 ms /   421 tokens\n",
      " 10%|███████▋                                                                       | 29/297 [45:52<4:31:48, 60.85s/it]Llama.generate: 21 prefix-match hit, remaining 226 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   16736.93 ms /   226 tokens (   74.06 ms per token,    13.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =   44229.09 ms /   238 runs   (  185.84 ms per token,     5.38 tokens per second)\n",
      "llama_perf_context_print:       total time =   61129.08 ms /   464 tokens\n",
      " 10%|███████▉                                                                       | 30/297 [46:53<4:31:12, 60.95s/it]Llama.generate: 21 prefix-match hit, remaining 257 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   19218.03 ms /   257 tokens (   74.78 ms per token,    13.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =   27957.90 ms /   150 runs   (  186.39 ms per token,     5.37 tokens per second)\n",
      "llama_perf_context_print:       total time =   47254.72 ms /   407 tokens\n",
      " 10%|████████▏                                                                      | 31/297 [47:40<4:12:01, 56.85s/it]Llama.generate: 22 prefix-match hit, remaining 213 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   17060.57 ms /   213 tokens (   80.10 ms per token,    12.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =   23391.67 ms /   128 runs   (  182.75 ms per token,     5.47 tokens per second)\n",
      "llama_perf_context_print:       total time =   40517.98 ms /   341 tokens\n",
      " 11%|████████▌                                                                      | 32/297 [48:21<3:49:29, 51.96s/it]Llama.generate: 21 prefix-match hit, remaining 240 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   17915.31 ms /   240 tokens (   74.65 ms per token,    13.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =   23393.89 ms /   116 runs   (  201.67 ms per token,     4.96 tokens per second)\n",
      "llama_perf_context_print:       total time =   41368.17 ms /   356 tokens\n",
      " 11%|████████▊                                                                      | 33/297 [49:02<3:34:41, 48.80s/it]Llama.generate: 21 prefix-match hit, remaining 217 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   19916.81 ms /   217 tokens (   91.78 ms per token,    10.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =   42284.82 ms /   198 runs   (  213.56 ms per token,     4.68 tokens per second)\n",
      "llama_perf_context_print:       total time =   62337.29 ms /   415 tokens\n",
      " 11%|█████████                                                                      | 34/297 [50:05<3:51:43, 52.86s/it]Llama.generate: 21 prefix-match hit, remaining 284 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   27836.56 ms /   284 tokens (   98.02 ms per token,    10.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =   40613.96 ms /   201 runs   (  202.06 ms per token,     4.95 tokens per second)\n",
      "llama_perf_context_print:       total time =   68580.95 ms /   485 tokens\n",
      " 12%|█████████▎                                                                     | 35/297 [51:13<4:11:28, 57.59s/it]Llama.generate: 21 prefix-match hit, remaining 271 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   21411.27 ms /   271 tokens (   79.01 ms per token,    12.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =   52370.12 ms /   255 runs   (  205.37 ms per token,     4.87 tokens per second)\n",
      "llama_perf_context_print:       total time =   73977.68 ms /   526 tokens\n",
      " 12%|█████████▌                                                                     | 36/297 [52:27<4:31:59, 62.53s/it]Llama.generate: 21 prefix-match hit, remaining 215 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   18672.97 ms /   215 tokens (   86.85 ms per token,    11.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =   21547.86 ms /   105 runs   (  205.22 ms per token,     4.87 tokens per second)\n",
      "llama_perf_context_print:       total time =   40274.90 ms /   320 tokens\n",
      " 12%|█████████▊                                                                     | 37/297 [53:08<4:02:04, 55.86s/it]Llama.generate: 21 prefix-match hit, remaining 222 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   18047.65 ms /   222 tokens (   81.30 ms per token,    12.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =   23736.55 ms /   127 runs   (  186.90 ms per token,     5.35 tokens per second)\n",
      "llama_perf_context_print:       total time =   41849.78 ms /   349 tokens\n",
      " 13%|██████████                                                                     | 38/297 [53:50<3:43:03, 51.67s/it]Llama.generate: 21 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   17186.35 ms /   189 tokens (   90.93 ms per token,    11.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =   28474.47 ms /   145 runs   (  196.38 ms per token,     5.09 tokens per second)\n",
      "llama_perf_context_print:       total time =   45744.96 ms /   334 tokens\n",
      " 13%|██████████▎                                                                    | 39/297 [54:35<3:34:35, 49.90s/it]Llama.generate: 21 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   15791.44 ms /   194 tokens (   81.40 ms per token,    12.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =   32894.80 ms /   166 runs   (  198.16 ms per token,     5.05 tokens per second)\n",
      "llama_perf_context_print:       total time =   48792.14 ms /   360 tokens\n",
      " 13%|██████████▋                                                                    | 40/297 [55:24<3:32:22, 49.58s/it]Llama.generate: 21 prefix-match hit, remaining 175 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   14090.17 ms /   175 tokens (   80.52 ms per token,    12.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =   50110.27 ms /   255 runs   (  196.51 ms per token,     5.09 tokens per second)\n",
      "llama_perf_context_print:       total time =   64397.35 ms /   430 tokens\n",
      " 14%|██████████▉                                                                    | 41/297 [56:29<3:50:33, 54.04s/it]Llama.generate: 22 prefix-match hit, remaining 160 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   12775.16 ms /   160 tokens (   79.84 ms per token,    12.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =   41929.52 ms /   214 runs   (  195.93 ms per token,     5.10 tokens per second)\n",
      "llama_perf_context_print:       total time =   54856.11 ms /   374 tokens\n",
      " 14%|███████████▏                                                                   | 42/297 [57:24<3:50:44, 54.29s/it]Llama.generate: 21 prefix-match hit, remaining 248 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   20149.90 ms /   248 tokens (   81.25 ms per token,    12.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =   55489.88 ms /   255 runs   (  217.61 ms per token,     4.60 tokens per second)\n",
      "llama_perf_context_print:       total time =   75849.68 ms /   503 tokens\n",
      " 14%|███████████▍                                                                   | 43/297 [58:39<4:17:15, 60.77s/it]Llama.generate: 21 prefix-match hit, remaining 208 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   16631.65 ms /   208 tokens (   79.96 ms per token,    12.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =   55263.95 ms /   243 runs   (  227.42 ms per token,     4.40 tokens per second)\n",
      "llama_perf_context_print:       total time =   72098.65 ms /   451 tokens\n",
      " 15%|███████████▋                                                                   | 44/297 [59:52<4:30:36, 64.18s/it]Llama.generate: 21 prefix-match hit, remaining 480 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   41219.04 ms /   480 tokens (   85.87 ms per token,    11.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =   37121.10 ms /   179 runs   (  207.38 ms per token,     4.82 tokens per second)\n",
      "llama_perf_context_print:       total time =   78463.22 ms /   659 tokens\n",
      " 15%|███████████▋                                                                 | 45/297 [1:01:10<4:47:35, 68.47s/it]Llama.generate: 21 prefix-match hit, remaining 339 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   29960.15 ms /   339 tokens (   88.38 ms per token,    11.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =   49296.92 ms /   242 runs   (  203.71 ms per token,     4.91 tokens per second)\n",
      "llama_perf_context_print:       total time =   79438.97 ms /   581 tokens\n",
      " 15%|███████████▉                                                                 | 46/297 [1:02:30<5:00:15, 71.77s/it]Llama.generate: 21 prefix-match hit, remaining 172 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   13713.88 ms /   172 tokens (   79.73 ms per token,    12.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =   50621.43 ms /   255 runs   (  198.52 ms per token,     5.04 tokens per second)\n",
      "llama_perf_context_print:       total time =   64534.26 ms /   427 tokens\n",
      " 16%|████████████▏                                                                | 47/297 [1:03:34<4:50:02, 69.61s/it]Llama.generate: 21 prefix-match hit, remaining 226 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   18594.33 ms /   226 tokens (   82.28 ms per token,    12.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =   36392.94 ms /   158 runs   (  230.34 ms per token,     4.34 tokens per second)\n",
      "llama_perf_context_print:       total time =   55099.90 ms /   384 tokens\n",
      " 16%|████████████▍                                                                | 48/297 [1:04:29<4:30:51, 65.27s/it]Llama.generate: 21 prefix-match hit, remaining 255 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   20952.48 ms /   255 tokens (   82.17 ms per token,    12.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =   37388.58 ms /   187 runs   (  199.94 ms per token,     5.00 tokens per second)\n",
      "llama_perf_context_print:       total time =   58465.01 ms /   442 tokens\n",
      " 16%|████████████▋                                                                | 49/297 [1:05:28<4:21:22, 63.24s/it]Llama.generate: 22 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   16213.38 ms /   203 tokens (   79.87 ms per token,    12.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =   29777.42 ms /   148 runs   (  201.20 ms per token,     4.97 tokens per second)\n",
      "llama_perf_context_print:       total time =   46079.77 ms /   351 tokens\n",
      " 17%|████████████▉                                                                | 50/297 [1:06:14<3:59:10, 58.10s/it]Llama.generate: 21 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   16783.63 ms /   203 tokens (   82.68 ms per token,    12.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =   50958.35 ms /   255 runs   (  199.84 ms per token,     5.00 tokens per second)\n",
      "llama_perf_context_print:       total time =   67935.48 ms /   458 tokens\n",
      " 17%|█████████████▏                                                               | 51/297 [1:07:22<4:10:20, 61.06s/it]Llama.generate: 21 prefix-match hit, remaining 257 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   20717.64 ms /   257 tokens (   80.61 ms per token,    12.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =   13129.15 ms /    68 runs   (  193.08 ms per token,     5.18 tokens per second)\n",
      "llama_perf_context_print:       total time =   33876.98 ms /   325 tokens\n",
      " 18%|█████████████▍                                                               | 52/297 [1:07:56<3:36:04, 52.92s/it]Llama.generate: 21 prefix-match hit, remaining 252 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   21843.44 ms /   252 tokens (   86.68 ms per token,    11.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =   55235.89 ms /   255 runs   (  216.61 ms per token,     4.62 tokens per second)\n",
      "llama_perf_context_print:       total time =   77299.35 ms /   507 tokens\n",
      " 18%|█████████████▋                                                               | 53/297 [1:09:13<4:04:58, 60.24s/it]Llama.generate: 21 prefix-match hit, remaining 172 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   13772.45 ms /   172 tokens (   80.07 ms per token,    12.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =   11490.50 ms /    48 runs   (  239.39 ms per token,     4.18 tokens per second)\n",
      "llama_perf_context_print:       total time =   25288.13 ms /   220 tokens\n",
      " 18%|██████████████                                                               | 54/297 [1:09:38<3:21:32, 49.76s/it]Llama.generate: 21 prefix-match hit, remaining 244 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   19510.57 ms /   244 tokens (   79.96 ms per token,    12.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =   43148.42 ms /   185 runs   (  233.23 ms per token,     4.29 tokens per second)\n",
      "llama_perf_context_print:       total time =   62801.70 ms /   429 tokens\n",
      " 19%|██████████████▎                                                              | 55/297 [1:10:41<3:36:31, 53.69s/it]Llama.generate: 21 prefix-match hit, remaining 154 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   12062.77 ms /   154 tokens (   78.33 ms per token,    12.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =   29652.74 ms /   129 runs   (  229.87 ms per token,     4.35 tokens per second)\n",
      "llama_perf_context_print:       total time =   41805.59 ms /   283 tokens\n",
      " 19%|██████████████▌                                                              | 56/297 [1:11:23<3:21:21, 50.13s/it]Llama.generate: 21 prefix-match hit, remaining 211 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   16984.04 ms /   211 tokens (   80.49 ms per token,    12.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =   27159.67 ms /   114 runs   (  238.24 ms per token,     4.20 tokens per second)\n",
      "llama_perf_context_print:       total time =   44221.40 ms /   325 tokens\n",
      " 19%|██████████████▊                                                              | 57/297 [1:12:07<3:13:28, 48.37s/it]Llama.generate: 21 prefix-match hit, remaining 292 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   23182.29 ms /   292 tokens (   79.39 ms per token,    12.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8051.47 ms /    35 runs   (  230.04 ms per token,     4.35 tokens per second)\n",
      "llama_perf_context_print:       total time =   31251.51 ms /   327 tokens\n",
      " 20%|███████████████                                                              | 58/297 [1:12:39<2:52:15, 43.25s/it]Llama.generate: 21 prefix-match hit, remaining 210 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   17349.01 ms /   210 tokens (   82.61 ms per token,    12.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7000.86 ms /    29 runs   (  241.41 ms per token,     4.14 tokens per second)\n",
      "llama_perf_context_print:       total time =   24366.51 ms /   239 tokens\n",
      " 20%|███████████████▎                                                             | 59/297 [1:13:03<2:29:07, 37.59s/it]Llama.generate: 21 prefix-match hit, remaining 212 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   17602.36 ms /   212 tokens (   83.03 ms per token,    12.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =   43099.03 ms /   184 runs   (  234.23 ms per token,     4.27 tokens per second)\n",
      "llama_perf_context_print:       total time =   60842.01 ms /   396 tokens\n",
      " 20%|███████████████▌                                                             | 60/297 [1:14:04<2:56:04, 44.58s/it]Llama.generate: 21 prefix-match hit, remaining 213 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   17199.91 ms /   213 tokens (   80.75 ms per token,    12.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =   18996.09 ms /    94 runs   (  202.09 ms per token,     4.95 tokens per second)\n",
      "llama_perf_context_print:       total time =   36246.03 ms /   307 tokens\n",
      " 21%|███████████████▊                                                             | 61/297 [1:14:40<2:45:32, 42.09s/it]Llama.generate: 21 prefix-match hit, remaining 225 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   18107.29 ms /   225 tokens (   80.48 ms per token,    12.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =   35414.81 ms /   173 runs   (  204.71 ms per token,     4.88 tokens per second)\n",
      "llama_perf_context_print:       total time =   53639.98 ms /   398 tokens\n",
      " 21%|████████████████                                                             | 62/297 [1:15:34<2:58:26, 45.56s/it]Llama.generate: 21 prefix-match hit, remaining 294 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   23902.15 ms /   294 tokens (   81.30 ms per token,    12.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7413.01 ms /    37 runs   (  200.35 ms per token,     4.99 tokens per second)\n",
      "llama_perf_context_print:       total time =   31330.07 ms /   331 tokens\n",
      " 21%|████████████████▎                                                            | 63/297 [1:16:05<2:41:04, 41.30s/it]Llama.generate: 21 prefix-match hit, remaining 168 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   15172.79 ms /   168 tokens (   90.31 ms per token,    11.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7492.29 ms /    37 runs   (  202.49 ms per token,     4.94 tokens per second)\n",
      "llama_perf_context_print:       total time =   22681.38 ms /   205 tokens\n",
      " 22%|████████████████▌                                                            | 64/297 [1:16:28<2:18:43, 35.72s/it]Llama.generate: 21 prefix-match hit, remaining 317 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   28147.85 ms /   317 tokens (   88.79 ms per token,    11.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =   38842.28 ms /   207 runs   (  187.64 ms per token,     5.33 tokens per second)\n",
      "llama_perf_context_print:       total time =   67119.35 ms /   524 tokens\n",
      " 22%|████████████████▊                                                            | 65/297 [1:17:35<2:54:35, 45.15s/it]Llama.generate: 21 prefix-match hit, remaining 267 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   20758.91 ms /   267 tokens (   77.75 ms per token,    12.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =   14279.90 ms /    73 runs   (  195.62 ms per token,     5.11 tokens per second)\n",
      "llama_perf_context_print:       total time =   35071.40 ms /   340 tokens\n",
      " 22%|█████████████████                                                            | 66/297 [1:18:10<2:42:14, 42.14s/it]Llama.generate: 21 prefix-match hit, remaining 480 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   45698.61 ms /   480 tokens (   95.21 ms per token,    10.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =   53803.13 ms /   255 runs   (  210.99 ms per token,     4.74 tokens per second)\n",
      "llama_perf_context_print:       total time =   99709.10 ms /   735 tokens\n",
      " 23%|█████████████████▎                                                           | 67/297 [1:19:50<3:47:47, 59.42s/it]Llama.generate: 21 prefix-match hit, remaining 182 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   14376.87 ms /   182 tokens (   78.99 ms per token,    12.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =   46627.23 ms /   229 runs   (  203.61 ms per token,     4.91 tokens per second)\n",
      "llama_perf_context_print:       total time =   61167.25 ms /   411 tokens\n",
      " 23%|█████████████████▋                                                           | 68/297 [1:20:51<3:48:50, 59.96s/it]Llama.generate: 21 prefix-match hit, remaining 230 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   18332.48 ms /   230 tokens (   79.71 ms per token,    12.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =   51481.92 ms /   255 runs   (  201.89 ms per token,     4.95 tokens per second)\n",
      "llama_perf_context_print:       total time =   70008.29 ms /   485 tokens\n",
      " 23%|█████████████████▉                                                           | 69/297 [1:22:01<3:59:19, 62.98s/it]Llama.generate: 21 prefix-match hit, remaining 185 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   14887.56 ms /   185 tokens (   80.47 ms per token,    12.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =   24440.80 ms /   122 runs   (  200.33 ms per token,     4.99 tokens per second)\n",
      "llama_perf_context_print:       total time =   39393.69 ms /   307 tokens\n",
      " 24%|██████████████████▏                                                          | 70/297 [1:22:41<3:31:33, 55.92s/it]Llama.generate: 21 prefix-match hit, remaining 218 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   17483.86 ms /   218 tokens (   80.20 ms per token,    12.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =   16918.68 ms /    86 runs   (  196.73 ms per token,     5.08 tokens per second)\n",
      "llama_perf_context_print:       total time =   34443.17 ms /   304 tokens\n",
      " 24%|██████████████████▍                                                          | 71/297 [1:23:15<3:06:23, 49.48s/it]Llama.generate: 21 prefix-match hit, remaining 286 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   24145.75 ms /   286 tokens (   84.43 ms per token,    11.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =   25883.27 ms /   129 runs   (  200.65 ms per token,     4.98 tokens per second)\n",
      "llama_perf_context_print:       total time =   50098.23 ms /   415 tokens\n",
      " 24%|██████████████████▋                                                          | 72/297 [1:24:05<3:06:18, 49.68s/it]Llama.generate: 21 prefix-match hit, remaining 237 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   19417.94 ms /   237 tokens (   81.93 ms per token,    12.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =   58206.00 ms /   255 runs   (  228.26 ms per token,     4.38 tokens per second)\n",
      "llama_perf_context_print:       total time =   77844.27 ms /   492 tokens\n",
      " 25%|██████████████████▉                                                          | 73/297 [1:25:23<3:37:02, 58.13s/it]Llama.generate: 21 prefix-match hit, remaining 184 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   13779.70 ms /   184 tokens (   74.89 ms per token,    13.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8732.64 ms /    49 runs   (  178.22 ms per token,     5.61 tokens per second)\n",
      "llama_perf_context_print:       total time =   22531.74 ms /   233 tokens\n",
      " 25%|███████████████████▏                                                         | 74/297 [1:25:46<2:56:25, 47.47s/it]Llama.generate: 21 prefix-match hit, remaining 244 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   18796.56 ms /   244 tokens (   77.04 ms per token,    12.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =   28079.92 ms /   151 runs   (  185.96 ms per token,     5.38 tokens per second)\n",
      "llama_perf_context_print:       total time =   46955.38 ms /   395 tokens\n",
      " 25%|███████████████████▍                                                         | 75/297 [1:26:33<2:55:05, 47.32s/it]Llama.generate: 21 prefix-match hit, remaining 240 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   18167.68 ms /   240 tokens (   75.70 ms per token,    13.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =   28034.76 ms /   154 runs   (  182.04 ms per token,     5.49 tokens per second)\n",
      "llama_perf_context_print:       total time =   46285.36 ms /   394 tokens\n",
      " 26%|███████████████████▋                                                         | 76/297 [1:27:19<2:53:11, 47.02s/it]Llama.generate: 21 prefix-match hit, remaining 243 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   18114.42 ms /   243 tokens (   74.54 ms per token,    13.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =   24744.33 ms /   136 runs   (  181.94 ms per token,     5.50 tokens per second)\n",
      "llama_perf_context_print:       total time =   42927.62 ms /   379 tokens\n",
      " 26%|███████████████████▉                                                         | 77/297 [1:28:02<2:47:56, 45.80s/it]Llama.generate: 21 prefix-match hit, remaining 266 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   19946.51 ms /   266 tokens (   74.99 ms per token,    13.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =   38524.20 ms /   210 runs   (  183.45 ms per token,     5.45 tokens per second)\n",
      "llama_perf_context_print:       total time =   58601.79 ms /   476 tokens\n",
      " 26%|████████████████████▏                                                        | 78/297 [1:29:01<3:01:14, 49.65s/it]Llama.generate: 21 prefix-match hit, remaining 267 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   20176.79 ms /   267 tokens (   75.57 ms per token,    13.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =   45446.02 ms /   253 runs   (  179.63 ms per token,     5.57 tokens per second)\n",
      "llama_perf_context_print:       total time =   65799.57 ms /   520 tokens\n",
      " 27%|████████████████████▍                                                        | 79/297 [1:30:06<3:18:01, 54.50s/it]Llama.generate: 22 prefix-match hit, remaining 312 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   25919.63 ms /   312 tokens (   83.08 ms per token,    12.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =   52638.05 ms /   255 runs   (  206.42 ms per token,     4.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   78758.68 ms /   567 tokens\n",
      " 27%|████████████████████▋                                                        | 80/297 [1:31:25<3:43:28, 61.79s/it]Llama.generate: 21 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   15191.49 ms /   191 tokens (   79.54 ms per token,    12.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =   51851.44 ms /   255 runs   (  203.34 ms per token,     4.92 tokens per second)\n",
      "llama_perf_context_print:       total time =   67234.58 ms /   446 tokens\n",
      " 27%|█████████████████████                                                        | 81/297 [1:32:32<3:48:22, 63.44s/it]Llama.generate: 21 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   17982.34 ms /   192 tokens (   93.66 ms per token,    10.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =  109845.27 ms /   231 runs   (  475.52 ms per token,     2.10 tokens per second)\n",
      "llama_perf_context_print:       total time =  128231.94 ms /   423 tokens\n",
      " 28%|█████████████████████▎                                                       | 82/297 [1:34:41<4:57:00, 82.88s/it]Llama.generate: 21 prefix-match hit, remaining 186 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   40901.36 ms /   186 tokens (  219.90 ms per token,     4.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =   64804.21 ms /   138 runs   (  469.60 ms per token,     2.13 tokens per second)\n",
      "llama_perf_context_print:       total time =  105894.54 ms /   324 tokens\n",
      " 28%|█████████████████████▌                                                       | 83/297 [1:36:27<5:20:18, 89.81s/it]Llama.generate: 21 prefix-match hit, remaining 270 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   57526.05 ms /   270 tokens (  213.06 ms per token,     4.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =   95612.87 ms /   205 runs   (  466.40 ms per token,     2.14 tokens per second)\n",
      "llama_perf_context_print:       total time =  153481.53 ms /   475 tokens\n",
      " 28%|█████████████████████▍                                                      | 84/297 [1:39:00<6:26:41, 108.93s/it]Llama.generate: 22 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   58722.13 ms /   193 tokens (  304.26 ms per token,     3.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =   56661.41 ms /   117 runs   (  484.29 ms per token,     2.06 tokens per second)\n",
      "llama_perf_context_print:       total time =  115556.55 ms /   310 tokens\n",
      " 29%|█████████████████████▊                                                      | 85/297 [1:40:56<6:31:59, 110.94s/it]Llama.generate: 21 prefix-match hit, remaining 160 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   48654.57 ms /   160 tokens (  304.09 ms per token,     3.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =   24391.20 ms /    50 runs   (  487.82 ms per token,     2.05 tokens per second)\n",
      "llama_perf_context_print:       total time =   73105.07 ms /   210 tokens\n",
      " 29%|██████████████████████▎                                                      | 86/297 [1:42:09<5:50:17, 99.61s/it]Llama.generate: 21 prefix-match hit, remaining 223 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   66880.56 ms /   223 tokens (  299.91 ms per token,     3.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =  124809.21 ms /   255 runs   (  489.45 ms per token,     2.04 tokens per second)\n",
      "llama_perf_context_print:       total time =  192229.66 ms /   478 tokens\n",
      " 29%|██████████████████████▎                                                     | 87/297 [1:45:21<7:25:57, 127.42s/it]Llama.generate: 21 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   61298.05 ms /   207 tokens (  296.13 ms per token,     3.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =   51326.12 ms /   104 runs   (  493.52 ms per token,     2.03 tokens per second)\n",
      "llama_perf_context_print:       total time =  112768.31 ms /   311 tokens\n",
      " 30%|██████████████████████▌                                                     | 88/297 [1:47:14<7:08:36, 123.05s/it]Llama.generate: 21 prefix-match hit, remaining 247 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   74734.50 ms /   247 tokens (  302.57 ms per token,     3.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =   24531.24 ms /    50 runs   (  490.62 ms per token,     2.04 tokens per second)\n",
      "llama_perf_context_print:       total time =   99324.49 ms /   297 tokens\n",
      " 30%|██████████████████████▊                                                     | 89/297 [1:48:54<6:41:57, 115.95s/it]Llama.generate: 21 prefix-match hit, remaining 267 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   81131.70 ms /   267 tokens (  303.86 ms per token,     3.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =   72924.90 ms /   151 runs   (  482.95 ms per token,     2.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  154296.05 ms /   418 tokens\n",
      " 30%|███████████████████████                                                     | 90/297 [1:51:28<7:19:47, 127.48s/it]Llama.generate: 21 prefix-match hit, remaining 252 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   75090.15 ms /   252 tokens (  297.98 ms per token,     3.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =  114748.77 ms /   234 runs   (  490.38 ms per token,     2.04 tokens per second)\n",
      "llama_perf_context_print:       total time =  190299.09 ms /   486 tokens\n",
      " 31%|███████████████████████▎                                                    | 91/297 [1:54:38<8:22:26, 146.34s/it]Llama.generate: 22 prefix-match hit, remaining 389 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  118785.24 ms /   389 tokens (  305.36 ms per token,     3.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =   66113.59 ms /   135 runs   (  489.73 ms per token,     2.04 tokens per second)\n",
      "llama_perf_context_print:       total time =  185110.08 ms /   524 tokens\n",
      " 31%|███████████████████████▌                                                    | 92/297 [1:57:43<8:59:49, 158.00s/it]Llama.generate: 21 prefix-match hit, remaining 331 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  101272.17 ms /   331 tokens (  305.96 ms per token,     3.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =   15103.77 ms /    31 runs   (  487.22 ms per token,     2.05 tokens per second)\n",
      "llama_perf_context_print:       total time =  116413.45 ms /   362 tokens\n",
      " 31%|███████████████████████▊                                                    | 93/297 [1:59:40<8:14:50, 145.54s/it]Llama.generate: 21 prefix-match hit, remaining 273 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   82874.39 ms /   273 tokens (  303.57 ms per token,     3.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =  125029.82 ms /   255 runs   (  490.31 ms per token,     2.04 tokens per second)\n",
      "llama_perf_context_print:       total time =  208430.64 ms /   528 tokens\n",
      " 32%|████████████████████████                                                    | 94/297 [2:03:08<9:16:20, 164.43s/it]Llama.generate: 21 prefix-match hit, remaining 249 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   75185.41 ms /   249 tokens (  301.95 ms per token,     3.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =   85662.70 ms /   176 runs   (  486.72 ms per token,     2.05 tokens per second)\n",
      "llama_perf_context_print:       total time =  161139.83 ms /   425 tokens\n",
      " 32%|████████████████████████▎                                                   | 95/297 [2:05:50<9:10:20, 163.47s/it]Llama.generate: 21 prefix-match hit, remaining 299 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   44107.57 ms /   299 tokens (  147.52 ms per token,     6.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8716.32 ms /    43 runs   (  202.71 ms per token,     4.93 tokens per second)\n",
      "llama_perf_context_print:       total time =   52842.66 ms /   342 tokens\n",
      " 32%|████████████████████████▌                                                   | 96/297 [2:06:43<7:16:29, 130.29s/it]Llama.generate: 21 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   15989.31 ms /   191 tokens (   83.71 ms per token,    11.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =   51948.50 ms /   255 runs   (  203.72 ms per token,     4.91 tokens per second)\n",
      "llama_perf_context_print:       total time =   68128.45 ms /   446 tokens\n",
      " 33%|████████████████████████▊                                                   | 97/297 [2:07:51<6:12:10, 111.65s/it]Llama.generate: 21 prefix-match hit, remaining 832 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   74989.11 ms /   832 tokens (   90.13 ms per token,    11.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9389.44 ms /    44 runs   (  213.40 ms per token,     4.69 tokens per second)\n",
      "llama_perf_context_print:       total time =   84399.06 ms /   876 tokens\n",
      " 33%|█████████████████████████                                                   | 98/297 [2:09:15<5:43:13, 103.49s/it]Llama.generate: 21 prefix-match hit, remaining 290 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   25085.23 ms /   290 tokens (   86.50 ms per token,    11.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =   51825.53 ms /   254 runs   (  204.04 ms per token,     4.90 tokens per second)\n",
      "llama_perf_context_print:       total time =   77102.83 ms /   544 tokens\n",
      " 33%|█████████████████████████▋                                                   | 99/297 [2:10:32<5:15:25, 95.58s/it]Llama.generate: 21 prefix-match hit, remaining 181 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   14833.77 ms /   181 tokens (   81.95 ms per token,    12.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8422.07 ms /    42 runs   (  200.53 ms per token,     4.99 tokens per second)\n",
      "llama_perf_context_print:       total time =   23272.69 ms /   223 tokens\n",
      " 34%|█████████████████████████▌                                                  | 100/297 [2:10:56<4:02:37, 73.90s/it]Llama.generate: 21 prefix-match hit, remaining 255 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   21567.67 ms /   255 tokens (   84.58 ms per token,    11.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =   35673.36 ms /   176 runs   (  202.69 ms per token,     4.93 tokens per second)\n",
      "llama_perf_context_print:       total time =   57352.49 ms /   431 tokens\n",
      " 34%|█████████████████████████▊                                                  | 101/297 [2:11:53<3:45:13, 68.95s/it]Llama.generate: 21 prefix-match hit, remaining 472 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   39559.17 ms /   472 tokens (   83.81 ms per token,    11.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =   38363.68 ms /   183 runs   (  209.64 ms per token,     4.77 tokens per second)\n",
      "llama_perf_context_print:       total time =   78044.32 ms /   655 tokens\n",
      " 34%|██████████████████████████                                                  | 102/297 [2:13:11<3:52:58, 71.69s/it]Llama.generate: 21 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   16090.78 ms /   198 tokens (   81.27 ms per token,    12.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10542.35 ms /    49 runs   (  215.15 ms per token,     4.65 tokens per second)\n",
      "llama_perf_context_print:       total time =   26654.88 ms /   247 tokens\n",
      " 35%|██████████████████████████▎                                                 | 103/297 [2:13:38<3:08:07, 58.18s/it]Llama.generate: 21 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   18177.38 ms /   207 tokens (   87.81 ms per token,    11.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =   33524.80 ms /   162 runs   (  206.94 ms per token,     4.83 tokens per second)\n",
      "llama_perf_context_print:       total time =   51805.38 ms /   369 tokens\n",
      " 35%|██████████████████████████▌                                                 | 104/297 [2:14:30<3:01:02, 56.28s/it]Llama.generate: 22 prefix-match hit, remaining 220 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   20096.88 ms /   220 tokens (   91.35 ms per token,    10.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9928.30 ms /    49 runs   (  202.62 ms per token,     4.94 tokens per second)\n",
      "llama_perf_context_print:       total time =   30044.28 ms /   269 tokens\n",
      " 35%|██████████████████████████▊                                                 | 105/297 [2:15:00<2:34:56, 48.42s/it]Llama.generate: 21 prefix-match hit, remaining 216 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   18068.42 ms /   216 tokens (   83.65 ms per token,    11.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =   25961.68 ms /   129 runs   (  201.25 ms per token,     4.97 tokens per second)\n",
      "llama_perf_context_print:       total time =   44099.23 ms /   345 tokens\n",
      " 36%|███████████████████████████                                                 | 106/297 [2:15:44<2:30:01, 47.13s/it]Llama.generate: 21 prefix-match hit, remaining 170 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   14089.45 ms /   170 tokens (   82.88 ms per token,    12.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =   31767.38 ms /   159 runs   (  199.79 ms per token,     5.01 tokens per second)\n",
      "llama_perf_context_print:       total time =   45952.04 ms /   329 tokens\n",
      " 36%|███████████████████████████▍                                                | 107/297 [2:16:30<2:28:09, 46.78s/it]Llama.generate: 21 prefix-match hit, remaining 202 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   16805.39 ms /   202 tokens (   83.20 ms per token,    12.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =   15375.40 ms /    78 runs   (  197.12 ms per token,     5.07 tokens per second)\n",
      "llama_perf_context_print:       total time =   32215.00 ms /   280 tokens\n",
      " 36%|███████████████████████████▋                                                | 108/297 [2:17:02<2:13:38, 42.42s/it]Llama.generate: 21 prefix-match hit, remaining 234 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   19613.10 ms /   234 tokens (   83.82 ms per token,    11.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7638.16 ms /    38 runs   (  201.00 ms per token,     4.98 tokens per second)\n",
      "llama_perf_context_print:       total time =   27265.36 ms /   272 tokens\n",
      " 37%|███████████████████████████▉                                                | 109/297 [2:17:29<1:58:42, 37.88s/it]Llama.generate: 21 prefix-match hit, remaining 251 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   21796.08 ms /   251 tokens (   86.84 ms per token,    11.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =   25955.39 ms /   127 runs   (  204.37 ms per token,     4.89 tokens per second)\n",
      "llama_perf_context_print:       total time =   47821.51 ms /   378 tokens\n",
      " 37%|████████████████████████████▏                                               | 110/297 [2:18:17<2:07:23, 40.88s/it]Llama.generate: 21 prefix-match hit, remaining 273 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   23289.01 ms /   273 tokens (   85.31 ms per token,    11.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =   34622.17 ms /   168 runs   (  206.08 ms per token,     4.85 tokens per second)\n",
      "llama_perf_context_print:       total time =   58015.29 ms /   441 tokens\n",
      " 37%|████████████████████████████▍                                               | 111/297 [2:19:15<2:22:40, 46.02s/it]Llama.generate: 21 prefix-match hit, remaining 149 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   12321.44 ms /   149 tokens (   82.69 ms per token,    12.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =   34718.35 ms /   171 runs   (  203.03 ms per token,     4.93 tokens per second)\n",
      "llama_perf_context_print:       total time =   47142.55 ms /   320 tokens\n",
      " 38%|████████████████████████████▋                                               | 112/297 [2:20:02<2:22:58, 46.37s/it]Llama.generate: 21 prefix-match hit, remaining 314 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   26567.88 ms /   314 tokens (   84.61 ms per token,    11.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =   31420.47 ms /   150 runs   (  209.47 ms per token,     4.77 tokens per second)\n",
      "llama_perf_context_print:       total time =   58086.38 ms /   464 tokens\n",
      " 38%|████████████████████████████▉                                               | 113/297 [2:21:00<2:33:00, 49.89s/it]Llama.generate: 21 prefix-match hit, remaining 255 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   21571.78 ms /   255 tokens (   84.60 ms per token,    11.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10860.38 ms /    54 runs   (  201.12 ms per token,     4.97 tokens per second)\n",
      "llama_perf_context_print:       total time =   32456.69 ms /   309 tokens\n",
      " 38%|█████████████████████████████▏                                              | 114/297 [2:21:33<2:16:14, 44.67s/it]Llama.generate: 21 prefix-match hit, remaining 299 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   25703.41 ms /   299 tokens (   85.96 ms per token,    11.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7538.54 ms /    37 runs   (  203.74 ms per token,     4.91 tokens per second)\n",
      "llama_perf_context_print:       total time =   33257.38 ms /   336 tokens\n",
      " 39%|█████████████████████████████▍                                              | 115/297 [2:22:06<2:05:08, 41.26s/it]Llama.generate: 21 prefix-match hit, remaining 178 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   15234.28 ms /   178 tokens (   85.59 ms per token,    11.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =   50730.43 ms /   247 runs   (  205.39 ms per token,     4.87 tokens per second)\n",
      "llama_perf_context_print:       total time =   66148.56 ms /   425 tokens\n",
      " 39%|█████████████████████████████▋                                              | 116/297 [2:23:12<2:27:00, 48.73s/it]Llama.generate: 21 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   17858.09 ms /   201 tokens (   88.85 ms per token,    11.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =   43309.96 ms /   205 runs   (  211.27 ms per token,     4.73 tokens per second)\n",
      "llama_perf_context_print:       total time =   61309.99 ms /   406 tokens\n",
      " 39%|█████████████████████████████▉                                              | 117/297 [2:24:14<2:37:32, 52.52s/it]Llama.generate: 21 prefix-match hit, remaining 340 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   28956.63 ms /   340 tokens (   85.17 ms per token,    11.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =   53751.66 ms /   255 runs   (  210.79 ms per token,     4.74 tokens per second)\n",
      "llama_perf_context_print:       total time =   82899.74 ms /   595 tokens\n",
      " 40%|██████████████████████████████▏                                             | 118/297 [2:25:37<3:03:53, 61.64s/it]Llama.generate: 21 prefix-match hit, remaining 202 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   16642.99 ms /   202 tokens (   82.39 ms per token,    12.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6446.15 ms /    31 runs   (  207.94 ms per token,     4.81 tokens per second)\n",
      "llama_perf_context_print:       total time =   23103.53 ms /   233 tokens\n",
      " 40%|██████████████████████████████▍                                             | 119/297 [2:26:00<2:28:35, 50.09s/it]Llama.generate: 21 prefix-match hit, remaining 490 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   42641.08 ms /   490 tokens (   87.02 ms per token,    11.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8002.86 ms /    38 runs   (  210.60 ms per token,     4.75 tokens per second)\n",
      "llama_perf_context_print:       total time =   50660.57 ms /   528 tokens\n",
      " 40%|██████████████████████████████▋                                             | 120/297 [2:26:51<2:28:17, 50.27s/it]Llama.generate: 21 prefix-match hit, remaining 255 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   22678.59 ms /   255 tokens (   88.94 ms per token,    11.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =   33813.19 ms /   162 runs   (  208.72 ms per token,     4.79 tokens per second)\n",
      "llama_perf_context_print:       total time =   56596.91 ms /   417 tokens\n",
      " 41%|██████████████████████████████▉                                             | 121/297 [2:27:47<2:33:03, 52.18s/it]Llama.generate: 21 prefix-match hit, remaining 221 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   18351.16 ms /   221 tokens (   83.04 ms per token,    12.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8628.02 ms /    41 runs   (  210.44 ms per token,     4.75 tokens per second)\n",
      "llama_perf_context_print:       total time =   26999.36 ms /   262 tokens\n",
      " 41%|███████████████████████████████▏                                            | 122/297 [2:28:14<2:10:11, 44.63s/it]Llama.generate: 22 prefix-match hit, remaining 184 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   15477.56 ms /   184 tokens (   84.12 ms per token,    11.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =   20068.96 ms /    98 runs   (  204.79 ms per token,     4.88 tokens per second)\n",
      "llama_perf_context_print:       total time =   35597.76 ms /   282 tokens\n",
      " 41%|███████████████████████████████▍                                            | 123/297 [2:28:50<2:01:36, 41.93s/it]Llama.generate: 22 prefix-match hit, remaining 174 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   14614.73 ms /   174 tokens (   83.99 ms per token,    11.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =   31287.22 ms /   153 runs   (  204.49 ms per token,     4.89 tokens per second)\n",
      "llama_perf_context_print:       total time =   45996.94 ms /   327 tokens\n",
      " 42%|███████████████████████████████▋                                            | 124/297 [2:29:36<2:04:26, 43.16s/it]Llama.generate: 21 prefix-match hit, remaining 141 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   11614.37 ms /   141 tokens (   82.37 ms per token,    12.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =   20026.90 ms /    97 runs   (  206.46 ms per token,     4.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   31692.12 ms /   238 tokens\n",
      " 42%|███████████████████████████████▉                                            | 125/297 [2:30:08<1:53:53, 39.73s/it]Llama.generate: 21 prefix-match hit, remaining 186 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   15512.11 ms /   186 tokens (   83.40 ms per token,    11.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =   46872.84 ms /   223 runs   (  210.19 ms per token,     4.76 tokens per second)\n",
      "llama_perf_context_print:       total time =   62543.73 ms /   409 tokens\n",
      " 42%|████████████████████████████████▏                                           | 126/297 [2:31:10<2:12:45, 46.58s/it]Llama.generate: 21 prefix-match hit, remaining 298 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   26089.87 ms /   298 tokens (   87.55 ms per token,    11.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =   31028.49 ms /   151 runs   (  205.49 ms per token,     4.87 tokens per second)\n",
      "llama_perf_context_print:       total time =   57207.09 ms /   449 tokens\n",
      " 43%|████████████████████████████████▍                                           | 127/297 [2:32:07<2:21:02, 49.78s/it]Llama.generate: 21 prefix-match hit, remaining 170 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   14301.19 ms /   170 tokens (   84.12 ms per token,    11.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =   31394.18 ms /   153 runs   (  205.19 ms per token,     4.87 tokens per second)\n",
      "llama_perf_context_print:       total time =   45790.83 ms /   323 tokens\n",
      " 43%|████████████████████████████████▊                                           | 128/297 [2:32:53<2:16:51, 48.59s/it]Llama.generate: 21 prefix-match hit, remaining 185 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   15362.72 ms /   185 tokens (   83.04 ms per token,    12.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =   23035.86 ms /   113 runs   (  203.86 ms per token,     4.91 tokens per second)\n",
      "llama_perf_context_print:       total time =   38462.31 ms /   298 tokens\n",
      " 43%|█████████████████████████████████                                           | 129/297 [2:33:32<2:07:34, 45.56s/it]Llama.generate: 21 prefix-match hit, remaining 205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   17222.92 ms /   205 tokens (   84.01 ms per token,    11.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =   37564.57 ms /   183 runs   (  205.27 ms per token,     4.87 tokens per second)\n",
      "llama_perf_context_print:       total time =   54911.77 ms /   388 tokens\n",
      " 44%|█████████████████████████████████▎                                          | 130/297 [2:34:27<2:14:38, 48.37s/it]Llama.generate: 21 prefix-match hit, remaining 232 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   19811.24 ms /   232 tokens (   85.39 ms per token,    11.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =   37401.82 ms /   181 runs   (  206.64 ms per token,     4.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   57335.22 ms /   413 tokens\n",
      " 44%|█████████████████████████████████▌                                          | 131/297 [2:35:24<2:21:18, 51.07s/it]Llama.generate: 21 prefix-match hit, remaining 236 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   19770.32 ms /   236 tokens (   83.77 ms per token,    11.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =   48004.91 ms /   232 runs   (  206.92 ms per token,     4.83 tokens per second)\n",
      "llama_perf_context_print:       total time =   67942.42 ms /   468 tokens\n",
      " 44%|█████████████████████████████████▊                                          | 132/297 [2:36:32<2:34:23, 56.14s/it]Llama.generate: 21 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   15541.78 ms /   187 tokens (   83.11 ms per token,    12.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =   52684.17 ms /   255 runs   (  206.60 ms per token,     4.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   68421.18 ms /   442 tokens\n",
      " 45%|██████████████████████████████████                                          | 133/297 [2:37:40<2:43:32, 59.83s/it]Llama.generate: 21 prefix-match hit, remaining 211 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   17717.93 ms /   211 tokens (   83.97 ms per token,    11.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =   19167.12 ms /    96 runs   (  199.66 ms per token,     5.01 tokens per second)\n",
      "llama_perf_context_print:       total time =   36936.15 ms /   307 tokens\n",
      " 45%|██████████████████████████████████▎                                         | 134/297 [2:38:17<2:23:54, 52.97s/it]Llama.generate: 21 prefix-match hit, remaining 211 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   17822.90 ms /   211 tokens (   84.47 ms per token,    11.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =   34855.68 ms /   172 runs   (  202.65 ms per token,     4.93 tokens per second)\n",
      "llama_perf_context_print:       total time =   52790.02 ms /   383 tokens\n",
      " 45%|██████████████████████████████████▌                                         | 135/297 [2:39:10<2:22:54, 52.93s/it]Llama.generate: 21 prefix-match hit, remaining 186 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   15584.43 ms /   186 tokens (   83.79 ms per token,    11.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =   53394.00 ms /   255 runs   (  209.39 ms per token,     4.78 tokens per second)\n",
      "llama_perf_context_print:       total time =   69176.25 ms /   441 tokens\n",
      " 46%|██████████████████████████████████▊                                         | 136/297 [2:40:19<2:35:07, 57.81s/it]Llama.generate: 21 prefix-match hit, remaining 298 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   25900.51 ms /   298 tokens (   86.91 ms per token,    11.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =   41746.50 ms /   200 runs   (  208.73 ms per token,     4.79 tokens per second)\n",
      "llama_perf_context_print:       total time =   67778.06 ms /   498 tokens\n",
      " 46%|███████████████████████████████████                                         | 137/297 [2:41:27<2:42:09, 60.81s/it]Llama.generate: 21 prefix-match hit, remaining 170 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   14343.13 ms /   170 tokens (   84.37 ms per token,    11.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =   33145.03 ms /   163 runs   (  203.34 ms per token,     4.92 tokens per second)\n",
      "llama_perf_context_print:       total time =   47591.40 ms /   333 tokens\n",
      " 46%|███████████████████████████████████▎                                        | 138/297 [2:42:15<2:30:39, 56.85s/it]Llama.generate: 21 prefix-match hit, remaining 185 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   15491.84 ms /   185 tokens (   83.74 ms per token,    11.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =   20272.90 ms /   100 runs   (  202.73 ms per token,     4.93 tokens per second)\n",
      "llama_perf_context_print:       total time =   35815.10 ms /   285 tokens\n",
      " 47%|███████████████████████████████████▌                                        | 139/297 [2:42:51<2:13:07, 50.55s/it]Llama.generate: 21 prefix-match hit, remaining 205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   17336.53 ms /   205 tokens (   84.57 ms per token,    11.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7289.80 ms /    36 runs   (  202.49 ms per token,     4.94 tokens per second)\n",
      "llama_perf_context_print:       total time =   24640.39 ms /   241 tokens\n",
      " 47%|███████████████████████████████████▊                                        | 140/297 [2:43:15<1:51:57, 42.78s/it]Llama.generate: 21 prefix-match hit, remaining 232 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   20421.90 ms /   232 tokens (   88.03 ms per token,    11.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =   34597.36 ms /   164 runs   (  210.96 ms per token,     4.74 tokens per second)\n",
      "llama_perf_context_print:       total time =   55126.09 ms /   396 tokens\n",
      " 47%|████████████████████████████████████                                        | 141/297 [2:44:11<2:00:53, 46.50s/it]Llama.generate: 21 prefix-match hit, remaining 236 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   19661.21 ms /   236 tokens (   83.31 ms per token,    12.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =   52524.07 ms /   255 runs   (  205.98 ms per token,     4.85 tokens per second)\n",
      "llama_perf_context_print:       total time =   72383.11 ms /   491 tokens\n",
      " 48%|████████████████████████████████████▎                                       | 142/297 [2:45:23<2:20:12, 54.27s/it]Llama.generate: 21 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   15484.96 ms /   187 tokens (   82.81 ms per token,    12.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =   52313.38 ms /   255 runs   (  205.15 ms per token,     4.87 tokens per second)\n",
      "llama_perf_context_print:       total time =   67999.16 ms /   442 tokens\n",
      " 48%|████████████████████████████████████▌                                       | 143/297 [2:46:31<2:29:53, 58.40s/it]Llama.generate: 21 prefix-match hit, remaining 211 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   17606.72 ms /   211 tokens (   83.44 ms per token,    11.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =   26209.17 ms /   129 runs   (  203.17 ms per token,     4.92 tokens per second)\n",
      "llama_perf_context_print:       total time =   43885.34 ms /   340 tokens\n",
      " 48%|████████████████████████████████████▊                                       | 144/297 [2:47:15<2:17:49, 54.05s/it]Llama.generate: 21 prefix-match hit, remaining 211 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   17805.99 ms /   211 tokens (   84.39 ms per token,    11.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =   40099.00 ms /   195 runs   (  205.64 ms per token,     4.86 tokens per second)\n",
      "llama_perf_context_print:       total time =   58043.79 ms /   406 tokens\n",
      " 49%|█████████████████████████████████████                                       | 145/297 [2:48:13<2:19:58, 55.26s/it]Llama.generate: 21 prefix-match hit, remaining 177 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   14743.48 ms /   177 tokens (   83.30 ms per token,    12.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =   30628.17 ms /   149 runs   (  205.56 ms per token,     4.86 tokens per second)\n",
      "llama_perf_context_print:       total time =   45464.07 ms /   326 tokens\n",
      " 49%|█████████████████████████████████████▎                                      | 146/297 [2:48:58<2:11:41, 52.32s/it]Llama.generate: 21 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   15978.10 ms /   191 tokens (   83.65 ms per token,    11.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =   31096.15 ms /   152 runs   (  204.58 ms per token,     4.89 tokens per second)\n",
      "llama_perf_context_print:       total time =   47167.03 ms /   343 tokens\n",
      " 49%|█████████████████████████████████████▌                                      | 147/297 [2:49:46<2:06:57, 50.79s/it]Llama.generate: 21 prefix-match hit, remaining 284 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   24199.57 ms /   284 tokens (   85.21 ms per token,    11.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =   14477.77 ms /    71 runs   (  203.91 ms per token,     4.90 tokens per second)\n",
      "llama_perf_context_print:       total time =   38710.55 ms /   355 tokens\n",
      " 50%|█████████████████████████████████████▊                                      | 148/297 [2:50:24<1:57:08, 47.17s/it]Llama.generate: 21 prefix-match hit, remaining 225 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   19122.66 ms /   225 tokens (   84.99 ms per token,    11.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =   53632.16 ms /   255 runs   (  210.32 ms per token,     4.75 tokens per second)\n",
      "llama_perf_context_print:       total time =   72961.57 ms /   480 tokens\n",
      " 50%|██████████████████████████████████████▏                                     | 149/297 [2:51:37<2:15:27, 54.92s/it]Llama.generate: 21 prefix-match hit, remaining 236 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   19958.55 ms /   236 tokens (   84.57 ms per token,    11.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7904.89 ms /    40 runs   (  197.62 ms per token,     5.06 tokens per second)\n",
      "llama_perf_context_print:       total time =   27881.04 ms /   276 tokens\n",
      " 51%|██████████████████████████████████████▍                                     | 150/297 [2:52:05<1:54:41, 46.81s/it]Llama.generate: 21 prefix-match hit, remaining 182 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   15538.28 ms /   182 tokens (   85.38 ms per token,    11.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8798.59 ms /    43 runs   (  204.62 ms per token,     4.89 tokens per second)\n",
      "llama_perf_context_print:       total time =   24354.95 ms /   225 tokens\n",
      " 51%|██████████████████████████████████████▋                                     | 151/297 [2:52:30<1:37:32, 40.08s/it]Llama.generate: 21 prefix-match hit, remaining 250 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   21329.31 ms /   250 tokens (   85.32 ms per token,    11.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7106.56 ms /    36 runs   (  197.40 ms per token,     5.07 tokens per second)\n",
      "llama_perf_context_print:       total time =   28452.09 ms /   286 tokens\n",
      " 51%|██████████████████████████████████████▉                                     | 152/297 [2:52:58<1:28:27, 36.60s/it]Llama.generate: 22 prefix-match hit, remaining 275 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   23449.04 ms /   275 tokens (   85.27 ms per token,    11.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =   40213.34 ms /   197 runs   (  204.13 ms per token,     4.90 tokens per second)\n",
      "llama_perf_context_print:       total time =   63789.57 ms /   472 tokens\n",
      " 52%|███████████████████████████████████████▏                                    | 153/297 [2:54:02<1:47:26, 44.77s/it]Llama.generate: 21 prefix-match hit, remaining 209 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   17542.12 ms /   209 tokens (   83.93 ms per token,    11.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =   35939.58 ms /   174 runs   (  206.55 ms per token,     4.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   53600.61 ms /   383 tokens\n",
      " 52%|███████████████████████████████████████▍                                    | 154/297 [2:54:56<1:53:01, 47.42s/it]Llama.generate: 21 prefix-match hit, remaining 182 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   15279.68 ms /   182 tokens (   83.95 ms per token,    11.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =   41656.71 ms /   203 runs   (  205.21 ms per token,     4.87 tokens per second)\n",
      "llama_perf_context_print:       total time =   57080.53 ms /   385 tokens\n",
      " 52%|███████████████████████████████████████▋                                    | 155/297 [2:55:53<1:59:06, 50.33s/it]Llama.generate: 21 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   15664.62 ms /   187 tokens (   83.77 ms per token,    11.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =   24320.93 ms /   121 runs   (  201.00 ms per token,     4.98 tokens per second)\n",
      "llama_perf_context_print:       total time =   40049.86 ms /   308 tokens\n",
      " 53%|███████████████████████████████████████▉                                    | 156/297 [2:56:33<1:51:02, 47.25s/it]Llama.generate: 21 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   17331.26 ms /   206 tokens (   84.13 ms per token,    11.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =   25831.28 ms /   127 runs   (  203.40 ms per token,     4.92 tokens per second)\n",
      "llama_perf_context_print:       total time =   43232.89 ms /   333 tokens\n",
      " 53%|████████████████████████████████████████▏                                   | 157/297 [2:57:16<1:47:27, 46.06s/it]Llama.generate: 21 prefix-match hit, remaining 239 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   20454.33 ms /   239 tokens (   85.58 ms per token,    11.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =   47465.74 ms /   230 runs   (  206.37 ms per token,     4.85 tokens per second)\n",
      "llama_perf_context_print:       total time =   68087.81 ms /   469 tokens\n",
      " 53%|████████████████████████████████████████▍                                   | 158/297 [2:58:24<2:02:01, 52.67s/it]Llama.generate: 21 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   17354.93 ms /   198 tokens (   87.65 ms per token,    11.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =   38803.27 ms /   191 runs   (  203.16 ms per token,     4.92 tokens per second)\n",
      "llama_perf_context_print:       total time =   56291.82 ms /   389 tokens\n",
      " 54%|████████████████████████████████████████▋                                   | 159/297 [2:59:20<2:03:40, 53.77s/it]Llama.generate: 21 prefix-match hit, remaining 228 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   19155.09 ms /   228 tokens (   84.01 ms per token,    11.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =   18396.59 ms /    91 runs   (  202.16 ms per token,     4.95 tokens per second)\n",
      "llama_perf_context_print:       total time =   37596.66 ms /   319 tokens\n",
      " 54%|████████████████████████████████████████▉                                   | 160/297 [2:59:58<1:51:42, 48.93s/it]Llama.generate: 22 prefix-match hit, remaining 290 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   24998.38 ms /   290 tokens (   86.20 ms per token,    11.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =   53129.63 ms /   255 runs   (  208.35 ms per token,     4.80 tokens per second)\n",
      "llama_perf_context_print:       total time =   78334.05 ms /   545 tokens\n",
      " 54%|█████████████████████████████████████████▏                                  | 161/297 [3:01:16<2:10:54, 57.76s/it]Llama.generate: 21 prefix-match hit, remaining 286 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   24017.02 ms /   286 tokens (   83.98 ms per token,    11.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =   51890.48 ms /   255 runs   (  203.49 ms per token,     4.91 tokens per second)\n",
      "llama_perf_context_print:       total time =   76101.60 ms /   541 tokens\n",
      " 55%|█████████████████████████████████████████▍                                  | 162/297 [3:02:33<2:22:21, 63.27s/it]Llama.generate: 21 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   16350.23 ms /   196 tokens (   83.42 ms per token,    11.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =   52769.34 ms /   255 runs   (  206.94 ms per token,     4.83 tokens per second)\n",
      "llama_perf_context_print:       total time =   69320.48 ms /   451 tokens\n",
      " 55%|█████████████████████████████████████████▋                                  | 163/297 [3:03:42<2:25:22, 65.09s/it]Llama.generate: 21 prefix-match hit, remaining 284 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   23973.16 ms /   284 tokens (   84.41 ms per token,    11.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =   49776.88 ms /   246 runs   (  202.35 ms per token,     4.94 tokens per second)\n",
      "llama_perf_context_print:       total time =   73941.26 ms /   530 tokens\n",
      " 55%|█████████████████████████████████████████▉                                  | 164/297 [3:04:56<2:30:11, 67.76s/it]Llama.generate: 21 prefix-match hit, remaining 176 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   14870.84 ms /   176 tokens (   84.49 ms per token,    11.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =   34570.98 ms /   169 runs   (  204.56 ms per token,     4.89 tokens per second)\n",
      "llama_perf_context_print:       total time =   49548.93 ms /   345 tokens\n",
      " 56%|██████████████████████████████████████████▏                                 | 165/297 [3:05:45<2:17:03, 62.30s/it]Llama.generate: 21 prefix-match hit, remaining 777 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   68343.06 ms /   777 tokens (   87.96 ms per token,    11.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =   36723.56 ms /   174 runs   (  211.05 ms per token,     4.74 tokens per second)\n",
      "llama_perf_context_print:       total time =  105173.52 ms /   951 tokens\n",
      " 56%|██████████████████████████████████████████▍                                 | 166/297 [3:07:31<2:44:07, 75.17s/it]Llama.generate: 21 prefix-match hit, remaining 166 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   13974.52 ms /   166 tokens (   84.18 ms per token,    11.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =   42665.12 ms /   211 runs   (  202.20 ms per token,     4.95 tokens per second)\n",
      "llama_perf_context_print:       total time =   56783.57 ms /   377 tokens\n",
      " 56%|██████████████████████████████████████████▋                                 | 167/297 [3:08:27<2:30:56, 69.66s/it]Llama.generate: 21 prefix-match hit, remaining 220 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   18321.44 ms /   220 tokens (   83.28 ms per token,    12.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =   27461.03 ms /   138 runs   (  198.99 ms per token,     5.03 tokens per second)\n",
      "llama_perf_context_print:       total time =   45860.99 ms /   358 tokens\n",
      " 57%|██████████████████████████████████████████▉                                 | 168/297 [3:09:13<2:14:26, 62.53s/it]Llama.generate: 21 prefix-match hit, remaining 225 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   18838.02 ms /   225 tokens (   83.72 ms per token,    11.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =   33398.17 ms /   167 runs   (  199.99 ms per token,     5.00 tokens per second)\n",
      "llama_perf_context_print:       total time =   52335.38 ms /   392 tokens\n",
      " 57%|███████████████████████████████████████████▏                                | 169/297 [3:10:06<2:06:53, 59.48s/it]Llama.generate: 21 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   16798.73 ms /   201 tokens (   83.58 ms per token,    11.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =   35426.06 ms /   176 runs   (  201.28 ms per token,     4.97 tokens per second)\n",
      "llama_perf_context_print:       total time =   52332.02 ms /   377 tokens\n",
      " 57%|███████████████████████████████████████████▌                                | 170/297 [3:10:58<2:01:22, 57.34s/it]Llama.generate: 67 prefix-match hit, remaining 231 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   19630.67 ms /   231 tokens (   84.98 ms per token,    11.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =   22928.88 ms /   113 runs   (  202.91 ms per token,     4.93 tokens per second)\n",
      "llama_perf_context_print:       total time =   42623.14 ms /   344 tokens\n",
      " 58%|███████████████████████████████████████████▊                                | 171/297 [3:11:41<1:51:14, 52.97s/it]Llama.generate: 21 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   16617.83 ms /   196 tokens (   84.78 ms per token,    11.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =   13296.94 ms /    67 runs   (  198.46 ms per token,     5.04 tokens per second)\n",
      "llama_perf_context_print:       total time =   29944.27 ms /   263 tokens\n",
      " 58%|████████████████████████████████████████████                                | 172/297 [3:12:11<1:35:59, 46.07s/it]Llama.generate: 59 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   17067.71 ms /   201 tokens (   84.91 ms per token,    11.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =   42545.49 ms /   209 runs   (  203.57 ms per token,     4.91 tokens per second)\n",
      "llama_perf_context_print:       total time =   59768.85 ms /   410 tokens\n",
      " 58%|████████████████████████████████████████████▎                               | 173/297 [3:13:11<1:43:43, 50.19s/it]Llama.generate: 21 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   16758.77 ms /   200 tokens (   83.79 ms per token,    11.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =   19036.47 ms /    94 runs   (  202.52 ms per token,     4.94 tokens per second)\n",
      "llama_perf_context_print:       total time =   35846.56 ms /   294 tokens\n",
      " 59%|████████████████████████████████████████████▌                               | 174/297 [3:13:47<1:34:05, 45.90s/it]Llama.generate: 21 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   16840.17 ms /   194 tokens (   86.81 ms per token,    11.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7204.41 ms /    35 runs   (  205.84 ms per token,     4.86 tokens per second)\n",
      "llama_perf_context_print:       total time =   24062.32 ms /   229 tokens\n",
      " 59%|████████████████████████████████████████████▊                               | 175/297 [3:14:11<1:20:01, 39.36s/it]Llama.generate: 21 prefix-match hit, remaining 247 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   21143.56 ms /   247 tokens (   85.60 ms per token,    11.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =   15690.57 ms /    76 runs   (  206.45 ms per token,     4.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   36870.19 ms /   323 tokens\n",
      " 59%|█████████████████████████████████████████████                               | 176/297 [3:14:47<1:17:53, 38.62s/it]Llama.generate: 22 prefix-match hit, remaining 289 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   24553.98 ms /   289 tokens (   84.96 ms per token,    11.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =   37689.39 ms /   184 runs   (  204.83 ms per token,     4.88 tokens per second)\n",
      "llama_perf_context_print:       total time =   62365.37 ms /   473 tokens\n",
      " 60%|█████████████████████████████████████████████▎                              | 177/297 [3:15:50<1:31:30, 45.75s/it]Llama.generate: 21 prefix-match hit, remaining 231 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   19609.32 ms /   231 tokens (   84.89 ms per token,    11.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =   39770.87 ms /   196 runs   (  202.91 ms per token,     4.93 tokens per second)\n",
      "llama_perf_context_print:       total time =   59512.06 ms /   427 tokens\n",
      " 60%|█████████████████████████████████████████████▌                              | 178/297 [3:16:49<1:38:56, 49.89s/it]Llama.generate: 21 prefix-match hit, remaining 281 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   23820.17 ms /   281 tokens (   84.77 ms per token,    11.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7773.96 ms /    38 runs   (  204.58 ms per token,     4.89 tokens per second)\n",
      "llama_perf_context_print:       total time =   31613.12 ms /   319 tokens\n",
      " 60%|█████████████████████████████████████████████▊                              | 179/297 [3:17:21<1:27:20, 44.41s/it]Llama.generate: 21 prefix-match hit, remaining 288 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   24712.93 ms /   288 tokens (   85.81 ms per token,    11.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =   43576.02 ms /   211 runs   (  206.52 ms per token,     4.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   68443.63 ms /   499 tokens\n",
      " 61%|██████████████████████████████████████████████                              | 180/297 [3:18:30<1:40:40, 51.63s/it]Llama.generate: 21 prefix-match hit, remaining 285 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   23857.82 ms /   285 tokens (   83.71 ms per token,    11.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =   19121.56 ms /    93 runs   (  205.61 ms per token,     4.86 tokens per second)\n",
      "llama_perf_context_print:       total time =   43024.51 ms /   378 tokens\n",
      " 61%|██████████████████████████████████████████████▎                             | 181/297 [3:19:13<1:34:50, 49.06s/it]Llama.generate: 21 prefix-match hit, remaining 216 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   18550.22 ms /   216 tokens (   85.88 ms per token,    11.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =   45174.94 ms /   227 runs   (  199.01 ms per token,     5.02 tokens per second)\n",
      "llama_perf_context_print:       total time =   63890.47 ms /   443 tokens\n",
      " 61%|██████████████████████████████████████████████▌                             | 182/297 [3:20:17<1:42:34, 53.52s/it]Llama.generate: 22 prefix-match hit, remaining 289 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   24394.94 ms /   289 tokens (   84.41 ms per token,    11.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =   28801.03 ms /   139 runs   (  207.20 ms per token,     4.83 tokens per second)\n",
      "llama_perf_context_print:       total time =   53269.03 ms /   428 tokens\n",
      " 62%|██████████████████████████████████████████████▊                             | 183/297 [3:21:10<1:41:33, 53.45s/it]Llama.generate: 21 prefix-match hit, remaining 305 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   26033.31 ms /   305 tokens (   85.36 ms per token,    11.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2957.46 ms /    15 runs   (  197.16 ms per token,     5.07 tokens per second)\n",
      "llama_perf_context_print:       total time =   28997.66 ms /   320 tokens\n",
      " 62%|███████████████████████████████████████████████                             | 184/297 [3:21:39<1:26:51, 46.12s/it]Llama.generate: 21 prefix-match hit, remaining 324 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   28300.69 ms /   324 tokens (   87.35 ms per token,    11.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =   14752.85 ms /    72 runs   (  204.90 ms per token,     4.88 tokens per second)\n",
      "llama_perf_context_print:       total time =   43088.98 ms /   396 tokens\n",
      " 62%|███████████████████████████████████████████████▎                            | 185/297 [3:22:22<1:24:24, 45.22s/it]Llama.generate: 21 prefix-match hit, remaining 269 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   22601.89 ms /   269 tokens (   84.02 ms per token,    11.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =   26406.86 ms /   130 runs   (  203.13 ms per token,     4.92 tokens per second)\n",
      "llama_perf_context_print:       total time =   49082.69 ms /   399 tokens\n",
      " 63%|███████████████████████████████████████████████▌                            | 186/297 [3:23:11<1:25:49, 46.39s/it]Llama.generate: 22 prefix-match hit, remaining 281 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   23678.56 ms /   281 tokens (   84.27 ms per token,    11.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7346.44 ms /    36 runs   (  204.07 ms per token,     4.90 tokens per second)\n",
      "llama_perf_context_print:       total time =   31038.76 ms /   317 tokens\n",
      " 63%|███████████████████████████████████████████████▊                            | 187/297 [3:23:42<1:16:37, 41.79s/it]Llama.generate: 21 prefix-match hit, remaining 229 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   19722.84 ms /   229 tokens (   86.13 ms per token,    11.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =   52033.21 ms /   255 runs   (  204.05 ms per token,     4.90 tokens per second)\n",
      "llama_perf_context_print:       total time =   71949.48 ms /   484 tokens\n",
      " 63%|████████████████████████████████████████████████                            | 188/297 [3:24:54<1:32:22, 50.85s/it]Llama.generate: 21 prefix-match hit, remaining 322 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   28661.09 ms /   322 tokens (   89.01 ms per token,    11.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =   33462.06 ms /   163 runs   (  205.29 ms per token,     4.87 tokens per second)\n",
      "llama_perf_context_print:       total time =   62226.47 ms /   485 tokens\n",
      " 64%|████████████████████████████████████████████████▎                           | 189/297 [3:25:56<1:37:41, 54.27s/it]Llama.generate: 21 prefix-match hit, remaining 277 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   23540.74 ms /   277 tokens (   84.98 ms per token,    11.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =   52226.61 ms /   255 runs   (  204.81 ms per token,     4.88 tokens per second)\n",
      "llama_perf_context_print:       total time =   75961.76 ms /   532 tokens\n",
      " 64%|████████████████████████████████████████████████▌                           | 190/297 [3:27:12<1:48:24, 60.79s/it]Llama.generate: 21 prefix-match hit, remaining 248 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   20932.20 ms /   248 tokens (   84.40 ms per token,    11.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4363.15 ms /    21 runs   (  207.77 ms per token,     4.81 tokens per second)\n",
      "llama_perf_context_print:       total time =   25303.86 ms /   269 tokens\n",
      " 64%|████████████████████████████████████████████████▉                           | 191/297 [3:27:38<1:28:35, 50.15s/it]Llama.generate: 21 prefix-match hit, remaining 221 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   19015.17 ms /   221 tokens (   86.04 ms per token,    11.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =   48077.53 ms /   238 runs   (  202.01 ms per token,     4.95 tokens per second)\n",
      "llama_perf_context_print:       total time =   67276.30 ms /   459 tokens\n",
      " 65%|█████████████████████████████████████████████████▏                          | 192/297 [3:28:45<1:36:45, 55.29s/it]Llama.generate: 22 prefix-match hit, remaining 227 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   18984.38 ms /   227 tokens (   83.63 ms per token,    11.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =   52088.71 ms /   255 runs   (  204.27 ms per token,     4.90 tokens per second)\n",
      "llama_perf_context_print:       total time =   71260.29 ms /   482 tokens\n",
      " 65%|█████████████████████████████████████████████████▍                          | 193/297 [3:29:56<1:44:09, 60.09s/it]Llama.generate: 21 prefix-match hit, remaining 258 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   21786.08 ms /   258 tokens (   84.44 ms per token,    11.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =   32415.98 ms /   159 runs   (  203.87 ms per token,     4.90 tokens per second)\n",
      "llama_perf_context_print:       total time =   54301.47 ms /   417 tokens\n",
      " 65%|█████████████████████████████████████████████████▋                          | 194/297 [3:30:51<1:40:11, 58.36s/it]Llama.generate: 21 prefix-match hit, remaining 326 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   27983.63 ms /   326 tokens (   85.84 ms per token,    11.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =   27853.96 ms /   138 runs   (  201.84 ms per token,     4.95 tokens per second)\n",
      "llama_perf_context_print:       total time =   55915.60 ms /   464 tokens\n",
      " 66%|█████████████████████████████████████████████████▉                          | 195/297 [3:31:47<1:37:58, 57.64s/it]Llama.generate: 21 prefix-match hit, remaining 210 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   18062.42 ms /   210 tokens (   86.01 ms per token,    11.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7449.75 ms /    37 runs   (  201.34 ms per token,     4.97 tokens per second)\n",
      "llama_perf_context_print:       total time =   25531.04 ms /   247 tokens\n",
      " 66%|██████████████████████████████████████████████████▏                         | 196/297 [3:32:12<1:20:49, 48.01s/it]Llama.generate: 21 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   16214.03 ms /   190 tokens (   85.34 ms per token,    11.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =   43354.75 ms /   213 runs   (  203.54 ms per token,     4.91 tokens per second)\n",
      "llama_perf_context_print:       total time =   59716.75 ms /   403 tokens\n",
      " 66%|██████████████████████████████████████████████████▍                         | 197/297 [3:33:12<1:25:53, 51.53s/it]Llama.generate: 21 prefix-match hit, remaining 219 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   18292.62 ms /   219 tokens (   83.53 ms per token,    11.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8782.44 ms /    43 runs   (  204.24 ms per token,     4.90 tokens per second)\n",
      "llama_perf_context_print:       total time =   27095.45 ms /   262 tokens\n",
      " 67%|██████████████████████████████████████████████████▋                         | 198/297 [3:33:39<1:12:56, 44.21s/it]Llama.generate: 21 prefix-match hit, remaining 211 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   18165.21 ms /   211 tokens (   86.09 ms per token,    11.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =   41765.39 ms /   206 runs   (  202.74 ms per token,     4.93 tokens per second)\n",
      "llama_perf_context_print:       total time =   60065.71 ms /   417 tokens\n",
      " 67%|██████████████████████████████████████████████████▉                         | 199/297 [3:34:39<1:19:59, 48.98s/it]Llama.generate: 21 prefix-match hit, remaining 274 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   23338.09 ms /   274 tokens (   85.18 ms per token,    11.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9363.49 ms /    46 runs   (  203.55 ms per token,     4.91 tokens per second)\n",
      "llama_perf_context_print:       total time =   32720.37 ms /   320 tokens\n",
      " 67%|███████████████████████████████████████████████████▏                        | 200/297 [3:35:12<1:11:18, 44.11s/it]Llama.generate: 21 prefix-match hit, remaining 229 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   19549.71 ms /   229 tokens (   85.37 ms per token,    11.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =   41735.88 ms /   200 runs   (  208.68 ms per token,     4.79 tokens per second)\n",
      "llama_perf_context_print:       total time =   61432.94 ms /   429 tokens\n",
      " 68%|███████████████████████████████████████████████████▍                        | 201/297 [3:36:13<1:18:54, 49.31s/it]Llama.generate: 21 prefix-match hit, remaining 256 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   21672.49 ms /   256 tokens (   84.66 ms per token,    11.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5669.83 ms /    29 runs   (  195.51 ms per token,     5.11 tokens per second)\n",
      "llama_perf_context_print:       total time =   27353.85 ms /   285 tokens\n",
      " 68%|███████████████████████████████████████████████████▋                        | 202/297 [3:36:41<1:07:39, 42.73s/it]Llama.generate: 21 prefix-match hit, remaining 176 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   15821.02 ms /   176 tokens (   89.89 ms per token,    11.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9180.17 ms /    45 runs   (  204.00 ms per token,     4.90 tokens per second)\n",
      "llama_perf_context_print:       total time =   25021.91 ms /   221 tokens\n",
      " 68%|█████████████████████████████████████████████████████▎                        | 203/297 [3:37:06<58:38, 37.43s/it]Llama.generate: 21 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   16210.11 ms /   191 tokens (   84.87 ms per token,    11.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3597.25 ms /    17 runs   (  211.60 ms per token,     4.73 tokens per second)\n",
      "llama_perf_context_print:       total time =   19815.30 ms /   208 tokens\n",
      " 69%|█████████████████████████████████████████████████████▌                        | 204/297 [3:37:26<49:49, 32.15s/it]Llama.generate: 21 prefix-match hit, remaining 282 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   24278.33 ms /   282 tokens (   86.09 ms per token,    11.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8374.56 ms /    41 runs   (  204.26 ms per token,     4.90 tokens per second)\n",
      "llama_perf_context_print:       total time =   32671.65 ms /   323 tokens\n",
      " 69%|█████████████████████████████████████████████████████▊                        | 205/297 [3:37:58<49:33, 32.32s/it]Llama.generate: 21 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   15972.06 ms /   188 tokens (   84.96 ms per token,    11.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10702.84 ms /    54 runs   (  198.20 ms per token,     5.05 tokens per second)\n",
      "llama_perf_context_print:       total time =   26698.89 ms /   242 tokens\n",
      " 69%|██████████████████████████████████████████████████████                        | 206/297 [3:38:25<46:28, 30.64s/it]Llama.generate: 21 prefix-match hit, remaining 626 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   55065.81 ms /   626 tokens (   87.96 ms per token,    11.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =   40091.31 ms /   188 runs   (  213.25 ms per token,     4.69 tokens per second)\n",
      "llama_perf_context_print:       total time =   95276.93 ms /   814 tokens\n",
      " 70%|████████████████████████████████████████████████████▉                       | 207/297 [3:40:00<1:15:03, 50.04s/it]Llama.generate: 21 prefix-match hit, remaining 231 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   19297.49 ms /   231 tokens (   83.54 ms per token,    11.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =   52558.73 ms /   255 runs   (  206.11 ms per token,     4.85 tokens per second)\n",
      "llama_perf_context_print:       total time =   72050.70 ms /   486 tokens\n",
      " 70%|█████████████████████████████████████████████████████▏                      | 208/297 [3:41:12<1:24:01, 56.65s/it]Llama.generate: 21 prefix-match hit, remaining 260 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   21907.42 ms /   260 tokens (   84.26 ms per token,    11.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =   53057.82 ms /   255 runs   (  208.07 ms per token,     4.81 tokens per second)\n",
      "llama_perf_context_print:       total time =   75164.19 ms /   515 tokens\n",
      " 70%|█████████████████████████████████████████████████████▍                      | 209/297 [3:42:28<1:31:14, 62.22s/it]Llama.generate: 21 prefix-match hit, remaining 211 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   18056.80 ms /   211 tokens (   85.58 ms per token,    11.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8170.82 ms /    40 runs   (  204.27 ms per token,     4.90 tokens per second)\n",
      "llama_perf_context_print:       total time =   26245.90 ms /   251 tokens\n",
      " 71%|█████████████████████████████████████████████████████▋                      | 210/297 [3:42:54<1:14:34, 51.43s/it]Llama.generate: 21 prefix-match hit, remaining 248 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   21216.69 ms /   248 tokens (   85.55 ms per token,    11.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6189.30 ms /    30 runs   (  206.31 ms per token,     4.85 tokens per second)\n",
      "llama_perf_context_print:       total time =   27418.57 ms /   278 tokens\n",
      " 71%|█████████████████████████████████████████████████████▉                      | 211/297 [3:43:21<1:03:24, 44.24s/it]Llama.generate: 21 prefix-match hit, remaining 242 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   20884.24 ms /   242 tokens (   86.30 ms per token,    11.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =   32806.81 ms /   162 runs   (  202.51 ms per token,     4.94 tokens per second)\n",
      "llama_perf_context_print:       total time =   53797.57 ms /   404 tokens\n",
      " 71%|██████████████████████████████████████████████████████▏                     | 212/297 [3:44:15<1:06:44, 47.11s/it]Llama.generate: 21 prefix-match hit, remaining 178 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   14714.86 ms /   178 tokens (   82.67 ms per token,    12.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =   52157.46 ms /   255 runs   (  204.54 ms per token,     4.89 tokens per second)\n",
      "llama_perf_context_print:       total time =   67069.96 ms /   433 tokens\n",
      " 72%|██████████████████████████████████████████████████████▌                     | 213/297 [3:45:22<1:14:21, 53.11s/it]Llama.generate: 21 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   15848.42 ms /   191 tokens (   82.98 ms per token,    12.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =   38314.79 ms /   191 runs   (  200.60 ms per token,     4.99 tokens per second)\n",
      "llama_perf_context_print:       total time =   54283.03 ms /   382 tokens\n",
      " 72%|██████████████████████████████████████████████████████▊                     | 214/297 [3:46:17<1:13:57, 53.47s/it]Llama.generate: 21 prefix-match hit, remaining 245 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   20653.40 ms /   245 tokens (   84.30 ms per token,    11.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =   38089.10 ms /   185 runs   (  205.89 ms per token,     4.86 tokens per second)\n",
      "llama_perf_context_print:       total time =   58862.76 ms /   430 tokens\n",
      " 72%|███████████████████████████████████████████████████████                     | 215/297 [3:47:15<1:15:17, 55.09s/it]Llama.generate: 21 prefix-match hit, remaining 246 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   20922.69 ms /   246 tokens (   85.05 ms per token,    11.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =   27742.89 ms /   138 runs   (  201.04 ms per token,     4.97 tokens per second)\n",
      "llama_perf_context_print:       total time =   48744.61 ms /   384 tokens\n",
      " 73%|███████████████████████████████████████████████████████▎                    | 216/297 [3:48:04<1:11:49, 53.20s/it]Llama.generate: 21 prefix-match hit, remaining 441 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   38074.93 ms /   441 tokens (   86.34 ms per token,    11.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =   21407.74 ms /   103 runs   (  207.84 ms per token,     4.81 tokens per second)\n",
      "llama_perf_context_print:       total time =   59535.70 ms /   544 tokens\n",
      " 73%|███████████████████████████████████████████████████████▌                    | 217/297 [3:49:04<1:13:28, 55.11s/it]Llama.generate: 21 prefix-match hit, remaining 174 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   14618.98 ms /   174 tokens (   84.02 ms per token,    11.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =   11900.10 ms /    59 runs   (  201.70 ms per token,     4.96 tokens per second)\n",
      "llama_perf_context_print:       total time =   26545.35 ms /   233 tokens\n",
      " 73%|███████████████████████████████████████████████████████▊                    | 218/297 [3:49:30<1:01:17, 46.55s/it]Llama.generate: 21 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   17707.87 ms /   197 tokens (   89.89 ms per token,    11.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8774.07 ms /    43 runs   (  204.05 ms per token,     4.90 tokens per second)\n",
      "llama_perf_context_print:       total time =   26499.91 ms /   240 tokens\n",
      " 74%|█████████████████████████████████████████████████████████▌                    | 219/297 [3:49:57<52:42, 40.54s/it]Llama.generate: 21 prefix-match hit, remaining 221 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   19003.29 ms /   221 tokens (   85.99 ms per token,    11.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5514.78 ms /    28 runs   (  196.96 ms per token,     5.08 tokens per second)\n",
      "llama_perf_context_print:       total time =   24529.85 ms /   249 tokens\n",
      " 74%|█████████████████████████████████████████████████████████▊                    | 220/297 [3:50:21<45:52, 35.75s/it]Llama.generate: 21 prefix-match hit, remaining 159 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   13592.34 ms /   159 tokens (   85.49 ms per token,    11.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =   29558.43 ms /   146 runs   (  202.45 ms per token,     4.94 tokens per second)\n",
      "llama_perf_context_print:       total time =   43239.15 ms /   305 tokens\n",
      " 74%|██████████████████████████████████████████████████████████                    | 221/297 [3:51:05<48:08, 38.00s/it]Llama.generate: 21 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   17102.28 ms /   198 tokens (   86.38 ms per token,    11.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =   38362.02 ms /   188 runs   (  204.05 ms per token,     4.90 tokens per second)\n",
      "llama_perf_context_print:       total time =   55590.84 ms /   386 tokens\n",
      " 75%|██████████████████████████████████████████████████████████▎                   | 222/297 [3:52:00<54:06, 43.29s/it]Llama.generate: 21 prefix-match hit, remaining 258 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   21919.22 ms /   258 tokens (   84.96 ms per token,    11.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =   25924.27 ms /   127 runs   (  204.13 ms per token,     4.90 tokens per second)\n",
      "llama_perf_context_print:       total time =   47918.84 ms /   385 tokens\n",
      " 75%|██████████████████████████████████████████████████████████▌                   | 223/297 [3:52:48<55:06, 44.69s/it]Llama.generate: 22 prefix-match hit, remaining 239 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   20389.58 ms /   239 tokens (   85.31 ms per token,    11.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =   24970.28 ms /   125 runs   (  199.76 ms per token,     5.01 tokens per second)\n",
      "llama_perf_context_print:       total time =   45427.42 ms /   364 tokens\n",
      " 75%|██████████████████████████████████████████████████████████▊                   | 224/297 [3:53:34<54:38, 44.92s/it]Llama.generate: 22 prefix-match hit, remaining 184 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   15448.76 ms /   184 tokens (   83.96 ms per token,    11.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =   32707.60 ms /   162 runs   (  201.90 ms per token,     4.95 tokens per second)\n",
      "llama_perf_context_print:       total time =   48256.76 ms /   346 tokens\n",
      " 76%|███████████████████████████████████████████████████████████                   | 225/297 [3:54:22<55:06, 45.93s/it]Llama.generate: 21 prefix-match hit, remaining 232 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   19705.37 ms /   232 tokens (   84.94 ms per token,    11.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =   25167.62 ms /   125 runs   (  201.34 ms per token,     4.97 tokens per second)\n",
      "llama_perf_context_print:       total time =   44944.93 ms /   357 tokens\n",
      " 76%|███████████████████████████████████████████████████████████▎                  | 226/297 [3:55:07<54:00, 45.64s/it]Llama.generate: 21 prefix-match hit, remaining 218 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   18401.88 ms /   218 tokens (   84.41 ms per token,    11.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =   30755.71 ms /   150 runs   (  205.04 ms per token,     4.88 tokens per second)\n",
      "llama_perf_context_print:       total time =   49248.52 ms /   368 tokens\n",
      " 76%|███████████████████████████████████████████████████████████▌                  | 227/297 [3:55:56<54:31, 46.73s/it]Llama.generate: 21 prefix-match hit, remaining 648 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   58339.97 ms /   648 tokens (   90.03 ms per token,    11.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =   32619.55 ms /   154 runs   (  211.82 ms per token,     4.72 tokens per second)\n",
      "llama_perf_context_print:       total time =   91056.60 ms /   802 tokens\n",
      " 77%|██████████████████████████████████████████████████████████▎                 | 228/297 [3:57:27<1:09:02, 60.04s/it]Llama.generate: 21 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   15698.90 ms /   189 tokens (   83.06 ms per token,    12.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =   35609.97 ms /   175 runs   (  203.49 ms per token,     4.91 tokens per second)\n",
      "llama_perf_context_print:       total time =   51418.99 ms /   364 tokens\n",
      " 77%|██████████████████████████████████████████████████████████▌                 | 229/297 [3:58:19<1:05:07, 57.46s/it]Llama.generate: 21 prefix-match hit, remaining 182 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   15263.46 ms /   182 tokens (   83.87 ms per token,    11.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7797.44 ms /    39 runs   (  199.93 ms per token,     5.00 tokens per second)\n",
      "llama_perf_context_print:       total time =   23077.54 ms /   221 tokens\n",
      " 77%|████████████████████████████████████████████████████████████▍                 | 230/297 [3:58:42<52:39, 47.15s/it]Llama.generate: 21 prefix-match hit, remaining 278 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   23752.30 ms /   278 tokens (   85.44 ms per token,    11.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =   35904.44 ms /   173 runs   (  207.54 ms per token,     4.82 tokens per second)\n",
      "llama_perf_context_print:       total time =   59762.84 ms /   451 tokens\n",
      " 78%|████████████████████████████████████████████████████████████▋                 | 231/297 [3:59:42<56:02, 50.95s/it]Llama.generate: 21 prefix-match hit, remaining 281 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   23775.90 ms /   281 tokens (   84.61 ms per token,    11.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =   52395.45 ms /   255 runs   (  205.47 ms per token,     4.87 tokens per second)\n",
      "llama_perf_context_print:       total time =   76366.44 ms /   536 tokens\n",
      " 78%|███████████████████████████████████████████████████████████▎                | 232/297 [4:00:58<1:03:27, 58.58s/it]Llama.generate: 21 prefix-match hit, remaining 236 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   19957.92 ms /   236 tokens (   84.57 ms per token,    11.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =   45209.57 ms /   219 runs   (  206.44 ms per token,     4.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   65331.49 ms /   455 tokens\n",
      " 78%|███████████████████████████████████████████████████████████▌                | 233/297 [4:02:03<1:04:39, 60.61s/it]Llama.generate: 21 prefix-match hit, remaining 228 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   19215.76 ms /   228 tokens (   84.28 ms per token,    11.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =   42271.47 ms /   210 runs   (  201.29 ms per token,     4.97 tokens per second)\n",
      "llama_perf_context_print:       total time =   61631.20 ms /   438 tokens\n",
      " 79%|███████████████████████████████████████████████████████████▉                | 234/297 [4:03:05<1:03:58, 60.93s/it]Llama.generate: 21 prefix-match hit, remaining 266 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   22507.94 ms /   266 tokens (   84.62 ms per token,    11.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =   33098.69 ms /   160 runs   (  206.87 ms per token,     4.83 tokens per second)\n",
      "llama_perf_context_print:       total time =   55703.11 ms /   426 tokens\n",
      " 79%|████████████████████████████████████████████████████████████▏               | 235/297 [4:04:01<1:01:20, 59.37s/it]Llama.generate: 21 prefix-match hit, remaining 216 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   18069.97 ms /   216 tokens (   83.66 ms per token,    11.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =   51881.70 ms /   255 runs   (  203.46 ms per token,     4.92 tokens per second)\n",
      "llama_perf_context_print:       total time =   70145.18 ms /   471 tokens\n",
      " 79%|████████████████████████████████████████████████████████████▍               | 236/297 [4:05:11<1:03:39, 62.61s/it]Llama.generate: 21 prefix-match hit, remaining 285 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   24071.33 ms /   285 tokens (   84.46 ms per token,    11.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =   23098.31 ms /   112 runs   (  206.23 ms per token,     4.85 tokens per second)\n",
      "llama_perf_context_print:       total time =   47229.00 ms /   397 tokens\n",
      " 80%|██████████████████████████████████████████████████████████████▏               | 237/297 [4:05:58<58:00, 58.00s/it]Llama.generate: 21 prefix-match hit, remaining 267 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   22796.52 ms /   267 tokens (   85.38 ms per token,    11.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10231.21 ms /    48 runs   (  213.15 ms per token,     4.69 tokens per second)\n",
      "llama_perf_context_print:       total time =   33049.94 ms /   315 tokens\n",
      " 80%|██████████████████████████████████████████████████████████████▌               | 238/297 [4:06:31<49:41, 50.53s/it]Llama.generate: 21 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   16144.25 ms /   188 tokens (   85.87 ms per token,    11.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =   19514.79 ms /    97 runs   (  201.18 ms per token,     4.97 tokens per second)\n",
      "llama_perf_context_print:       total time =   35708.38 ms /   285 tokens\n",
      " 80%|██████████████████████████████████████████████████████████████▊               | 239/297 [4:07:07<44:33, 46.09s/it]Llama.generate: 22 prefix-match hit, remaining 218 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   18822.53 ms /   218 tokens (   86.34 ms per token,    11.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =   51620.42 ms /   255 runs   (  202.43 ms per token,     4.94 tokens per second)\n",
      "llama_perf_context_print:       total time =   70627.33 ms /   473 tokens\n",
      " 81%|███████████████████████████████████████████████████████████████               | 240/297 [4:08:18<50:47, 53.46s/it]Llama.generate: 21 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   16613.54 ms /   198 tokens (   83.91 ms per token,    11.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =   43251.71 ms /   214 runs   (  202.11 ms per token,     4.95 tokens per second)\n",
      "llama_perf_context_print:       total time =   60016.16 ms /   412 tokens\n",
      " 81%|███████████████████████████████████████████████████████████████▎              | 241/297 [4:09:18<51:44, 55.43s/it]Llama.generate: 21 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   16010.22 ms /   190 tokens (   84.26 ms per token,    11.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5183.30 ms /    26 runs   (  199.36 ms per token,     5.02 tokens per second)\n",
      "llama_perf_context_print:       total time =   21203.04 ms /   216 tokens\n",
      " 81%|███████████████████████████████████████████████████████████████▌              | 242/297 [4:09:39<41:24, 45.17s/it]Llama.generate: 21 prefix-match hit, remaining 215 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   18422.58 ms /   215 tokens (   85.69 ms per token,    11.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =   18011.86 ms /    89 runs   (  202.38 ms per token,     4.94 tokens per second)\n",
      "llama_perf_context_print:       total time =   36480.37 ms /   304 tokens\n",
      " 82%|███████████████████████████████████████████████████████████████▊              | 243/297 [4:10:15<38:18, 42.57s/it]Llama.generate: 21 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   16138.04 ms /   190 tokens (   84.94 ms per token,    11.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =   25890.32 ms /   128 runs   (  202.27 ms per token,     4.94 tokens per second)\n",
      "llama_perf_context_print:       total time =   42104.99 ms /   318 tokens\n",
      " 82%|████████████████████████████████████████████████████████████████              | 244/297 [4:10:58<37:29, 42.44s/it]Llama.generate: 21 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   16870.62 ms /   199 tokens (   84.78 ms per token,    11.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =   52061.50 ms /   255 runs   (  204.16 ms per token,     4.90 tokens per second)\n",
      "llama_perf_context_print:       total time =   69131.38 ms /   454 tokens\n",
      " 82%|████████████████████████████████████████████████████████████████▎             | 245/297 [4:12:07<43:43, 50.45s/it]Llama.generate: 21 prefix-match hit, remaining 489 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   43952.85 ms /   489 tokens (   89.88 ms per token,    11.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =   40915.83 ms /   192 runs   (  213.10 ms per token,     4.69 tokens per second)\n",
      "llama_perf_context_print:       total time =   85004.34 ms /   681 tokens\n",
      " 83%|████████████████████████████████████████████████████████████████▌             | 246/297 [4:13:32<51:42, 60.83s/it]Llama.generate: 21 prefix-match hit, remaining 176 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   14639.86 ms /   176 tokens (   83.18 ms per token,    12.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =   26633.67 ms /   130 runs   (  204.87 ms per token,     4.88 tokens per second)\n",
      "llama_perf_context_print:       total time =   41345.27 ms /   306 tokens\n",
      " 83%|████████████████████████████████████████████████████████████████▊             | 247/297 [4:14:13<45:49, 54.99s/it]Llama.generate: 21 prefix-match hit, remaining 325 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   27738.30 ms /   325 tokens (   85.35 ms per token,    11.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =   36073.64 ms /   177 runs   (  203.81 ms per token,     4.91 tokens per second)\n",
      "llama_perf_context_print:       total time =   63925.52 ms /   502 tokens\n",
      " 84%|█████████████████████████████████████████████████████████████████▏            | 248/297 [4:15:17<47:06, 57.68s/it]Llama.generate: 21 prefix-match hit, remaining 231 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   19481.94 ms /   231 tokens (   84.34 ms per token,    11.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =   52452.91 ms /   255 runs   (  205.70 ms per token,     4.86 tokens per second)\n",
      "llama_perf_context_print:       total time =   72129.22 ms /   486 tokens\n",
      " 84%|█████████████████████████████████████████████████████████████████▍            | 249/297 [4:16:29<49:37, 62.02s/it]Llama.generate: 21 prefix-match hit, remaining 250 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   21096.89 ms /   250 tokens (   84.39 ms per token,    11.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =   35378.90 ms /   174 runs   (  203.33 ms per token,     4.92 tokens per second)\n",
      "llama_perf_context_print:       total time =   56588.21 ms /   424 tokens\n",
      " 84%|█████████████████████████████████████████████████████████████████▋            | 250/297 [4:17:26<47:18, 60.40s/it]Llama.generate: 21 prefix-match hit, remaining 259 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   22460.53 ms /   259 tokens (   86.72 ms per token,    11.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =   19656.90 ms /    94 runs   (  209.12 ms per token,     4.78 tokens per second)\n",
      "llama_perf_context_print:       total time =   42167.07 ms /   353 tokens\n",
      " 85%|█████████████████████████████████████████████████████████████████▉            | 251/297 [4:18:08<42:07, 54.94s/it]Llama.generate: 21 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   16998.72 ms /   199 tokens (   85.42 ms per token,    11.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =   47837.05 ms /   235 runs   (  203.56 ms per token,     4.91 tokens per second)\n",
      "llama_perf_context_print:       total time =   65011.68 ms /   434 tokens\n",
      " 85%|██████████████████████████████████████████████████████████████████▏           | 252/297 [4:19:13<43:28, 57.97s/it]Llama.generate: 21 prefix-match hit, remaining 171 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   14281.49 ms /   171 tokens (   83.52 ms per token,    11.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =   14381.65 ms /    71 runs   (  202.56 ms per token,     4.94 tokens per second)\n",
      "llama_perf_context_print:       total time =   28699.04 ms /   242 tokens\n",
      " 85%|██████████████████████████████████████████████████████████████████▍           | 253/297 [4:19:42<36:04, 49.20s/it]Llama.generate: 21 prefix-match hit, remaining 325 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   28156.86 ms /   325 tokens (   86.64 ms per token,    11.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =   12289.28 ms /    61 runs   (  201.46 ms per token,     4.96 tokens per second)\n",
      "llama_perf_context_print:       total time =   40474.18 ms /   386 tokens\n",
      " 86%|██████████████████████████████████████████████████████████████████▋           | 254/297 [4:20:22<33:23, 46.59s/it]Llama.generate: 21 prefix-match hit, remaining 227 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   19563.32 ms /   227 tokens (   86.18 ms per token,    11.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =   42686.76 ms /   210 runs   (  203.27 ms per token,     4.92 tokens per second)\n",
      "llama_perf_context_print:       total time =   62393.89 ms /   437 tokens\n",
      " 86%|██████████████████████████████████████████████████████████████████▉           | 255/297 [4:21:25<35:56, 51.34s/it]Llama.generate: 21 prefix-match hit, remaining 183 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   15478.73 ms /   183 tokens (   84.58 ms per token,    11.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =   41838.10 ms /   205 runs   (  204.09 ms per token,     4.90 tokens per second)\n",
      "llama_perf_context_print:       total time =   57462.01 ms /   388 tokens\n",
      " 86%|███████████████████████████████████████████████████████████████████▏          | 256/297 [4:22:22<36:20, 53.18s/it]Llama.generate: 21 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   16377.12 ms /   196 tokens (   83.56 ms per token,    11.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5886.38 ms /    29 runs   (  202.98 ms per token,     4.93 tokens per second)\n",
      "llama_perf_context_print:       total time =   22276.87 ms /   225 tokens\n",
      " 87%|███████████████████████████████████████████████████████████████████▍          | 257/297 [4:22:45<29:16, 43.92s/it]Llama.generate: 21 prefix-match hit, remaining 248 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   21310.13 ms /   248 tokens (   85.93 ms per token,    11.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =   40818.02 ms /   201 runs   (  203.07 ms per token,     4.92 tokens per second)\n",
      "llama_perf_context_print:       total time =   62258.48 ms /   449 tokens\n",
      " 87%|███████████████████████████████████████████████████████████████████▊          | 258/297 [4:23:47<32:07, 49.43s/it]Llama.generate: 21 prefix-match hit, remaining 159 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   13187.68 ms /   159 tokens (   82.94 ms per token,    12.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8640.01 ms /    43 runs   (  200.93 ms per token,     4.98 tokens per second)\n",
      "llama_perf_context_print:       total time =   21846.58 ms /   202 tokens\n",
      " 87%|████████████████████████████████████████████████████████████████████          | 259/297 [4:24:09<26:04, 41.16s/it]Llama.generate: 21 prefix-match hit, remaining 246 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   21125.85 ms /   246 tokens (   85.88 ms per token,    11.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =   32753.79 ms /   163 runs   (  200.94 ms per token,     4.98 tokens per second)\n",
      "llama_perf_context_print:       total time =   53975.88 ms /   409 tokens\n",
      " 88%|████████████████████████████████████████████████████████████████████▎         | 260/297 [4:25:03<27:45, 45.01s/it]Llama.generate: 21 prefix-match hit, remaining 213 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   18662.14 ms /   213 tokens (   87.62 ms per token,    11.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =   18766.54 ms /    93 runs   (  201.79 ms per token,     4.96 tokens per second)\n",
      "llama_perf_context_print:       total time =   37476.16 ms /   306 tokens\n",
      " 88%|████████████████████████████████████████████████████████████████████▌         | 261/297 [4:25:40<25:39, 42.76s/it]Llama.generate: 21 prefix-match hit, remaining 267 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   22679.41 ms /   267 tokens (   84.94 ms per token,    11.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =   25199.67 ms /   123 runs   (  204.88 ms per token,     4.88 tokens per second)\n",
      "llama_perf_context_print:       total time =   47945.68 ms /   390 tokens\n",
      " 88%|████████████████████████████████████████████████████████████████████▊         | 262/297 [4:26:28<25:51, 44.32s/it]Llama.generate: 21 prefix-match hit, remaining 155 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   13021.66 ms /   155 tokens (   84.01 ms per token,    11.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =   26734.34 ms /   131 runs   (  204.08 ms per token,     4.90 tokens per second)\n",
      "llama_perf_context_print:       total time =   39835.52 ms /   286 tokens\n",
      " 89%|█████████████████████████████████████████████████████████████████████         | 263/297 [4:27:08<24:21, 42.99s/it]Llama.generate: 22 prefix-match hit, remaining 282 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   24092.93 ms /   282 tokens (   85.44 ms per token,    11.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =   11887.21 ms /    57 runs   (  208.55 ms per token,     4.80 tokens per second)\n",
      "llama_perf_context_print:       total time =   36006.07 ms /   339 tokens\n",
      " 89%|█████████████████████████████████████████████████████████████████████▎        | 264/297 [4:27:44<22:29, 40.90s/it]Llama.generate: 21 prefix-match hit, remaining 216 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   18531.76 ms /   216 tokens (   85.80 ms per token,    11.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7362.62 ms /    36 runs   (  204.52 ms per token,     4.89 tokens per second)\n",
      "llama_perf_context_print:       total time =   25909.95 ms /   252 tokens\n",
      " 89%|█████████████████████████████████████████████████████████████████████▌        | 265/297 [4:28:10<19:25, 36.41s/it]Llama.generate: 21 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   18585.08 ms /   207 tokens (   89.78 ms per token,    11.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =   51869.56 ms /   255 runs   (  203.41 ms per token,     4.92 tokens per second)\n",
      "llama_perf_context_print:       total time =   70655.04 ms /   462 tokens\n",
      " 90%|█████████████████████████████████████████████████████████████████████▊        | 266/297 [4:29:21<24:07, 46.69s/it]Llama.generate: 21 prefix-match hit, remaining 278 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   23487.94 ms /   278 tokens (   84.49 ms per token,    11.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =   43491.98 ms /   210 runs   (  207.10 ms per token,     4.83 tokens per second)\n",
      "llama_perf_context_print:       total time =   67128.42 ms /   488 tokens\n",
      " 90%|██████████████████████████████████████████████████████████████████████        | 267/297 [4:30:28<26:24, 52.83s/it]Llama.generate: 21 prefix-match hit, remaining 125 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   10331.80 ms /   125 tokens (   82.65 ms per token,    12.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =   37945.31 ms /   192 runs   (  197.63 ms per token,     5.06 tokens per second)\n",
      "llama_perf_context_print:       total time =   48399.06 ms /   317 tokens\n",
      " 90%|██████████████████████████████████████████████████████████████████████▍       | 268/297 [4:31:16<24:53, 51.51s/it]Llama.generate: 21 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   17088.25 ms /   203 tokens (   84.18 ms per token,    11.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =   24581.77 ms /   120 runs   (  204.85 ms per token,     4.88 tokens per second)\n",
      "llama_perf_context_print:       total time =   41735.04 ms /   323 tokens\n",
      " 91%|██████████████████████████████████████████████████████████████████████▋       | 269/297 [4:31:58<22:40, 48.59s/it]Llama.generate: 21 prefix-match hit, remaining 240 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   20338.67 ms /   240 tokens (   84.74 ms per token,    11.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =   51333.64 ms /   255 runs   (  201.31 ms per token,     4.97 tokens per second)\n",
      "llama_perf_context_print:       total time =   71866.41 ms /   495 tokens\n",
      " 91%|██████████████████████████████████████████████████████████████████████▉       | 270/297 [4:33:10<25:00, 55.58s/it]Llama.generate: 21 prefix-match hit, remaining 250 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   22185.92 ms /   250 tokens (   88.74 ms per token,    11.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =   38283.92 ms /   188 runs   (  203.64 ms per token,     4.91 tokens per second)\n",
      "llama_perf_context_print:       total time =   60594.58 ms /   438 tokens\n",
      " 91%|███████████████████████████████████████████████████████████████████████▏      | 271/297 [4:34:11<24:44, 57.09s/it]Llama.generate: 21 prefix-match hit, remaining 282 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   23917.07 ms /   282 tokens (   84.81 ms per token,    11.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6321.16 ms /    31 runs   (  203.91 ms per token,     4.90 tokens per second)\n",
      "llama_perf_context_print:       total time =   30250.17 ms /   313 tokens\n",
      " 92%|███████████████████████████████████████████████████████████████████████▍      | 272/297 [4:34:41<20:26, 49.05s/it]Llama.generate: 21 prefix-match hit, remaining 231 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   20085.59 ms /   231 tokens (   86.95 ms per token,    11.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8040.60 ms /    39 runs   (  206.17 ms per token,     4.85 tokens per second)\n",
      "llama_perf_context_print:       total time =   28144.48 ms /   270 tokens\n",
      " 92%|███████████████████████████████████████████████████████████████████████▋      | 273/297 [4:35:09<17:06, 42.79s/it]Llama.generate: 21 prefix-match hit, remaining 265 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   22843.32 ms /   265 tokens (   86.20 ms per token,    11.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =   32892.55 ms /   160 runs   (  205.58 ms per token,     4.86 tokens per second)\n",
      "llama_perf_context_print:       total time =   55835.21 ms /   425 tokens\n",
      " 92%|███████████████████████████████████████████████████████████████████████▉      | 274/297 [4:36:05<17:54, 46.71s/it]Llama.generate: 21 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   17169.68 ms /   203 tokens (   84.58 ms per token,    11.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6716.12 ms /    33 runs   (  203.52 ms per token,     4.91 tokens per second)\n",
      "llama_perf_context_print:       total time =   23900.19 ms /   236 tokens\n",
      " 93%|████████████████████████████████████████████████████████████████████████▏     | 275/297 [4:36:29<14:37, 39.87s/it]Llama.generate: 21 prefix-match hit, remaining 259 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   22650.96 ms /   259 tokens (   87.46 ms per token,    11.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5385.99 ms /    26 runs   (  207.15 ms per token,     4.83 tokens per second)\n",
      "llama_perf_context_print:       total time =   28048.52 ms /   285 tokens\n",
      " 93%|████████████████████████████████████████████████████████████████████████▍     | 276/297 [4:36:57<12:43, 36.33s/it]Llama.generate: 21 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   17903.26 ms /   206 tokens (   86.91 ms per token,    11.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =   26258.02 ms /   131 runs   (  200.44 ms per token,     4.99 tokens per second)\n",
      "llama_perf_context_print:       total time =   44235.35 ms /   337 tokens\n",
      " 93%|████████████████████████████████████████████████████████████████████████▋     | 277/297 [4:37:41<12:54, 38.71s/it]Llama.generate: 21 prefix-match hit, remaining 270 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   23186.44 ms /   270 tokens (   85.88 ms per token,    11.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8936.59 ms /    43 runs   (  207.83 ms per token,     4.81 tokens per second)\n",
      "llama_perf_context_print:       total time =   32142.63 ms /   313 tokens\n",
      " 94%|█████████████████████████████████████████████████████████████████████████     | 278/297 [4:38:13<11:38, 36.75s/it]Llama.generate: 21 prefix-match hit, remaining 281 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   24346.79 ms /   281 tokens (   86.64 ms per token,    11.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =   38881.20 ms /   189 runs   (  205.72 ms per token,     4.86 tokens per second)\n",
      "llama_perf_context_print:       total time =   63350.98 ms /   470 tokens\n",
      " 94%|█████████████████████████████████████████████████████████████████████████▎    | 279/297 [4:39:17<13:25, 44.74s/it]Llama.generate: 21 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   16897.31 ms /   201 tokens (   84.07 ms per token,    11.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7649.45 ms /    38 runs   (  201.30 ms per token,     4.97 tokens per second)\n",
      "llama_perf_context_print:       total time =   24563.20 ms /   239 tokens\n",
      " 94%|█████████████████████████████████████████████████████████████████████████▌    | 280/297 [4:39:41<10:57, 38.69s/it]Llama.generate: 21 prefix-match hit, remaining 227 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   19614.52 ms /   227 tokens (   86.41 ms per token,    11.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7125.84 ms /    35 runs   (  203.60 ms per token,     4.91 tokens per second)\n",
      "llama_perf_context_print:       total time =   26756.27 ms /   262 tokens\n",
      " 95%|█████████████████████████████████████████████████████████████████████████▊    | 281/297 [4:40:08<09:21, 35.12s/it]Llama.generate: 21 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   16859.09 ms /   196 tokens (   86.02 ms per token,    11.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =   39554.76 ms /   196 runs   (  201.81 ms per token,     4.96 tokens per second)\n",
      "llama_perf_context_print:       total time =   56543.49 ms /   392 tokens\n",
      " 95%|██████████████████████████████████████████████████████████████████████████    | 282/297 [4:41:05<10:23, 41.56s/it]Llama.generate: 22 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   16381.50 ms /   192 tokens (   85.32 ms per token,    11.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =   24006.40 ms /   119 runs   (  201.73 ms per token,     4.96 tokens per second)\n",
      "llama_perf_context_print:       total time =   40453.57 ms /   311 tokens\n",
      " 95%|██████████████████████████████████████████████████████████████████████████▎   | 283/297 [4:41:45<09:37, 41.23s/it]Llama.generate: 21 prefix-match hit, remaining 293 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   25130.60 ms /   293 tokens (   85.77 ms per token,    11.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6747.68 ms /    32 runs   (  210.86 ms per token,     4.74 tokens per second)\n",
      "llama_perf_context_print:       total time =   31891.49 ms /   325 tokens\n",
      " 96%|██████████████████████████████████████████████████████████████████████████▌   | 284/297 [4:42:17<08:19, 38.44s/it]Llama.generate: 21 prefix-match hit, remaining 224 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   19229.70 ms /   224 tokens (   85.85 ms per token,    11.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =   52623.61 ms /   255 runs   (  206.37 ms per token,     4.85 tokens per second)\n",
      "llama_perf_context_print:       total time =   72056.33 ms /   479 tokens\n",
      " 96%|██████████████████████████████████████████████████████████████████████████▊   | 285/297 [4:43:29<09:42, 48.53s/it]Llama.generate: 21 prefix-match hit, remaining 279 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   23866.43 ms /   279 tokens (   85.54 ms per token,    11.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =   22449.93 ms /   109 runs   (  205.96 ms per token,     4.86 tokens per second)\n",
      "llama_perf_context_print:       total time =   46372.70 ms /   388 tokens\n",
      " 96%|███████████████████████████████████████████████████████████████████████████   | 286/297 [4:44:16<08:46, 47.89s/it]Llama.generate: 21 prefix-match hit, remaining 184 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   15414.45 ms /   184 tokens (   83.77 ms per token,    11.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =   18634.73 ms /    91 runs   (  204.78 ms per token,     4.88 tokens per second)\n",
      "llama_perf_context_print:       total time =   34095.52 ms /   275 tokens\n",
      " 97%|███████████████████████████████████████████████████████████████████████████▎  | 287/297 [4:44:50<07:17, 43.76s/it]Llama.generate: 21 prefix-match hit, remaining 209 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   17899.19 ms /   209 tokens (   85.64 ms per token,    11.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10278.59 ms /    51 runs   (  201.54 ms per token,     4.96 tokens per second)\n",
      "llama_perf_context_print:       total time =   28201.25 ms /   260 tokens\n",
      " 97%|███████████████████████████████████████████████████████████████████████████▋  | 288/297 [4:45:18<05:51, 39.10s/it]Llama.generate: 21 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   16556.93 ms /   194 tokens (   85.34 ms per token,    11.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =   42693.69 ms /   209 runs   (  204.28 ms per token,     4.90 tokens per second)\n",
      "llama_perf_context_print:       total time =   59400.98 ms /   403 tokens\n",
      " 97%|███████████████████████████████████████████████████████████████████████████▉  | 289/297 [4:46:17<06:01, 45.20s/it]Llama.generate: 21 prefix-match hit, remaining 240 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   20312.37 ms /   240 tokens (   84.63 ms per token,    11.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =   19676.90 ms /    98 runs   (  200.78 ms per token,     4.98 tokens per second)\n",
      "llama_perf_context_print:       total time =   40041.54 ms /   338 tokens\n",
      " 98%|████████████████████████████████████████████████████████████████████████████▏ | 290/297 [4:46:57<05:05, 43.66s/it]Llama.generate: 21 prefix-match hit, remaining 299 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   26207.91 ms /   299 tokens (   87.65 ms per token,    11.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =   36919.83 ms /   177 runs   (  208.59 ms per token,     4.79 tokens per second)\n",
      "llama_perf_context_print:       total time =   63241.77 ms /   476 tokens\n",
      " 98%|████████████████████████████████████████████████████████████████████████████▍ | 291/297 [4:48:01<04:57, 49.54s/it]Llama.generate: 21 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   16765.68 ms /   200 tokens (   83.83 ms per token,    11.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10153.76 ms /    51 runs   (  199.09 ms per token,     5.02 tokens per second)\n",
      "llama_perf_context_print:       total time =   26940.72 ms /   251 tokens\n",
      " 98%|████████████████████████████████████████████████████████████████████████████▋ | 292/297 [4:48:28<03:33, 42.77s/it]Llama.generate: 21 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   16001.77 ms /   189 tokens (   84.67 ms per token,    11.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =   27191.96 ms /   133 runs   (  204.45 ms per token,     4.89 tokens per second)\n",
      "llama_perf_context_print:       total time =   43271.54 ms /   322 tokens\n",
      " 99%|████████████████████████████████████████████████████████████████████████████▉ | 293/297 [4:49:11<02:51, 42.93s/it]Llama.generate: 21 prefix-match hit, remaining 264 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   22312.05 ms /   264 tokens (   84.52 ms per token,    11.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =   11551.35 ms /    57 runs   (  202.66 ms per token,     4.93 tokens per second)\n",
      "llama_perf_context_print:       total time =   33887.42 ms /   321 tokens\n",
      " 99%|█████████████████████████████████████████████████████████████████████████████▏| 294/297 [4:49:45<02:00, 40.23s/it]Llama.generate: 21 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   16347.68 ms /   191 tokens (   85.59 ms per token,    11.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =   13937.18 ms /    70 runs   (  199.10 ms per token,     5.02 tokens per second)\n",
      "llama_perf_context_print:       total time =   30320.49 ms /   261 tokens\n",
      " 99%|█████████████████████████████████████████████████████████████████████████████▍| 295/297 [4:50:15<01:14, 37.26s/it]Llama.generate: 21 prefix-match hit, remaining 228 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   19731.43 ms /   228 tokens (   86.54 ms per token,    11.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =   44962.18 ms /   224 runs   (  200.72 ms per token,     4.98 tokens per second)\n",
      "llama_perf_context_print:       total time =   64849.24 ms /   452 tokens\n",
      "100%|█████████████████████████████████████████████████████████████████████████████▋| 296/297 [4:51:20<00:45, 45.55s/it]Llama.generate: 21 prefix-match hit, remaining 223 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   18728.15 ms /   223 tokens (   83.98 ms per token,    11.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =   20481.08 ms /    99 runs   (  206.88 ms per token,     4.83 tokens per second)\n",
      "llama_perf_context_print:       total time =   39257.16 ms /   322 tokens\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 297/297 [4:51:59<00:00, 58.99s/it]\n"
     ]
    }
   ],
   "source": [
    "for item in tqdm.tqdm(data):\n",
    "    query = item[\"question\"]\n",
    "    top_chunks,i = retrieve_relevant_chunks(query,model = model,index = index, chunks = all_chunks, top_k=5)\n",
    "\n",
    "    prompt = build_prompt(query, top_chunks)\n",
    "\n",
    "    token_ids = llm.tokenize(prompt.encode(\"utf-8\"))\n",
    "\n",
    "\n",
    "    response = llm(prompt, max_tokens=256, temperature=0.7, stop=[\"###\"])\n",
    "    answer = response[\"choices\"][0][\"text\"].strip()\n",
    "    updated_qa_data.append({\n",
    "        \"question\": query,\n",
    "        \"answer\": answer\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72959884-382b-4515-b92e-e9320526d4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Answers generated and saved to 'updated_qa_pairs.json'.\n"
     ]
    }
   ],
   "source": [
    "with open(\"updated_qa_pairs.json\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "    json.dump(updated_qa_data, out_file, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\" Answers generated and saved to 'updated_qa_pairs.json'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "346b7f43-62f2-4c53-ad2c-3619eef4baaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total QA pairs: 297\n",
      "Sample QA pair: {'question': 'What collaborative initiative was announced by the European Microfinance Network (EMN) and the Microfinance Centre (MFC) in April 2024, and what are its main objectives for supporting the European microfinance sector?', 'answer': \"The European Microfinance Network (EMN) and the Microfinance Centre (MFC) announced a collaborative initiative in April 2024 to capture data from the vast majority of European microfinance institutions. The main objectives of this initiative are to provide the most comprehensive dataset available on the sector today and to remain the leading source of data and analysis on the microfinance sector in Europe. This evaluation is undertaken as part of the Commission's commitment to evidence-based policy making under the Better Regulation policy.\"}\n"
     ]
    }
   ],
   "source": [
    "with open(\"updated_qa_pairs.json\", \"r\", encoding=\"utf-8\") as in_file:\n",
    "    updated_qa_data = json.load(in_file)\n",
    "\n",
    "print(\"Total QA pairs:\", len(updated_qa_data))\n",
    "print(\"Sample QA pair:\", updated_qa_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858fdab6-c5e7-4936-be7c-d3f25c3894f6",
   "metadata": {},
   "source": [
    "Evaluation Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "46fcc4f4-9eae-48a9-85a0-c04c85d8249e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 297/297 [00:23<00:00, 12.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Average BLEU Score (1-gram): 0.1933\n",
      "🔹 Average Semantic Similarity: 0.7523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "assert len(data) == len(updated_qa_data), \"Mismatch in number of QA pairs\"\n",
    "\n",
    "bleu_scores = []\n",
    "similarity_scores = []\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "for gpt_item, my_item in tqdm.tqdm(zip(data, updated_qa_data), total=len(data)):\n",
    "    reference = gpt_item['answer'].strip()\n",
    "    hypothesis = my_item['answer'].strip()\n",
    "\n",
    "    # BLEU \n",
    "    smoothie = SmoothingFunction().method4\n",
    "    bleu = sentence_bleu([reference.split()], hypothesis.split(), weights=(1, 0, 0, 0), smoothing_function=smoothie)\n",
    "    bleu_scores.append(bleu)\n",
    "\n",
    "    # Semantic Similarity (cosine similarity)\n",
    "    emb_ref = model.encode(reference, convert_to_tensor=True)\n",
    "    emb_hyp = model.encode(hypothesis, convert_to_tensor=True)\n",
    "    sim_score = util.pytorch_cos_sim(emb_ref, emb_hyp).item()\n",
    "    similarity_scores.append(sim_score)\n",
    "\n",
    "\n",
    "average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "average_similarity = sum(similarity_scores) / len(similarity_scores)\n",
    "\n",
    "print(f\"🔹 Average BLEU Score (1-gram): {average_bleu:.4f}\")\n",
    "print(f\"🔹 Average Semantic Similarity: {average_similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "702ed207-61e1-4647-a575-1a4f72a318e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shri\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "131da4e0-d948-4e80-8eb4-c7a525a1dead",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 297/297 [00:25<00:00, 11.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Average METEOR: 0.2599\n",
      "🔹 Average ROUGE-1: 0.3275\n",
      "🔹 Average ROUGE-2: 0.0954\n",
      "🔹 Average ROUGE-L: 0.2045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "meteor_scores = []\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougeL_scores = []\n",
    "\n",
    "for gpt, mine in tqdm.tqdm(zip(data, updated_qa_data), total=len(data)):\n",
    "    ref = gpt[\"answer\"].strip()\n",
    "    hyp = mine[\"answer\"].strip()\n",
    "\n",
    "    # METEOR\n",
    "    meteor = single_meteor_score(word_tokenize(ref), word_tokenize(hyp))\n",
    "    meteor_scores.append(meteor)\n",
    "\n",
    "    # ROUGE\n",
    "    scores = rouge.score(ref, hyp)\n",
    "    rouge1_scores.append(scores[\"rouge1\"].fmeasure)\n",
    "    rouge2_scores.append(scores[\"rouge2\"].fmeasure)\n",
    "    rougeL_scores.append(scores[\"rougeL\"].fmeasure)\n",
    "\n",
    "print(\"🔹 Average METEOR:\", round(sum(meteor_scores) / len(meteor_scores), 4))\n",
    "print(\"🔹 Average ROUGE-1:\", round(sum(rouge1_scores) / len(rouge1_scores), 4))\n",
    "print(\"🔹 Average ROUGE-2:\", round(sum(rouge2_scores) / len(rouge2_scores), 4))\n",
    "print(\"🔹 Average ROUGE-L:\", round(sum(rougeL_scores) / len(rougeL_scores), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2697ca33-9b0d-4c71-bb70-b25a9c206754",
   "metadata": {},
   "source": [
    "Let's see some reference and generated answers side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e5405894-b519-4c0c-a28c-4be470a7deb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔢 Pair 1\n",
      "❓ Question: What collaborative initiative was announced by the European Microfinance Network (EMN) and the Microfinance Centre (MFC) in April 2024, and what are its main objectives for supporting the European microfinance sector?\n",
      "✅ Reference Answer (ChatGPT):\n",
      "In April 2024, the European Microfinance Network (EMN) and the Microfinance Centre (MFC) announced a joint strategy aimed at strengthening the European microfinance sector. This collaborative initiative focuses on promoting financial inclusion, developing capacity-building resources, and creating a unified voice to influence policy-making at the European level. The partnership aims to better support microfinance institutions and expand access to responsible finance for underserved populations across Europe.\n",
      "🤖 Generated Answer (Model):\n",
      "The European Microfinance Network (EMN) and the Microfinance Centre (MFC) announced a collaborative initiative in April 2024 to capture data from the vast majority of European microfinance institutions. The main objectives of this initiative are to provide the most comprehensive dataset available on the sector today and to remain the leading source of data and analysis on the microfinance sector in Europe. This evaluation is undertaken as part of the Commission's commitment to evidence-based policy making under the Better Regulation policy.\n",
      "--------------------------------------------------------------------------------\n",
      "🔢 Pair 2\n",
      "❓ Question: How did the AccessibleEU initiative exceed its yearly objectives in promoting accessibility for persons with disabilities across Europe?\n",
      "✅ Reference Answer (ChatGPT):\n",
      "The AccessibleEU initiative surpassed its yearly objectives by significantly advancing accessibility measures across European Union member states. It achieved this through extensive stakeholder engagement, implementation of accessibility audits, development of best practice toolkits, and organization of awareness campaigns. The initiative focused on inclusive design, policy harmonization, and improved accessibility in public infrastructure, thereby contributing to a more inclusive environment for persons with disabilities throughout Europe.\n",
      "🤖 Generated Answer (Model):\n",
      "The AccessibleEU initiative exceeded its yearly objectives in promoting accessibility for persons with disabilities across Europe by successfully hosting 88 events, growing its Community of Practitioners to over 3,400 members, establishing a Moodle forum, and keeping building its digital library with 163 new Good Practices and 136 new references on accessibility. Additionally, the entry into force of the European Accessibility Act (EU Directive 2019/882) marks an important step towards achieving a truly accessible Europe.\n",
      "--------------------------------------------------------------------------------\n",
      "🔢 Pair 3\n",
      "❓ Question: What key policy development took place on July 3, 2025, involving the European Commission and its implications for AI regulation in the EU?\n",
      "✅ Reference Answer (ChatGPT):\n",
      "On July 3, 2025, the European Commission and the European Parliament reached a political agreement on the AI Act, a landmark piece of legislation aimed at regulating artificial intelligence technologies across the EU. This act sets out a risk-based framework, classifying AI systems by their potential impact on fundamental rights and safety. It introduces strict obligations for high-risk AI applications, including requirements for transparency, accountability, and human oversight. The policy aims to foster trust in AI while supporting innovation and ensuring compliance with EU values.\n",
      "🤖 Generated Answer (Model):\n",
      "On July 3, 2025, the European Commission published its first AI strategy, which aimed to position Europe as a global leader in AI research and development. The strategy included several key policy developments, including the establishment of an AI ethics committee to ensure that AI is developed and used in a responsible and ethical manner. This committee would be tasked with developing guidelines and standards for AI, as well as addressing concerns around privacy, security, and bias.\n",
      "\n",
      "The strategy also included plans to invest in AI research and development, as well as to create new jobs in the field. The European Commission committed to investing €20 billion in AI research and development over the next seven years, and to creating 200,000 new jobs in the field.\n",
      "\n",
      "Overall, the European Commission's first AI strategy represents a significant effort to address the challenges and opportunities presented by AI, and to position Europe as a global leader in the field.\n",
      "--------------------------------------------------------------------------------\n",
      "🔢 Pair 4\n",
      "❓ Question: What environmentally sustainable measures has Hadag, Hamburg’s ferry service operator, implemented to modernize its fleet and reduce emissions?\n",
      "✅ Reference Answer (ChatGPT):\n",
      "Hadag, the ferry service operator in Hamburg, has undertaken a modernization program that includes the introduction of hybrid and fully electric ferries to its fleet. These environmentally sustainable measures are designed to reduce greenhouse gas emissions and improve energy efficiency in public transport. The initiative aligns with Hamburg’s broader climate strategy and emphasizes the shift towards low-emission urban mobility solutions in maritime transportation.\n",
      "🤖 Generated Answer (Model):\n",
      "Hadag, Hamburg's ferry service operator, has implemented the delivery of three electric ferries in 2024 and has more on the way, which will enable it to operate a fully zero-emissions fleet. Additionally, the company is utilizing optimisation algorithms and cloud-based planning tools to position itself as a leader in sustainable, water-based public transport. The PM² Methodologies are also being highlighted as a valuable addition to the agenda, emphasizing the importance of project management skills in delivering impactful solutions across all sectors.\n",
      "--------------------------------------------------------------------------------\n",
      "🔢 Pair 5\n",
      "❓ Question: What actions did the Israeli government undertake following recent Cabinet resolutions concerning the security situation in the Gaza Strip?\n",
      "✅ Reference Answer (ChatGPT):\n",
      "Following recent Cabinet resolutions, the Israeli government implemented heightened security operations targeting militant infrastructure in the Gaza Strip. These actions included airstrikes on weapon manufacturing sites and border surveillance enhancements. The operations were positioned as a response to ongoing threats and aimed at restoring stability and deterring further escalation in the region.\n",
      "🤖 Generated Answer (Model):\n",
      "The Israeli government undertook several actions following recent Cabinet resolutions concerning the security situation in the Gaza Strip. These actions include an increase in daily trucks for food and non-food items to enter Gaza, the opening of several other crossing points in both the northern and southern areas, the reopening of the Jordanian and Egyptian aid routes, enabling the distribution of food supplies through bakeries and public kitchens throughout the Gaza strip, the resumption of fuel deliveries for use by humanitarian facilities up to an operational level, the protection of aid workers, and the repair and facilitation of works on vital infrastructure like the resumption of the power supply to the water desalination facility.\n",
      "--------------------------------------------------------------------------------\n",
      "🔢 Pair 6\n",
      "❓ Question: What is the purpose of the EU Flight Emissions Label (FEL) initiative involving the European Union Aviation Safety Agency (EASA) and Air France-KLM, and how does it aim to enhance transparency and sustainability in European aviation?\n",
      "✅ Reference Answer (ChatGPT):\n",
      "The EU Flight Emissions Label (FEL) initiative is a collaborative effort between the European Union Aviation Safety Agency (EASA) and Air France-KLM aimed at enhancing transparency in flight emissions reporting. Under a Memorandum of Cooperation signed in 2024, Air France and KLM will pilot the FEL scheme to provide reliable, harmonized data on flight emissions based on factors like aircraft type, passenger load, freight volume, and fuel type. The goal is to allow passengers to make more informed choices, protect them from greenwashing, and promote the adoption of sustainable aviation fuels, thereby reinforcing decarbonization and fair competition within the EU aviation market.\n",
      "🤖 Generated Answer (Model):\n",
      "The EU Flight Emissions Label (FEL) initiative involves the European Union Aviation Safety Agency (EASA) and Air France-KLM, and its purpose is to enhance transparency and sustainability in European aviation. The FEL scheme allows airlines to display their flight emissions data and use these estimates when, for example, offering sustainable aviation fuel or other emissions offsets to passengers. This initiative encourages airlines to be more transparent about their carbon footprint and to take steps towards reducing their emissions. The Memorandum of Cooperation signed by EASA and Air France-KLM includes support from EASA experts to guide the airline group through the onboarding process.\n",
      "--------------------------------------------------------------------------------\n",
      "🔢 Pair 7\n",
      "❓ Question: What were the key findings and contributions of the two-year research initiative on circular economy monitoring, and how many indicators were ultimately selected and tested?\n",
      "✅ Reference Answer (ChatGPT):\n",
      "The two-year research initiative on circular economy monitoring resulted in a comprehensive set of options for tracking circularity across multiple sectors, including batteries, vehicles, electronics, bioeconomy, food, construction, plastics, and textiles. Out of over 730 potential indicators catalogued, 60 were selected and tested through 19 detailed case studies. These indicators support policy development by offering scalable monitoring tools at regional, national, and international levels, and across households, companies, and industries. The project aimed to strengthen understanding and implementation of circular economy policies and tools to address resource use and climate challenges.\n",
      "🤖 Generated Answer (Model):\n",
      "The two-year research initiative on circular economy monitoring concluded with a set of key findings and contributions that advanced our understanding of effective circular economy policies and support tools. The initiative focused on complementary and expanding aspects of existing monitoring efforts, such as the Circular Economy Monitoring Framework, to develop a comprehensive set of options for monitoring circularity across critical policy areas and sectors. Ultimately, 42 indicators were selected and tested as a result of the research project.\n",
      "--------------------------------------------------------------------------------\n",
      "🔢 Pair 8\n",
      "❓ Question: Why does the European Union continue to commemorate the genocide in Srebrenica, and what is its stance on historical denial and reconciliation in the Western Balkans?\n",
      "✅ Reference Answer (ChatGPT):\n",
      "The European Union commemorates the genocide in Srebrenica to honor the memory of over 8,300 Bosniak men and boys who were killed in July 1995 and to support the survivors and families still affected by the tragedy. The EU emphasizes the importance of preserving historical truth, recognizing its past responsibility in failing to prevent the atrocity, and firmly condemns any denial, distortion, or glorification of war crimes. The EU urges political leaders in Bosnia and Herzegovina and the Western Balkans to acknowledge established facts, support genuine reconciliation, and work towards a peaceful and unified future. It reaffirms its commitment to Bosnia and Herzegovina's path toward EU membership.\n",
      "🤖 Generated Answer (Model):\n",
      "The European Union continues to commemorate the genocide in Srebrenica because it was a significant event in the history of the Western Balkans and it represents one of the darkest moments in European history. The European Union firmly rejects and condemns any denial, distortion, or minimisation of the Srebrenica genocide, as well as the glorification of war criminals.\n",
      "\n",
      "In terms of historical denial and reconciliation in the Western Balkans, the European Union believes that it is essential to confront the roots of hatred that led to these atrocities in order to promote reconciliation and peace in the region. The European Union stands with the people of Bosnia and Herzegovina and supports their efforts to promote reconciliation and peace. The European Union also encourages dialogue and cooperation between different communities in the region to promote a shared future for all. The European Union recognises that historical denial and hatred are major barriers to reconciliation and peace and believes that it is essential to address these issues in order to promote a stable and prosperous future for the Western Balkans.\n",
      "--------------------------------------------------------------------------------\n",
      "🔢 Pair 9\n",
      "❓ Question: What changes were introduced in the European Commission's 'quick fix' amendments to the European Sustainability Reporting Standards (ESRS), and how do these impact wave one companies reporting for financial years 2025 and 2026?\n",
      "✅ Reference Answer (ChatGPT):\n",
      "The European Commission introduced 'quick fix' amendments to the European Sustainability Reporting Standards (ESRS) to ease the burden on 'wave one' companies that began reporting in financial year 2024. The amendments allow these companies to omit certain disclosures, such as anticipated financial impacts of sustainability-related risks, not only for 2024 but also for 2025 and 2026. The changes align wave one companies with the phase-in provisions granted to smaller firms and address the fact that they were excluded from the 'stop-the-clock' Directive that deferred requirements for 'wave two' and 'wave three' companies. A broader ESRS revision is underway, aiming for simplification and consistency by 2027.\n",
      "🤖 Generated Answer (Model):\n",
      "The European Commission has adopted targeted \"quick fix\" amendments to the first set of European Sustainability Reporting Standards (ESRS), which impact wave one companies reporting for financial years 2025 and 2026. These amendments aim to clarify certain provisions and reduce the number of data requirements, which will help simplify reporting requirements for companies. Additionally, the Commission is working on a broader revision of the ESRS, with the goal of improving consistency with other legislation and reducing the number of data requirements further. This revision will be implemented in the future, but it is not clear when it will be finalized and implemented. Overall, the changes introduced in the \"quick fix\" amendments and future revisions to the ESRS aim to improve the clarity and simplicity of sustainability reporting requirements for companies.\n",
      "--------------------------------------------------------------------------------\n",
      "🔢 Pair 10\n",
      "❓ Question: What led to the imposition of anti-dumping duties on lysine imports from China, and what are the expected effects on the EU industry and environmental sustainability?\n",
      "✅ Reference Answer (ChatGPT):\n",
      "The EU imposed anti-dumping duties on lysine imports from China after an investigation revealed that these imports were being sold at unfairly low prices, harming European producers. Lysine, a critical amino acid used in animal feed, pharmaceuticals, and dietary supplements, plays an essential role in improving animal nutrition and reducing environmental impacts such as nitrogen pollution. The duties are expected to level the playing field for EU lysine manufacturers and promote fair competition while supporting environmentally sustainable agricultural practices in the EU.\n",
      "🤖 Generated Answer (Model):\n",
      "The imposition of anti-dumping duties on lysine imports from China was the result of an investigation that found that these imports were harming EU industry. The investigation revealed that the Chinese companies were selling lysine at artificially low prices, which made it difficult for EU lysine makers to compete. The duties imposed today were expected to help EU lysine makers to compete on a more equal footing with their Chinese counterparts.\n",
      "\n",
      "The decision to impose duties on lysine imports from China also had implications for environmental sustainability. Dumping of lysine from China led to environmental pollution in the EU, and the imposition of duties could help to protect the environment by reducing the amount of imported lysine on the market.\n",
      "\n",
      "However, it is important to note that the imposition of duties could also have unintended consequences. It is possible that the reduced availability of imported lysine could lead to higher prices for EU consumers, and it is also possible that the reduction in imports could lead to a decrease in competition, which could be harmful to innovation and technological progress.\n",
      "\n",
      "Overall, the imposition of anti-dumping duties on lysine imports from China reflects the\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i, (ref, model) in enumerate(zip(data, updated_qa_data)):\n",
    "    print(f\" Pair {i+1}\")\n",
    "    print(f\" Question: {ref['question']}\")\n",
    "    print(f\" Reference Answer (ChatGPT):\\n{ref['answer']}\")\n",
    "    print(f\" Generated Answer (Model):\\n{model['answer']}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    if i == 9:\n",
    "        break  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a176d3a0-cb3d-4a7c-95f0-70cada0003c0",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "* This model has low Rogue BLUE and METEOR score but decent similarity score.Means, the model generates semantically similar answers but does not generates correct phrases.\n",
    "* Sometimes the information is incorrect also.\n",
    "* And sometime the model deviates from the actual question.\n",
    "* Overall similarity is good.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e36148-662d-49f4-9968-69b6c478acbd",
   "metadata": {},
   "source": [
    "Now, let's try with the next base-V2 model encodings, 500 word length chunks and the same mistral 7B model as chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc886fc2-8fa4-4253-aec2-6789be39f63f",
   "metadata": {},
   "source": [
    "Let's first try with 10 outputs only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c16e137-e297-49c0-b55e-1aa88b6de2b8",
   "metadata": {},
   "source": [
    "Loading model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d834bca8-fa8d-430e-ac21-bd38dbae2638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ GPU not available, using CPU.\n"
     ]
    }
   ],
   "source": [
    "model_2 = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model_2 = model_2.to('cuda')\n",
    "    print(\"✅ Model loaded to GPU.\")\n",
    "else:\n",
    "    print(\"⚠️ GPU not available, using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ab8a60-b345-4351-b4bf-69685f4151cf",
   "metadata": {},
   "source": [
    "Second index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "35cc4cd4-ebcb-4933-ab81-67c1e984f0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_2 = faiss.read_index(\"chunk_index_v2_cosine_x.faiss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aa4d7e-d91a-4369-a803-085725d0db48",
   "metadata": {},
   "source": [
    "Second Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "92208929-eec9-44d1-8f18-a9f43dfeb76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_chunks = []\n",
    "with open(\"filtered_chunks_v2_50.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        loaded_chunks.append(json.loads(line)[\"chunk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e087a26f-5796-4cb9-8aaa-d31d9ce62fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 3842\n",
      "Sample chunk: The European Microfinance Network (EMN) and the Microfinance Centre (MFC) are pleased to present the12th editionof their flagship publication:Microfinance in Europe: Survey Report. This long-standing survey remains the leading source of data and analysis on the microfinance sector in Europe. For thesixth consecutive survey edition, EMN and MFC have joined forces to capture data from the vast majority of European microfinance institutions, providing the most comprehensivedatasetavailable on the sector today. This edition focuses on thetypes of businesses reached by microfinanceand highlights thesocial performance of business loans, along with theimpact measurement approachesadopted by MFIs.\n",
      "Index dimension: 768\n",
      "Vectors in index: 3842\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of chunks:\", len(loaded_chunks))\n",
    "print(\"Sample chunk:\", loaded_chunks[0])\n",
    "print(\"Index dimension:\", index_2.d)     # embedding vector size\n",
    "print(\"Vectors in index:\", index_2.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "093920f0-3afb-445f-b803-32e2c9d755bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_chunks_cosine(query,model,index,chunk, top_k=5):\n",
    "    query_embedding = model.encode([query]).astype(\"float32\")\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    D, I = index.search(query_embedding, top_k)\n",
    "    return [chunk[i] for i in I[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "27b0af1c-9441-4924-bc1a-13780fc4ece2",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_qa_data_v2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c8658cef-f9bf-4751-ae6d-e1e304563d4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/297 [00:00<?, ?it/s]Llama.generate: 21 prefix-match hit, remaining 703 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   65348.11 ms /   703 tokens (   92.96 ms per token,    10.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =   36567.98 ms /   155 runs   (  235.92 ms per token,     4.24 tokens per second)\n",
      "llama_perf_context_print:       total time =  102028.42 ms /   858 tokens\n",
      "  0%|▎                                                                              | 1/297 [01:42<8:25:06, 102.39s/it]Llama.generate: 21 prefix-match hit, remaining 532 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   46702.98 ms /   532 tokens (   87.79 ms per token,    11.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =   47034.84 ms /   223 runs   (  210.92 ms per token,     4.74 tokens per second)\n",
      "llama_perf_context_print:       total time =   93904.47 ms /   755 tokens\n",
      "  1%|▌                                                                               | 2/297 [03:16<7:59:13, 97.47s/it]Llama.generate: 21 prefix-match hit, remaining 739 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   65975.09 ms /   739 tokens (   89.28 ms per token,    11.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =   43653.92 ms /   200 runs   (  218.27 ms per token,     4.58 tokens per second)\n",
      "llama_perf_context_print:       total time =  109774.39 ms /   939 tokens\n",
      "  1%|▊                                                                              | 3/297 [05:06<8:25:22, 103.14s/it]Llama.generate: 21 prefix-match hit, remaining 527 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   46074.05 ms /   527 tokens (   87.43 ms per token,    11.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =   51941.41 ms /   243 runs   (  213.75 ms per token,     4.68 tokens per second)\n",
      "llama_perf_context_print:       total time =   98194.67 ms /   770 tokens\n",
      "  1%|█                                                                              | 4/297 [06:44<8:14:17, 101.22s/it]Llama.generate: 21 prefix-match hit, remaining 421 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   35395.83 ms /   421 tokens (   84.08 ms per token,    11.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =   52835.43 ms /   252 runs   (  209.66 ms per token,     4.77 tokens per second)\n",
      "llama_perf_context_print:       total time =   88430.42 ms /   673 tokens\n",
      "  2%|█▎                                                                              | 5/297 [08:13<7:50:19, 96.64s/it]Llama.generate: 21 prefix-match hit, remaining 514 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   44695.60 ms /   514 tokens (   86.96 ms per token,    11.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =   51285.09 ms /   192 runs   (  267.11 ms per token,     3.74 tokens per second)\n",
      "llama_perf_context_print:       total time =   96151.28 ms /   706 tokens\n",
      "  2%|█▌                                                                              | 6/297 [09:49<7:48:08, 96.52s/it]Llama.generate: 21 prefix-match hit, remaining 274 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   21418.13 ms /   274 tokens (   78.17 ms per token,    12.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =   29914.70 ms /   117 runs   (  255.68 ms per token,     3.91 tokens per second)\n",
      "llama_perf_context_print:       total time =   51419.19 ms /   391 tokens\n",
      "  2%|█▉                                                                              | 7/297 [10:40<6:35:23, 81.81s/it]Llama.generate: 21 prefix-match hit, remaining 268 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   21464.32 ms /   268 tokens (   80.09 ms per token,    12.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =   33240.92 ms /   135 runs   (  246.23 ms per token,     4.06 tokens per second)\n",
      "llama_perf_context_print:       total time =   54803.91 ms /   403 tokens\n",
      "  3%|██▏                                                                             | 8/297 [11:35<5:52:47, 73.24s/it]Llama.generate: 21 prefix-match hit, remaining 406 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   32903.04 ms /   406 tokens (   81.04 ms per token,    12.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =   41610.76 ms /   163 runs   (  255.28 ms per token,     3.92 tokens per second)\n",
      "llama_perf_context_print:       total time =   74640.67 ms /   569 tokens\n",
      "  3%|██▍                                                                             | 9/297 [12:50<5:53:49, 73.71s/it]Llama.generate: 21 prefix-match hit, remaining 336 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   26918.18 ms /   336 tokens (   80.11 ms per token,    12.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =   21600.23 ms /    91 runs   (  237.37 ms per token,     4.21 tokens per second)\n",
      "llama_perf_context_print:       total time =   48576.91 ms /   427 tokens\n",
      "  3%|██▋                                                                            | 10/297 [13:39<5:15:36, 65.98s/it]Llama.generate: 21 prefix-match hit, remaining 1097 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  102115.95 ms /  1097 tokens (   93.09 ms per token,    10.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =   56854.99 ms /   255 runs   (  222.96 ms per token,     4.49 tokens per second)\n",
      "llama_perf_context_print:       total time =  159170.69 ms /  1352 tokens\n",
      "  4%|██▉                                                                            | 11/297 [16:18<7:30:40, 94.55s/it]Llama.generate: 21 prefix-match hit, remaining 1799 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  174727.71 ms /  1799 tokens (   97.12 ms per token,    10.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =   29824.80 ms /   127 runs   (  234.84 ms per token,     4.26 tokens per second)\n",
      "llama_perf_context_print:       total time =  204622.48 ms /  1926 tokens\n",
      "  4%|███                                                                          | 12/297 [19:43<10:08:18, 128.07s/it]Llama.generate: 21 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   40854.42 ms /   452 tokens (   90.39 ms per token,    11.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =   24918.65 ms /   120 runs   (  207.66 ms per token,     4.82 tokens per second)\n",
      "llama_perf_context_print:       total time =   65837.07 ms /   572 tokens\n",
      "  4%|███▍                                                                          | 13/297 [20:49<8:37:04, 109.24s/it]Llama.generate: 21 prefix-match hit, remaining 911 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   86807.32 ms /   911 tokens (   95.29 ms per token,    10.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =   56664.99 ms /   255 runs   (  222.22 ms per token,     4.50 tokens per second)\n",
      "llama_perf_context_print:       total time =  143672.09 ms /  1166 tokens\n",
      "  5%|███▋                                                                          | 14/297 [23:12<9:24:26, 119.67s/it]Llama.generate: 21 prefix-match hit, remaining 562 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   49764.40 ms /   562 tokens (   88.55 ms per token,    11.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =   62224.41 ms /   238 runs   (  261.45 ms per token,     3.82 tokens per second)\n",
      "llama_perf_context_print:       total time =  112213.79 ms /   800 tokens\n",
      "  5%|███▉                                                                          | 15/297 [25:05<9:12:01, 117.45s/it]Llama.generate: 21 prefix-match hit, remaining 409 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   33213.09 ms /   409 tokens (   81.21 ms per token,    12.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =   64149.60 ms /   255 runs   (  251.57 ms per token,     3.98 tokens per second)\n",
      "llama_perf_context_print:       total time =   97613.09 ms /   664 tokens\n",
      "  5%|████▏                                                                         | 16/297 [26:42<8:42:14, 111.51s/it]Llama.generate: 21 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   37529.96 ms /   454 tokens (   82.67 ms per token,    12.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =   28347.88 ms /   137 runs   (  206.92 ms per token,     4.83 tokens per second)\n",
      "llama_perf_context_print:       total time =   65955.01 ms /   591 tokens\n",
      "  6%|████▌                                                                          | 17/297 [27:49<7:36:35, 97.84s/it]Llama.generate: 22 prefix-match hit, remaining 654 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   59161.79 ms /   654 tokens (   90.46 ms per token,    11.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =   26068.39 ms /   123 runs   (  211.94 ms per token,     4.72 tokens per second)\n",
      "llama_perf_context_print:       total time =   85300.43 ms /   777 tokens\n",
      "  6%|████▊                                                                          | 18/297 [29:14<7:17:33, 94.10s/it]Llama.generate: 21 prefix-match hit, remaining 709 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   64848.51 ms /   709 tokens (   91.46 ms per token,    10.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =   63384.85 ms /   255 runs   (  248.57 ms per token,     4.02 tokens per second)\n",
      "llama_perf_context_print:       total time =  128473.14 ms /   964 tokens\n",
      "  6%|████▉                                                                         | 19/297 [31:23<8:03:58, 104.45s/it]Llama.generate: 21 prefix-match hit, remaining 871 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   77510.72 ms /   871 tokens (   88.99 ms per token,    11.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =   49615.88 ms /   192 runs   (  258.42 ms per token,     3.87 tokens per second)\n",
      "llama_perf_context_print:       total time =  127281.44 ms /  1063 tokens\n",
      "  7%|█████▎                                                                        | 20/297 [33:30<8:34:00, 111.34s/it]Llama.generate: 21 prefix-match hit, remaining 376 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   30264.81 ms /   376 tokens (   80.49 ms per token,    12.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =   52331.79 ms /   205 runs   (  255.28 ms per token,     3.92 tokens per second)\n",
      "llama_perf_context_print:       total time =   82776.19 ms /   581 tokens\n",
      "  7%|█████▌                                                                        | 21/297 [34:53<7:52:51, 102.79s/it]Llama.generate: 21 prefix-match hit, remaining 602 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   52018.24 ms /   602 tokens (   86.41 ms per token,    11.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =   65895.49 ms /   255 runs   (  258.41 ms per token,     3.87 tokens per second)\n",
      "llama_perf_context_print:       total time =  118145.47 ms /   857 tokens\n",
      "  7%|█████▊                                                                        | 22/297 [36:51<8:12:24, 107.43s/it]Llama.generate: 22 prefix-match hit, remaining 964 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   87126.11 ms /   964 tokens (   90.38 ms per token,    11.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =   43503.38 ms /   164 runs   (  265.26 ms per token,     3.77 tokens per second)\n",
      "llama_perf_context_print:       total time =  130759.93 ms /  1128 tokens\n",
      "  8%|██████                                                                        | 23/297 [39:02<8:42:42, 114.46s/it]Llama.generate: 21 prefix-match hit, remaining 539 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   46163.68 ms /   539 tokens (   85.65 ms per token,    11.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =   35572.54 ms /   138 runs   (  257.77 ms per token,     3.88 tokens per second)\n",
      "llama_perf_context_print:       total time =   81832.94 ms /   677 tokens\n",
      "  8%|██████▎                                                                       | 24/297 [40:24<7:56:23, 104.70s/it]Llama.generate: 21 prefix-match hit, remaining 312 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   24849.11 ms /   312 tokens (   79.64 ms per token,    12.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =   32606.09 ms /   134 runs   (  243.33 ms per token,     4.11 tokens per second)\n",
      "llama_perf_context_print:       total time =   57546.79 ms /   446 tokens\n",
      "  8%|██████▋                                                                        | 25/297 [41:21<6:50:38, 90.58s/it]Llama.generate: 21 prefix-match hit, remaining 287 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   24666.14 ms /   287 tokens (   85.94 ms per token,    11.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =   30080.45 ms /   147 runs   (  204.63 ms per token,     4.89 tokens per second)\n",
      "llama_perf_context_print:       total time =   54835.77 ms /   434 tokens\n",
      "  9%|██████▉                                                                        | 26/297 [42:16<6:00:49, 79.89s/it]Llama.generate: 21 prefix-match hit, remaining 561 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   53663.97 ms /   561 tokens (   95.66 ms per token,    10.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =   51746.43 ms /   255 runs   (  202.93 ms per token,     4.93 tokens per second)\n",
      "llama_perf_context_print:       total time =  105601.81 ms /   816 tokens\n",
      "  9%|███████▏                                                                       | 27/297 [44:02<6:34:20, 87.63s/it]Llama.generate: 21 prefix-match hit, remaining 850 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   89691.53 ms /   850 tokens (  105.52 ms per token,     9.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =   48182.63 ms /   224 runs   (  215.10 ms per token,     4.65 tokens per second)\n",
      "llama_perf_context_print:       total time =  138039.70 ms /  1074 tokens\n",
      "  9%|███████▎                                                                      | 28/297 [46:20<7:40:49, 102.79s/it]Llama.generate: 21 prefix-match hit, remaining 408 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   36400.48 ms /   408 tokens (   89.22 ms per token,    11.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =   28112.17 ms /   144 runs   (  195.22 ms per token,     5.12 tokens per second)\n",
      "llama_perf_context_print:       total time =   64589.22 ms /   552 tokens\n",
      " 10%|███████▋                                                                       | 29/297 [47:25<6:48:03, 91.36s/it]Llama.generate: 21 prefix-match hit, remaining 650 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   68860.54 ms /   650 tokens (  105.94 ms per token,     9.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =   49858.63 ms /   218 runs   (  228.71 ms per token,     4.37 tokens per second)\n",
      "llama_perf_context_print:       total time =  118891.22 ms /   868 tokens\n",
      " 10%|███████▉                                                                       | 30/297 [49:24<7:23:24, 99.64s/it]Llama.generate: 21 prefix-match hit, remaining 730 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   65876.96 ms /   730 tokens (   90.24 ms per token,    11.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =   29309.85 ms /   139 runs   (  210.86 ms per token,     4.74 tokens per second)\n",
      "llama_perf_context_print:       total time =   95270.70 ms /   869 tokens\n",
      " 10%|████████▏                                                                      | 31/297 [50:59<7:16:04, 98.36s/it]Llama.generate: 21 prefix-match hit, remaining 234 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   17957.86 ms /   234 tokens (   76.74 ms per token,    13.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =   23868.43 ms /   120 runs   (  198.90 ms per token,     5.03 tokens per second)\n",
      "llama_perf_context_print:       total time =   41898.76 ms /   354 tokens\n",
      " 11%|████████▌                                                                      | 32/297 [51:41<5:59:45, 81.45s/it]Llama.generate: 21 prefix-match hit, remaining 871 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   72051.94 ms /   871 tokens (   82.72 ms per token,    12.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =   55475.05 ms /   255 runs   (  217.55 ms per token,     4.60 tokens per second)\n",
      "llama_perf_context_print:       total time =  127739.00 ms /  1126 tokens\n",
      " 11%|████████▊                                                                      | 33/297 [53:49<6:59:39, 95.38s/it]Llama.generate: 21 prefix-match hit, remaining 932 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   77020.95 ms /   932 tokens (   82.64 ms per token,    12.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =   53521.40 ms /   255 runs   (  209.89 ms per token,     4.76 tokens per second)\n",
      "llama_perf_context_print:       total time =  130746.94 ms /  1187 tokens\n",
      " 11%|████████▉                                                                     | 34/297 [56:00<7:44:43, 106.02s/it]Llama.generate: 21 prefix-match hit, remaining 805 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   65907.60 ms /   805 tokens (   81.87 ms per token,    12.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =   36297.87 ms /   141 runs   (  257.43 ms per token,     3.88 tokens per second)\n",
      "llama_perf_context_print:       total time =  102293.36 ms /   946 tokens\n",
      " 12%|█████████▏                                                                    | 35/297 [57:42<7:38:14, 104.94s/it]Llama.generate: 21 prefix-match hit, remaining 792 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   87084.21 ms /   792 tokens (  109.95 ms per token,     9.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =   51512.22 ms /   255 runs   (  202.01 ms per token,     4.95 tokens per second)\n",
      "llama_perf_context_print:       total time =  138794.05 ms /  1047 tokens\n",
      " 12%|█████████▏                                                                  | 36/297 [1:00:01<8:20:48, 115.13s/it]Llama.generate: 21 prefix-match hit, remaining 265 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   21887.83 ms /   265 tokens (   82.60 ms per token,    12.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =   26960.38 ms /   138 runs   (  195.37 ms per token,     5.12 tokens per second)\n",
      "llama_perf_context_print:       total time =   48928.67 ms /   403 tokens\n",
      " 12%|█████████▌                                                                   | 37/297 [1:00:50<6:52:56, 95.30s/it]Llama.generate: 21 prefix-match hit, remaining 859 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   80612.01 ms /   859 tokens (   93.84 ms per token,    10.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =   53355.55 ms /   255 runs   (  209.24 ms per token,     4.78 tokens per second)\n",
      "llama_perf_context_print:       total time =  134167.00 ms /  1114 tokens\n",
      " 13%|█████████▋                                                                  | 38/297 [1:03:05<7:41:49, 106.99s/it]Llama.generate: 21 prefix-match hit, remaining 894 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  210722.46 ms /   894 tokens (  235.71 ms per token,     4.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =  112966.65 ms /   232 runs   (  486.93 ms per token,     2.05 tokens per second)\n",
      "llama_perf_context_print:       total time =  324114.08 ms /  1126 tokens\n",
      " 13%|█████████▊                                                                 | 39/297 [1:08:29<12:20:15, 172.15s/it]Llama.generate: 21 prefix-match hit, remaining 647 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  149927.09 ms /   647 tokens (  231.73 ms per token,     4.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =   91859.64 ms /   184 runs   (  499.24 ms per token,     2.00 tokens per second)\n",
      "llama_perf_context_print:       total time =  242096.93 ms /   831 tokens\n",
      " 13%|██████████                                                                 | 40/297 [1:12:31<13:47:33, 193.21s/it]Llama.generate: 22 prefix-match hit, remaining 570 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  132943.97 ms /   570 tokens (  233.24 ms per token,     4.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =  120503.69 ms /   255 runs   (  472.56 ms per token,     2.12 tokens per second)\n",
      "llama_perf_context_print:       total time =  253933.08 ms /   825 tokens\n",
      " 14%|██████████▎                                                                | 41/297 [1:16:45<15:02:33, 211.54s/it]Llama.generate: 21 prefix-match hit, remaining 800 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  187396.28 ms /   800 tokens (  234.25 ms per token,     4.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =  123009.45 ms /   255 runs   (  482.39 ms per token,     2.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  310862.79 ms /  1055 tokens\n",
      " 14%|██████████▌                                                                | 42/297 [1:21:57<17:05:57, 241.40s/it]Llama.generate: 21 prefix-match hit, remaining 745 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  202166.06 ms /   745 tokens (  271.36 ms per token,     3.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =  130327.50 ms /   255 runs   (  511.09 ms per token,     1.96 tokens per second)\n",
      "llama_perf_context_print:       total time =  333008.26 ms /  1000 tokens\n",
      " 14%|██████████▊                                                                | 43/297 [1:27:30<18:58:36, 268.96s/it]Llama.generate: 21 prefix-match hit, remaining 1006 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  267740.55 ms /  1006 tokens (  266.14 ms per token,     3.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =  122385.86 ms /   236 runs   (  518.58 ms per token,     1.93 tokens per second)\n",
      "llama_perf_context_print:       total time =  390569.96 ms /  1242 tokens\n",
      " 15%|███████████                                                                | 44/297 [1:34:01<21:28:27, 305.56s/it]Llama.generate: 21 prefix-match hit, remaining 1928 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  491142.55 ms /  1928 tokens (  254.74 ms per token,     3.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =  127816.62 ms /   225 runs   (  568.07 ms per token,     1.76 tokens per second)\n",
      "llama_perf_context_print:       total time =  619355.28 ms /  2153 tokens\n",
      " 15%|███████████▎                                                               | 45/297 [1:44:20<27:59:06, 399.79s/it]Llama.generate: 21 prefix-match hit, remaining 926 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  209994.50 ms /   926 tokens (  226.78 ms per token,     4.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =  136820.30 ms /   255 runs   (  536.55 ms per token,     1.86 tokens per second)\n",
      "llama_perf_context_print:       total time =  347359.30 ms /  1181 tokens\n",
      " 15%|███████████▌                                                               | 46/297 [1:50:08<26:47:07, 384.17s/it]Llama.generate: 22 prefix-match hit, remaining 411 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  114480.77 ms /   411 tokens (  278.54 ms per token,     3.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =  132789.03 ms /   255 runs   (  520.74 ms per token,     1.92 tokens per second)\n",
      "llama_perf_context_print:       total time =  247820.80 ms /   666 tokens\n",
      " 16%|███████████▊                                                               | 47/297 [1:54:16<23:50:36, 343.34s/it]Llama.generate: 21 prefix-match hit, remaining 415 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  110135.70 ms /   415 tokens (  265.39 ms per token,     3.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =  126298.17 ms /   255 runs   (  495.29 ms per token,     2.02 tokens per second)\n",
      "llama_perf_context_print:       total time =  236932.71 ms /   670 tokens\n",
      " 16%|████████████                                                               | 48/297 [1:58:13<21:32:44, 311.50s/it]Llama.generate: 21 prefix-match hit, remaining 524 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  115920.32 ms /   524 tokens (  221.22 ms per token,     4.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =  127899.54 ms /   244 runs   (  524.18 ms per token,     1.91 tokens per second)\n",
      "llama_perf_context_print:       total time =  244323.66 ms /   768 tokens\n",
      " 16%|████████████▎                                                              | 49/297 [2:02:18<20:04:32, 291.42s/it]Llama.generate: 21 prefix-match hit, remaining 419 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  101596.28 ms /   419 tokens (  242.47 ms per token,     4.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =   77112.90 ms /   156 runs   (  494.31 ms per token,     2.02 tokens per second)\n",
      "llama_perf_context_print:       total time =  178948.39 ms /   575 tokens\n",
      " 17%|████████████▋                                                              | 50/297 [2:05:17<17:41:05, 257.76s/it]Llama.generate: 21 prefix-match hit, remaining 804 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  179658.04 ms /   804 tokens (  223.46 ms per token,     4.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =  133507.13 ms /   255 runs   (  523.56 ms per token,     1.91 tokens per second)\n",
      "llama_perf_context_print:       total time =  313676.22 ms /  1059 tokens\n",
      " 17%|████████████▉                                                              | 51/297 [2:10:31<18:45:51, 274.60s/it]Llama.generate: 21 prefix-match hit, remaining 779 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  219036.23 ms /   779 tokens (  281.18 ms per token,     3.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =  131161.60 ms /   255 runs   (  514.36 ms per token,     1.94 tokens per second)\n",
      "llama_perf_context_print:       total time =  350699.57 ms /  1034 tokens\n",
      " 18%|█████████████▏                                                             | 52/297 [2:16:22<20:14:49, 297.51s/it]Llama.generate: 21 prefix-match hit, remaining 652 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  154645.10 ms /   652 tokens (  237.19 ms per token,     4.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =   91157.60 ms /   174 runs   (  523.89 ms per token,     1.91 tokens per second)\n",
      "llama_perf_context_print:       total time =  246128.63 ms /   826 tokens\n",
      " 18%|█████████████▍                                                             | 53/297 [2:20:28<19:07:29, 282.17s/it]Llama.generate: 22 prefix-match hit, remaining 502 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  117736.72 ms /   502 tokens (  234.54 ms per token,     4.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =  126981.34 ms /   255 runs   (  497.97 ms per token,     2.01 tokens per second)\n",
      "llama_perf_context_print:       total time =  245226.81 ms /   757 tokens\n",
      " 18%|█████████████▋                                                             | 54/297 [2:24:34<18:18:12, 271.16s/it]Llama.generate: 21 prefix-match hit, remaining 634 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  141163.40 ms /   634 tokens (  222.66 ms per token,     4.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =   62324.14 ms /   125 runs   (  498.59 ms per token,     2.01 tokens per second)\n",
      "llama_perf_context_print:       total time =  203658.61 ms /   759 tokens\n",
      " 19%|█████████████▉                                                             | 55/297 [2:27:58<16:52:16, 250.98s/it]Llama.generate: 21 prefix-match hit, remaining 284 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   68596.08 ms /   284 tokens (  241.54 ms per token,     4.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =   64346.17 ms /   143 runs   (  449.97 ms per token,     2.22 tokens per second)\n",
      "llama_perf_context_print:       total time =  133139.67 ms /   427 tokens\n",
      " 19%|██████████████▏                                                            | 56/297 [2:30:11<14:26:24, 215.70s/it]Llama.generate: 21 prefix-match hit, remaining 634 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  131006.35 ms /   634 tokens (  206.63 ms per token,     4.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =   53969.60 ms /   255 runs   (  211.65 ms per token,     4.72 tokens per second)\n",
      "llama_perf_context_print:       total time =  185172.88 ms /   889 tokens\n",
      " 19%|██████████████▍                                                            | 57/297 [2:33:17<13:46:25, 206.61s/it]Llama.generate: 21 prefix-match hit, remaining 252 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   20575.88 ms /   252 tokens (   81.65 ms per token,    12.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =   19640.92 ms /   100 runs   (  196.41 ms per token,     5.09 tokens per second)\n",
      "llama_perf_context_print:       total time =   40264.54 ms /   352 tokens\n",
      " 20%|██████████████▋                                                            | 58/297 [2:33:57<10:24:21, 156.74s/it]Llama.generate: 21 prefix-match hit, remaining 551 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   46868.36 ms /   551 tokens (   85.06 ms per token,    11.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =   50981.58 ms /   255 runs   (  199.93 ms per token,     5.00 tokens per second)\n",
      "llama_perf_context_print:       total time =   98045.02 ms /   806 tokens\n",
      " 20%|███████████████                                                             | 59/297 [2:35:35<9:11:58, 139.15s/it]Llama.generate: 21 prefix-match hit, remaining 245 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   19693.09 ms /   245 tokens (   80.38 ms per token,    12.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =   40675.10 ms /   194 runs   (  209.67 ms per token,     4.77 tokens per second)\n",
      "llama_perf_context_print:       total time =   60498.50 ms /   439 tokens\n",
      " 20%|███████████████▎                                                            | 60/297 [2:36:36<7:36:35, 115.59s/it]Llama.generate: 21 prefix-match hit, remaining 1079 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  109939.21 ms /  1079 tokens (  101.89 ms per token,     9.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =   67405.71 ms /   255 runs   (  264.34 ms per token,     3.78 tokens per second)\n",
      "llama_perf_context_print:       total time =  177554.65 ms /  1334 tokens\n",
      " 21%|███████████████▌                                                            | 61/297 [2:39:33<8:47:55, 134.22s/it]Llama.generate: 21 prefix-match hit, remaining 1273 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  122336.58 ms /  1273 tokens (   96.10 ms per token,    10.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =   31646.24 ms /   144 runs   (  219.77 ms per token,     4.55 tokens per second)\n",
      "llama_perf_context_print:       total time =  154071.91 ms /  1417 tokens\n",
      " 21%|███████████████▊                                                            | 62/297 [2:42:08<9:09:10, 140.21s/it]Llama.generate: 21 prefix-match hit, remaining 703 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   75914.98 ms /   703 tokens (  107.99 ms per token,     9.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =   45707.95 ms /   218 runs   (  209.67 ms per token,     4.77 tokens per second)\n",
      "llama_perf_context_print:       total time =  121775.23 ms /   921 tokens\n",
      " 21%|████████████████                                                            | 63/297 [2:44:09<8:45:22, 134.71s/it]Llama.generate: 21 prefix-match hit, remaining 1132 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  106329.76 ms /  1132 tokens (   93.93 ms per token,    10.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =   55946.27 ms /   255 runs   (  219.40 ms per token,     4.56 tokens per second)\n",
      "llama_perf_context_print:       total time =  162477.12 ms /  1387 tokens\n",
      " 22%|████████████████▍                                                           | 64/297 [2:46:52<9:15:35, 143.07s/it]Llama.generate: 21 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   40668.22 ms /   454 tokens (   89.58 ms per token,    11.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =   56352.56 ms /   255 runs   (  220.99 ms per token,     4.53 tokens per second)\n",
      "llama_perf_context_print:       total time =   97223.21 ms /   709 tokens\n",
      " 22%|████████████████▋                                                           | 65/297 [2:48:29<8:20:09, 129.35s/it]Llama.generate: 21 prefix-match hit, remaining 307 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   26303.35 ms /   307 tokens (   85.68 ms per token,    11.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =   32109.76 ms /   150 runs   (  214.07 ms per token,     4.67 tokens per second)\n",
      "llama_perf_context_print:       total time =   58527.66 ms /   457 tokens\n",
      " 22%|████████████████▉                                                           | 66/297 [2:49:28<6:56:19, 108.14s/it]Llama.generate: 21 prefix-match hit, remaining 1315 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  121041.54 ms /  1315 tokens (   92.05 ms per token,    10.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =   61965.98 ms /   255 runs   (  243.00 ms per token,     4.12 tokens per second)\n",
      "llama_perf_context_print:       total time =  183216.53 ms /  1570 tokens\n",
      " 23%|█████████████████▏                                                          | 67/297 [2:52:31<8:20:59, 130.69s/it]Llama.generate: 21 prefix-match hit, remaining 440 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   37863.38 ms /   440 tokens (   86.05 ms per token,    11.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =   57487.98 ms /   255 runs   (  225.44 ms per token,     4.44 tokens per second)\n",
      "llama_perf_context_print:       total time =   95565.00 ms /   695 tokens\n",
      " 23%|█████████████████▍                                                          | 68/297 [2:54:07<7:38:41, 120.18s/it]Llama.generate: 21 prefix-match hit, remaining 418 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   36968.77 ms /   418 tokens (   88.44 ms per token,    11.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =   56340.01 ms /   255 runs   (  220.94 ms per token,     4.53 tokens per second)\n",
      "llama_perf_context_print:       total time =   93509.74 ms /   673 tokens\n",
      " 23%|█████████████████▋                                                          | 69/297 [2:55:41<7:06:23, 112.21s/it]Llama.generate: 21 prefix-match hit, remaining 1315 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  123878.24 ms /  1315 tokens (   94.20 ms per token,    10.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =   45514.46 ms /   189 runs   (  240.82 ms per token,     4.15 tokens per second)\n",
      "llama_perf_context_print:       total time =  169531.10 ms /  1504 tokens\n",
      " 24%|█████████████████▉                                                          | 70/297 [2:58:30<8:09:40, 129.43s/it]Llama.generate: 21 prefix-match hit, remaining 774 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   70614.53 ms /   774 tokens (   91.23 ms per token,    10.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =   58224.46 ms /   255 runs   (  228.33 ms per token,     4.38 tokens per second)\n",
      "llama_perf_context_print:       total time =  129052.62 ms /  1029 tokens\n",
      " 24%|██████████████████▏                                                         | 71/297 [3:00:39<8:07:11, 129.34s/it]Llama.generate: 21 prefix-match hit, remaining 704 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   64076.44 ms /   704 tokens (   91.02 ms per token,    10.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =   41318.00 ms /   177 runs   (  233.44 ms per token,     4.28 tokens per second)\n",
      "llama_perf_context_print:       total time =  105519.46 ms /   881 tokens\n",
      " 24%|██████████████████▍                                                         | 72/297 [3:02:25<7:38:20, 122.23s/it]Llama.generate: 21 prefix-match hit, remaining 465 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   41506.98 ms /   465 tokens (   89.26 ms per token,    11.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =   39223.58 ms /   178 runs   (  220.36 ms per token,     4.54 tokens per second)\n",
      "llama_perf_context_print:       total time =   80845.27 ms /   643 tokens\n",
      " 25%|██████████████████▋                                                         | 73/297 [3:03:46<6:50:04, 109.84s/it]Llama.generate: 21 prefix-match hit, remaining 492 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   44660.92 ms /   492 tokens (   90.77 ms per token,    11.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =   46346.84 ms /   206 runs   (  224.98 ms per token,     4.44 tokens per second)\n",
      "llama_perf_context_print:       total time =   91158.65 ms /   698 tokens\n",
      " 25%|██████████████████▉                                                         | 74/297 [3:05:17<6:27:30, 104.26s/it]Llama.generate: 21 prefix-match hit, remaining 809 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   73525.00 ms /   809 tokens (   90.88 ms per token,    11.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =   37164.52 ms /   162 runs   (  229.41 ms per token,     4.36 tokens per second)\n",
      "llama_perf_context_print:       total time =  110797.99 ms /   971 tokens\n",
      " 25%|███████████████████▏                                                        | 75/297 [3:07:08<6:33:18, 106.30s/it]Llama.generate: 21 prefix-match hit, remaining 810 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   74207.12 ms /   810 tokens (   91.61 ms per token,    10.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =   56922.08 ms /   251 runs   (  226.78 ms per token,     4.41 tokens per second)\n",
      "llama_perf_context_print:       total time =  131339.21 ms /  1061 tokens\n",
      " 26%|███████████████████▍                                                        | 76/297 [3:09:20<6:59:18, 113.84s/it]Llama.generate: 21 prefix-match hit, remaining 682 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   61651.58 ms /   682 tokens (   90.40 ms per token,    11.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =   58526.16 ms /   255 runs   (  229.51 ms per token,     4.36 tokens per second)\n",
      "llama_perf_context_print:       total time =  120392.92 ms /   937 tokens\n",
      " 26%|███████████████████▋                                                        | 77/297 [3:11:20<7:04:42, 115.83s/it]Llama.generate: 21 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   40629.50 ms /   458 tokens (   88.71 ms per token,    11.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =   56790.66 ms /   255 runs   (  222.71 ms per token,     4.49 tokens per second)\n",
      "llama_perf_context_print:       total time =   97630.65 ms /   713 tokens\n",
      " 26%|███████████████████▉                                                        | 78/297 [3:12:58<6:42:57, 110.40s/it]Llama.generate: 21 prefix-match hit, remaining 801 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   72127.69 ms /   801 tokens (   90.05 ms per token,    11.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =   58927.78 ms /   255 runs   (  231.09 ms per token,     4.33 tokens per second)\n",
      "llama_perf_context_print:       total time =  131264.88 ms /  1056 tokens\n",
      " 27%|████████████████████▏                                                       | 79/297 [3:15:09<7:03:57, 116.68s/it]Llama.generate: 21 prefix-match hit, remaining 756 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   70167.60 ms /   756 tokens (   92.81 ms per token,    10.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =   48741.46 ms /   210 runs   (  232.10 ms per token,     4.31 tokens per second)\n",
      "llama_perf_context_print:       total time =  119065.88 ms /   966 tokens\n",
      " 27%|████████████████████▍                                                       | 80/297 [3:17:08<7:04:41, 117.43s/it]Llama.generate: 21 prefix-match hit, remaining 711 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   65520.92 ms /   711 tokens (   92.15 ms per token,    10.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =   60076.47 ms /   255 runs   (  235.59 ms per token,     4.24 tokens per second)\n",
      "llama_perf_context_print:       total time =  125797.55 ms /   966 tokens\n",
      " 27%|████████████████████▋                                                       | 81/297 [3:19:14<7:11:53, 119.97s/it]Llama.generate: 21 prefix-match hit, remaining 314 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   28220.44 ms /   314 tokens (   89.87 ms per token,    11.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =   44116.65 ms /   197 runs   (  223.94 ms per token,     4.47 tokens per second)\n",
      "llama_perf_context_print:       total time =   72483.30 ms /   511 tokens\n",
      " 28%|████████████████████▉                                                       | 82/297 [3:20:27<6:18:57, 105.76s/it]Llama.generate: 21 prefix-match hit, remaining 815 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   76150.50 ms /   815 tokens (   93.44 ms per token,    10.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =   44795.48 ms /   189 runs   (  237.01 ms per token,     4.22 tokens per second)\n",
      "llama_perf_context_print:       total time =  121082.10 ms /  1004 tokens\n",
      " 28%|█████████████████████▏                                                      | 83/297 [3:22:28<6:33:47, 110.41s/it]Llama.generate: 21 prefix-match hit, remaining 798 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   76294.20 ms /   798 tokens (   95.61 ms per token,    10.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =   59905.11 ms /   255 runs   (  234.92 ms per token,     4.26 tokens per second)\n",
      "llama_perf_context_print:       total time =  136412.25 ms /  1053 tokens\n",
      " 28%|█████████████████████▍                                                      | 84/297 [3:24:45<6:59:44, 118.24s/it]Llama.generate: 21 prefix-match hit, remaining 828 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   79684.75 ms /   828 tokens (   96.24 ms per token,    10.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =   49931.21 ms /   214 runs   (  233.32 ms per token,     4.29 tokens per second)\n",
      "llama_perf_context_print:       total time =  129769.77 ms /  1042 tokens\n",
      " 29%|█████████████████████▊                                                      | 85/297 [3:26:54<7:10:06, 121.73s/it]Llama.generate: 21 prefix-match hit, remaining 874 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   86814.48 ms /   874 tokens (   99.33 ms per token,    10.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =   60013.41 ms /   255 runs   (  235.35 ms per token,     4.25 tokens per second)\n",
      "llama_perf_context_print:       total time =  147047.00 ms /  1129 tokens\n",
      " 29%|██████████████████████                                                      | 86/297 [3:29:22<7:34:53, 129.35s/it]Llama.generate: 21 prefix-match hit, remaining 265 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   24782.52 ms /   265 tokens (   93.52 ms per token,    10.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =   32268.23 ms /   148 runs   (  218.03 ms per token,     4.59 tokens per second)\n",
      "llama_perf_context_print:       total time =   57145.34 ms /   413 tokens\n",
      " 29%|██████████████████████▎                                                     | 87/297 [3:30:19<6:17:00, 107.72s/it]Llama.generate: 21 prefix-match hit, remaining 302 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   28042.18 ms /   302 tokens (   92.85 ms per token,    10.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =   39908.36 ms /   184 runs   (  216.89 ms per token,     4.61 tokens per second)\n",
      "llama_perf_context_print:       total time =   68076.24 ms /   486 tokens\n",
      " 30%|██████████████████████▊                                                      | 88/297 [3:31:27<5:33:52, 95.85s/it]Llama.generate: 21 prefix-match hit, remaining 606 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   56751.57 ms /   606 tokens (   93.65 ms per token,    10.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =   58033.86 ms /   255 runs   (  227.58 ms per token,     4.39 tokens per second)\n",
      "llama_perf_context_print:       total time =  114985.85 ms /   861 tokens\n",
      " 30%|██████████████████████▊                                                     | 89/297 [3:33:22<5:52:19, 101.63s/it]Llama.generate: 21 prefix-match hit, remaining 1101 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  109264.72 ms /  1101 tokens (   99.24 ms per token,    10.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =   40822.12 ms /   172 runs   (  237.34 ms per token,     4.21 tokens per second)\n",
      "llama_perf_context_print:       total time =  150201.43 ms /  1273 tokens\n",
      " 30%|███████████████████████                                                     | 90/297 [3:35:52<6:40:59, 116.23s/it]Llama.generate: 21 prefix-match hit, remaining 815 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   79961.03 ms /   815 tokens (   98.11 ms per token,    10.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =   58229.07 ms /   255 runs   (  228.35 ms per token,     4.38 tokens per second)\n",
      "llama_perf_context_print:       total time =  138386.79 ms /  1070 tokens\n",
      " 31%|███████████████████████▎                                                    | 91/297 [3:38:11<7:01:59, 122.91s/it]Llama.generate: 21 prefix-match hit, remaining 595 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   57739.71 ms /   595 tokens (   97.04 ms per token,    10.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =   50512.13 ms /   228 runs   (  221.54 ms per token,     4.51 tokens per second)\n",
      "llama_perf_context_print:       total time =  108421.12 ms /   823 tokens\n",
      " 31%|███████████████████████▌                                                    | 92/297 [3:39:59<6:45:11, 118.59s/it]Llama.generate: 21 prefix-match hit, remaining 808 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   78601.18 ms /   808 tokens (   97.28 ms per token,    10.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =   63992.11 ms /   255 runs   (  250.95 ms per token,     3.98 tokens per second)\n",
      "llama_perf_context_print:       total time =  142793.15 ms /  1063 tokens\n",
      " 31%|███████████████████████▊                                                    | 93/297 [3:42:22<7:08:00, 125.88s/it]Llama.generate: 21 prefix-match hit, remaining 306 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   35349.73 ms /   306 tokens (  115.52 ms per token,     8.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =   34033.61 ms /   157 runs   (  216.77 ms per token,     4.61 tokens per second)\n",
      "llama_perf_context_print:       total time =   69474.50 ms /   463 tokens\n",
      " 32%|████████████████████████                                                    | 94/297 [3:43:32<6:08:45, 108.99s/it]Llama.generate: 21 prefix-match hit, remaining 654 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   61801.55 ms /   654 tokens (   94.50 ms per token,    10.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =   56051.69 ms /   251 runs   (  223.31 ms per token,     4.48 tokens per second)\n",
      "llama_perf_context_print:       total time =  118048.80 ms /   905 tokens\n",
      " 32%|████████████████████████▎                                                   | 95/297 [3:45:30<6:16:14, 111.75s/it]Llama.generate: 21 prefix-match hit, remaining 430 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   39728.41 ms /   430 tokens (   92.39 ms per token,    10.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =   32890.21 ms /   150 runs   (  219.27 ms per token,     4.56 tokens per second)\n",
      "llama_perf_context_print:       total time =   72701.89 ms /   580 tokens\n",
      " 32%|████████████████████████▌                                                   | 96/297 [3:46:43<5:35:13, 100.07s/it]Llama.generate: 21 prefix-match hit, remaining 570 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   54578.16 ms /   570 tokens (   95.75 ms per token,    10.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =   56762.78 ms /   255 runs   (  222.60 ms per token,     4.49 tokens per second)\n",
      "llama_perf_context_print:       total time =  111537.35 ms /   825 tokens\n",
      " 33%|████████████████████████▊                                                   | 97/297 [3:48:35<5:45:07, 103.54s/it]Llama.generate: 21 prefix-match hit, remaining 405 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   38055.55 ms /   405 tokens (   93.96 ms per token,    10.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =   56052.13 ms /   255 runs   (  219.81 ms per token,     4.55 tokens per second)\n",
      "llama_perf_context_print:       total time =   94300.04 ms /   660 tokens\n",
      " 33%|█████████████████████████                                                   | 98/297 [3:50:09<5:34:17, 100.79s/it]Llama.generate: 21 prefix-match hit, remaining 978 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   97371.15 ms /   978 tokens (   99.56 ms per token,    10.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =   59487.05 ms /   255 runs   (  233.28 ms per token,     4.29 tokens per second)\n",
      "llama_perf_context_print:       total time =  157054.68 ms /  1233 tokens\n",
      " 33%|█████████████████████████▎                                                  | 99/297 [3:52:46<6:28:24, 117.70s/it]Llama.generate: 21 prefix-match hit, remaining 616 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   59473.97 ms /   616 tokens (   96.55 ms per token,    10.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =   57856.00 ms /   255 runs   (  226.89 ms per token,     4.41 tokens per second)\n",
      "llama_perf_context_print:       total time =  117522.12 ms /   871 tokens\n",
      " 34%|█████████████████████████▎                                                 | 100/297 [3:54:44<6:26:26, 117.70s/it]Llama.generate: 21 prefix-match hit, remaining 272 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   25246.51 ms /   272 tokens (   92.82 ms per token,    10.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =   41514.49 ms /   190 runs   (  218.50 ms per token,     4.58 tokens per second)\n",
      "llama_perf_context_print:       total time =   66881.96 ms /   462 tokens\n",
      " 34%|█████████████████████████▌                                                 | 101/297 [3:55:51<5:34:47, 102.49s/it]Llama.generate: 21 prefix-match hit, remaining 1793 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  186794.84 ms /  1793 tokens (  104.18 ms per token,     9.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =   65442.77 ms /   255 runs   (  256.64 ms per token,     3.90 tokens per second)\n",
      "llama_perf_context_print:       total time =  252426.25 ms /  2048 tokens\n",
      " 34%|█████████████████████████▊                                                 | 102/297 [4:00:03<7:59:21, 147.49s/it]Llama.generate: 21 prefix-match hit, remaining 800 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   80389.95 ms /   800 tokens (  100.49 ms per token,     9.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =   27042.80 ms /   115 runs   (  235.15 ms per token,     4.25 tokens per second)\n",
      "llama_perf_context_print:       total time =  107498.35 ms /   915 tokens\n",
      " 35%|██████████████████████████                                                 | 103/297 [4:01:51<7:18:13, 135.53s/it]Llama.generate: 21 prefix-match hit, remaining 978 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   96919.95 ms /   978 tokens (   99.10 ms per token,    10.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =   59283.75 ms /   255 runs   (  232.49 ms per token,     4.30 tokens per second)\n",
      "llama_perf_context_print:       total time =  156398.90 ms /  1233 tokens\n",
      " 35%|██████████████████████████▎                                                | 104/297 [4:04:27<7:36:12, 141.82s/it]Llama.generate: 21 prefix-match hit, remaining 612 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   58573.72 ms /   612 tokens (   95.71 ms per token,    10.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =   58585.19 ms /   255 runs   (  229.75 ms per token,     4.35 tokens per second)\n",
      "llama_perf_context_print:       total time =  117360.61 ms /   867 tokens\n",
      " 35%|██████████████████████████▌                                                | 105/297 [4:06:25<7:10:27, 134.52s/it]Llama.generate: 22 prefix-match hit, remaining 888 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   89495.79 ms /   888 tokens (  100.78 ms per token,     9.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =   39251.20 ms /   167 runs   (  235.04 ms per token,     4.25 tokens per second)\n",
      "llama_perf_context_print:       total time =  128845.70 ms /  1055 tokens\n",
      " 36%|██████████████████████████▊                                                | 106/297 [4:08:34<7:02:53, 132.85s/it]Llama.generate: 22 prefix-match hit, remaining 596 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   57797.94 ms /   596 tokens (   96.98 ms per token,    10.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =   53242.56 ms /   234 runs   (  227.53 ms per token,     4.39 tokens per second)\n",
      "llama_perf_context_print:       total time =  111218.05 ms /   830 tokens\n",
      " 36%|███████████████████████████                                                | 107/297 [4:10:25<6:40:12, 126.38s/it]Llama.generate: 21 prefix-match hit, remaining 678 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   66271.33 ms /   678 tokens (   97.75 ms per token,    10.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =   58935.43 ms /   255 runs   (  231.12 ms per token,     4.33 tokens per second)\n",
      "llama_perf_context_print:       total time =  125402.51 ms /   933 tokens\n",
      " 36%|███████████████████████████▎                                               | 108/297 [4:12:31<6:37:16, 126.12s/it]Llama.generate: 21 prefix-match hit, remaining 362 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   35952.81 ms /   362 tokens (   99.32 ms per token,    10.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =   56634.91 ms /   254 runs   (  222.97 ms per token,     4.48 tokens per second)\n",
      "llama_perf_context_print:       total time =   92791.54 ms /   616 tokens\n",
      " 37%|███████████████████████████▌                                               | 109/297 [4:14:04<6:03:55, 116.15s/it]Llama.generate: 21 prefix-match hit, remaining 611 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   58747.17 ms /   611 tokens (   96.15 ms per token,    10.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =   39426.45 ms /   171 runs   (  230.56 ms per token,     4.34 tokens per second)\n",
      "llama_perf_context_print:       total time =   98282.94 ms /   782 tokens\n",
      " 37%|███████████████████████████▊                                               | 110/297 [4:15:42<5:45:23, 110.82s/it]Llama.generate: 21 prefix-match hit, remaining 1110 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  114406.91 ms /  1110 tokens (  103.07 ms per token,     9.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =   59736.13 ms /   255 runs   (  234.26 ms per token,     4.27 tokens per second)\n",
      "llama_perf_context_print:       total time =  174342.20 ms /  1365 tokens\n",
      " 37%|████████████████████████████                                               | 111/297 [4:18:36<6:42:41, 129.90s/it]Llama.generate: 21 prefix-match hit, remaining 998 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  102773.52 ms /   998 tokens (  102.98 ms per token,     9.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =   58782.27 ms /   248 runs   (  237.03 ms per token,     4.22 tokens per second)\n",
      "llama_perf_context_print:       total time =  161746.17 ms /  1246 tokens\n",
      " 38%|████████████████████████████▎                                              | 112/297 [4:21:18<7:10:04, 139.48s/it]Llama.generate: 21 prefix-match hit, remaining 670 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   67155.08 ms /   670 tokens (  100.23 ms per token,     9.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =   34678.87 ms /   149 runs   (  232.74 ms per token,     4.30 tokens per second)\n",
      "llama_perf_context_print:       total time =  101925.62 ms /   819 tokens\n",
      " 38%|████████████████████████████▌                                              | 113/297 [4:23:00<6:33:16, 128.24s/it]Llama.generate: 21 prefix-match hit, remaining 410 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   39551.41 ms /   410 tokens (   96.47 ms per token,    10.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =   57321.65 ms /   255 runs   (  224.79 ms per token,     4.45 tokens per second)\n",
      "llama_perf_context_print:       total time =   97071.16 ms /   665 tokens\n",
      " 38%|████████████████████████████▊                                              | 114/297 [4:24:37<6:02:46, 118.94s/it]Llama.generate: 21 prefix-match hit, remaining 594 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   58924.69 ms /   594 tokens (   99.20 ms per token,    10.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =   46312.13 ms /   207 runs   (  223.73 ms per token,     4.47 tokens per second)\n",
      "llama_perf_context_print:       total time =  105388.51 ms /   801 tokens\n",
      " 39%|█████████████████████████████                                              | 115/297 [4:26:23<5:48:32, 114.90s/it]Llama.generate: 22 prefix-match hit, remaining 225 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   19073.15 ms /   225 tokens (   84.77 ms per token,    11.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =   46390.61 ms /   222 runs   (  208.97 ms per token,     4.79 tokens per second)\n",
      "llama_perf_context_print:       total time =   65625.81 ms /   447 tokens\n",
      " 39%|█████████████████████████████▎                                             | 116/297 [4:27:29<5:02:07, 100.15s/it]Llama.generate: 21 prefix-match hit, remaining 712 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   68398.08 ms /   712 tokens (   96.06 ms per token,    10.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =   58956.66 ms /   254 runs   (  232.11 ms per token,     4.31 tokens per second)\n",
      "llama_perf_context_print:       total time =  127557.56 ms /   966 tokens\n",
      " 39%|█████████████████████████████▌                                             | 117/297 [4:29:36<5:25:13, 108.41s/it]Llama.generate: 21 prefix-match hit, remaining 895 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   89944.56 ms /   895 tokens (  100.50 ms per token,     9.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =   47642.07 ms /   199 runs   (  239.41 ms per token,     4.18 tokens per second)\n",
      "llama_perf_context_print:       total time =  137724.39 ms /  1094 tokens\n",
      " 40%|█████████████████████████████▊                                             | 118/297 [4:31:54<5:49:44, 117.23s/it]Llama.generate: 21 prefix-match hit, remaining 1335 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  141210.42 ms /  1335 tokens (  105.78 ms per token,     9.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =   63225.72 ms /   255 runs   (  247.94 ms per token,     4.03 tokens per second)\n",
      "llama_perf_context_print:       total time =  204637.01 ms /  1590 tokens\n",
      " 40%|██████████████████████████████                                             | 119/297 [4:35:19<7:05:40, 143.49s/it]Llama.generate: 21 prefix-match hit, remaining 166 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   15235.14 ms /   166 tokens (   91.78 ms per token,    10.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =   24062.46 ms /   107 runs   (  224.88 ms per token,     4.45 tokens per second)\n",
      "llama_perf_context_print:       total time =   39356.81 ms /   273 tokens\n",
      " 40%|██████████████████████████████▎                                            | 120/297 [4:35:58<5:31:13, 112.28s/it]Llama.generate: 21 prefix-match hit, remaining 595 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   58795.32 ms /   595 tokens (   98.82 ms per token,    10.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =   40761.50 ms /   184 runs   (  221.53 ms per token,     4.51 tokens per second)\n",
      "llama_perf_context_print:       total time =   99681.21 ms /   779 tokens\n",
      " 41%|██████████████████████████████▌                                            | 121/297 [4:37:38<5:18:21, 108.53s/it]Llama.generate: 21 prefix-match hit, remaining 666 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   58516.60 ms /   666 tokens (   87.86 ms per token,    11.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =   56109.50 ms /   255 runs   (  220.04 ms per token,     4.54 tokens per second)\n",
      "llama_perf_context_print:       total time =  114834.50 ms /   921 tokens\n",
      " 41%|██████████████████████████████▊                                            | 122/297 [4:39:33<5:22:08, 110.45s/it]Llama.generate: 21 prefix-match hit, remaining 349 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   29946.23 ms /   349 tokens (   85.81 ms per token,    11.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =   52707.32 ms /   248 runs   (  212.53 ms per token,     4.71 tokens per second)\n",
      "llama_perf_context_print:       total time =   82844.38 ms /   597 tokens\n",
      " 41%|███████████████████████████████                                            | 123/297 [4:40:56<4:56:25, 102.21s/it]Llama.generate: 21 prefix-match hit, remaining 579 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   51294.91 ms /   579 tokens (   88.59 ms per token,    11.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =   57589.48 ms /   255 runs   (  225.84 ms per token,     4.43 tokens per second)\n",
      "llama_perf_context_print:       total time =  109085.99 ms /   834 tokens\n",
      " 42%|███████████████████████████████▎                                           | 124/297 [4:42:45<5:00:44, 104.30s/it]Llama.generate: 21 prefix-match hit, remaining 335 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   29566.85 ms /   335 tokens (   88.26 ms per token,    11.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =   30510.90 ms /   145 runs   (  210.42 ms per token,     4.75 tokens per second)\n",
      "llama_perf_context_print:       total time =   60161.25 ms /   480 tokens\n",
      " 42%|███████████████████████████████▉                                            | 125/297 [4:43:45<4:21:07, 91.09s/it]Llama.generate: 21 prefix-match hit, remaining 763 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   68260.64 ms /   763 tokens (   89.46 ms per token,    11.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =   57224.02 ms /   255 runs   (  224.41 ms per token,     4.46 tokens per second)\n",
      "llama_perf_context_print:       total time =  125687.95 ms /  1018 tokens\n",
      " 42%|███████████████████████████████▊                                           | 126/297 [4:45:51<4:49:16, 101.50s/it]Llama.generate: 21 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   39719.95 ms /   457 tokens (   86.91 ms per token,    11.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =   51518.35 ms /   239 runs   (  215.56 ms per token,     4.64 tokens per second)\n",
      "llama_perf_context_print:       total time =   91427.18 ms /   696 tokens\n",
      " 43%|████████████████████████████████▍                                           | 127/297 [4:47:23<4:39:06, 98.51s/it]Llama.generate: 21 prefix-match hit, remaining 887 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   78652.32 ms /   887 tokens (   88.67 ms per token,    11.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =   27497.23 ms /   124 runs   (  221.75 ms per token,     4.51 tokens per second)\n",
      "llama_perf_context_print:       total time =  106217.97 ms /  1011 tokens\n",
      " 43%|████████████████████████████████▎                                          | 128/297 [4:49:09<4:44:03, 100.85s/it]Llama.generate: 21 prefix-match hit, remaining 730 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   64292.94 ms /   730 tokens (   88.07 ms per token,    11.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =   36508.81 ms /   167 runs   (  218.62 ms per token,     4.57 tokens per second)\n",
      "llama_perf_context_print:       total time =  100908.92 ms /   897 tokens\n",
      " 43%|████████████████████████████████▌                                          | 129/297 [4:50:50<4:42:30, 100.90s/it]Llama.generate: 21 prefix-match hit, remaining 406 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   35037.99 ms /   406 tokens (   86.30 ms per token,    11.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =   13867.24 ms /    65 runs   (  213.34 ms per token,     4.69 tokens per second)\n",
      "llama_perf_context_print:       total time =   48939.62 ms /   471 tokens\n",
      " 44%|█████████████████████████████████▎                                          | 130/297 [4:51:39<3:57:31, 85.34s/it]Llama.generate: 21 prefix-match hit, remaining 710 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   63210.27 ms /   710 tokens (   89.03 ms per token,    11.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =   56198.21 ms /   255 runs   (  220.39 ms per token,     4.54 tokens per second)\n",
      "llama_perf_context_print:       total time =  119612.11 ms /   965 tokens\n",
      " 44%|█████████████████████████████████▌                                          | 131/297 [4:53:39<4:24:37, 95.65s/it]Llama.generate: 21 prefix-match hit, remaining 1990 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  187317.33 ms /  1990 tokens (   94.13 ms per token,    10.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =   63055.17 ms /   255 runs   (  247.28 ms per token,     4.04 tokens per second)\n",
      "llama_perf_context_print:       total time =  250566.89 ms /  2245 tokens\n",
      " 44%|█████████████████████████████████▎                                         | 132/297 [4:57:50<6:30:55, 142.16s/it]Llama.generate: 21 prefix-match hit, remaining 717 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   63242.95 ms /   717 tokens (   88.20 ms per token,    11.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =   56245.21 ms /   255 runs   (  220.57 ms per token,     4.53 tokens per second)\n",
      "llama_perf_context_print:       total time =  119688.04 ms /   972 tokens\n",
      " 45%|█████████████████████████████████▌                                         | 133/297 [4:59:49<6:10:12, 135.44s/it]Llama.generate: 21 prefix-match hit, remaining 765 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   67490.54 ms /   765 tokens (   88.22 ms per token,    11.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =   33107.85 ms /   149 runs   (  222.20 ms per token,     4.50 tokens per second)\n",
      "llama_perf_context_print:       total time =  100692.90 ms /   914 tokens\n",
      " 45%|█████████████████████████████████▊                                         | 134/297 [5:01:30<5:39:42, 125.04s/it]Llama.generate: 21 prefix-match hit, remaining 555 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   48544.20 ms /   555 tokens (   87.47 ms per token,    11.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =   54758.23 ms /   255 runs   (  214.74 ms per token,     4.66 tokens per second)\n",
      "llama_perf_context_print:       total time =  103503.61 ms /   810 tokens\n",
      " 45%|██████████████████████████████████                                         | 135/297 [5:03:14<5:20:14, 118.61s/it]Llama.generate: 21 prefix-match hit, remaining 763 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   67973.06 ms /   763 tokens (   89.09 ms per token,    11.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =   57029.13 ms /   255 runs   (  223.64 ms per token,     4.47 tokens per second)\n",
      "llama_perf_context_print:       total time =  125197.97 ms /  1018 tokens\n",
      " 46%|██████████████████████████████████▎                                        | 136/297 [5:05:19<5:23:39, 120.62s/it]Llama.generate: 21 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   41630.27 ms /   457 tokens (   91.09 ms per token,    10.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =   55458.42 ms /   255 runs   (  217.48 ms per token,     4.60 tokens per second)\n",
      "llama_perf_context_print:       total time =   97291.89 ms /   712 tokens\n",
      " 46%|██████████████████████████████████▌                                        | 137/297 [5:06:56<5:03:03, 113.65s/it]Llama.generate: 21 prefix-match hit, remaining 887 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   78792.96 ms /   887 tokens (   88.83 ms per token,    11.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =   25787.63 ms /   116 runs   (  222.31 ms per token,     4.50 tokens per second)\n",
      "llama_perf_context_print:       total time =  104641.32 ms /  1003 tokens\n",
      " 46%|██████████████████████████████████▊                                        | 138/297 [5:08:41<4:54:04, 110.97s/it]Llama.generate: 21 prefix-match hit, remaining 730 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   64260.17 ms /   730 tokens (   88.03 ms per token,    11.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =   55572.77 ms /   255 runs   (  217.93 ms per token,     4.59 tokens per second)\n",
      "llama_perf_context_print:       total time =  120028.38 ms /   985 tokens\n",
      " 47%|███████████████████████████████████                                        | 139/297 [5:10:41<4:59:27, 113.72s/it]Llama.generate: 21 prefix-match hit, remaining 406 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   34996.95 ms /   406 tokens (   86.20 ms per token,    11.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =   42547.93 ms /   201 runs   (  211.68 ms per token,     4.72 tokens per second)\n",
      "llama_perf_context_print:       total time =   77686.24 ms /   607 tokens\n",
      " 47%|███████████████████████████████████▎                                       | 140/297 [5:11:59<4:29:20, 102.93s/it]Llama.generate: 21 prefix-match hit, remaining 710 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   62737.22 ms /   710 tokens (   88.36 ms per token,    11.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =   22924.06 ms /   106 runs   (  216.26 ms per token,     4.62 tokens per second)\n",
      "llama_perf_context_print:       total time =   85716.28 ms /   816 tokens\n",
      " 47%|████████████████████████████████████                                        | 141/297 [5:13:25<4:14:16, 97.79s/it]Llama.generate: 21 prefix-match hit, remaining 1990 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  187762.06 ms /  1990 tokens (   94.35 ms per token,    10.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =   63496.59 ms /   255 runs   (  249.01 ms per token,     4.02 tokens per second)\n",
      "llama_perf_context_print:       total time =  251466.72 ms /  2245 tokens\n",
      " 48%|███████████████████████████████████▊                                       | 142/297 [5:17:36<6:11:48, 143.93s/it]Llama.generate: 21 prefix-match hit, remaining 717 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   63381.80 ms /   717 tokens (   88.40 ms per token,    11.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =   36207.93 ms /   167 runs   (  216.81 ms per token,     4.61 tokens per second)\n",
      "llama_perf_context_print:       total time =   99694.11 ms /   884 tokens\n",
      " 48%|████████████████████████████████████                                       | 143/297 [5:19:16<5:35:25, 130.69s/it]Llama.generate: 21 prefix-match hit, remaining 765 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   67827.65 ms /   765 tokens (   88.66 ms per token,    11.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =   56237.68 ms /   255 runs   (  220.54 ms per token,     4.53 tokens per second)\n",
      "llama_perf_context_print:       total time =  124271.89 ms /  1020 tokens\n",
      " 48%|████████████████████████████████████▎                                      | 144/297 [5:21:21<5:28:23, 128.78s/it]Llama.generate: 21 prefix-match hit, remaining 555 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   48553.17 ms /   555 tokens (   87.48 ms per token,    11.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =   34187.01 ms /   161 runs   (  212.34 ms per token,     4.71 tokens per second)\n",
      "llama_perf_context_print:       total time =   82841.09 ms /   716 tokens\n",
      " 49%|████████████████████████████████████▌                                      | 145/297 [5:22:43<4:51:23, 115.03s/it]Llama.generate: 21 prefix-match hit, remaining 449 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   39688.74 ms /   449 tokens (   88.39 ms per token,    11.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =   42139.82 ms /   194 runs   (  217.22 ms per token,     4.60 tokens per second)\n",
      "llama_perf_context_print:       total time =   81963.06 ms /   643 tokens\n",
      " 49%|████████████████████████████████████▊                                      | 146/297 [5:24:06<4:24:35, 105.14s/it]Llama.generate: 21 prefix-match hit, remaining 340 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   29276.17 ms /   340 tokens (   86.11 ms per token,    11.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =   54902.63 ms /   255 runs   (  215.30 ms per token,     4.64 tokens per second)\n",
      "llama_perf_context_print:       total time =   84379.96 ms /   595 tokens\n",
      " 49%|█████████████████████████████████████▌                                      | 147/297 [5:25:30<4:07:20, 98.93s/it]Llama.generate: 21 prefix-match hit, remaining 707 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   64626.30 ms /   707 tokens (   91.41 ms per token,    10.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =   55815.94 ms /   255 runs   (  218.89 ms per token,     4.57 tokens per second)\n",
      "llama_perf_context_print:       total time =  120636.64 ms /   962 tokens\n",
      " 50%|█████████████████████████████████████▎                                     | 148/297 [5:27:31<4:21:56, 105.48s/it]Llama.generate: 21 prefix-match hit, remaining 1136 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  102649.96 ms /  1136 tokens (   90.36 ms per token,    11.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =   50249.54 ms /   221 runs   (  227.37 ms per token,     4.40 tokens per second)\n",
      "llama_perf_context_print:       total time =  153055.59 ms /  1357 tokens\n",
      " 50%|█████████████████████████████████████▋                                     | 149/297 [5:30:04<4:55:27, 119.78s/it]Llama.generate: 21 prefix-match hit, remaining 773 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   68417.02 ms /   773 tokens (   88.51 ms per token,    11.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =   54815.41 ms /   248 runs   (  221.03 ms per token,     4.52 tokens per second)\n",
      "llama_perf_context_print:       total time =  123426.12 ms /  1021 tokens\n",
      " 51%|█████████████████████████████████████▉                                     | 150/297 [5:32:07<4:56:12, 120.90s/it]Llama.generate: 21 prefix-match hit, remaining 781 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   69561.33 ms /   781 tokens (   89.07 ms per token,    11.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =   55967.68 ms /   255 runs   (  219.48 ms per token,     4.56 tokens per second)\n",
      "llama_perf_context_print:       total time =  125728.62 ms /  1036 tokens\n",
      " 51%|██████████████████████████████████████▏                                    | 151/297 [5:34:13<4:57:47, 122.38s/it]Llama.generate: 21 prefix-match hit, remaining 804 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   71964.61 ms /   804 tokens (   89.51 ms per token,    11.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =   41443.81 ms /   188 runs   (  220.45 ms per token,     4.54 tokens per second)\n",
      "llama_perf_context_print:       total time =  113530.27 ms /   992 tokens\n",
      " 51%|██████████████████████████████████████▍                                    | 152/297 [5:36:07<4:49:24, 119.76s/it]Llama.generate: 21 prefix-match hit, remaining 731 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   64732.45 ms /   731 tokens (   88.55 ms per token,    11.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =   43765.42 ms /   201 runs   (  217.74 ms per token,     4.59 tokens per second)\n",
      "llama_perf_context_print:       total time =  108638.44 ms /   932 tokens\n",
      " 52%|██████████████████████████████████████▋                                    | 153/297 [5:37:56<4:39:28, 116.45s/it]Llama.generate: 21 prefix-match hit, remaining 806 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   71355.65 ms /   806 tokens (   88.53 ms per token,    11.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =   29728.74 ms /   134 runs   (  221.86 ms per token,     4.51 tokens per second)\n",
      "llama_perf_context_print:       total time =  101166.87 ms /   940 tokens\n",
      " 52%|██████████████████████████████████████▉                                    | 154/297 [5:39:37<4:26:41, 111.90s/it]Llama.generate: 21 prefix-match hit, remaining 961 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   85425.71 ms /   961 tokens (   88.89 ms per token,    11.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =   51854.56 ms /   230 runs   (  225.45 ms per token,     4.44 tokens per second)\n",
      "llama_perf_context_print:       total time =  137451.57 ms /  1191 tokens\n",
      " 52%|███████████████████████████████████████▏                                   | 155/297 [5:41:54<4:43:02, 119.59s/it]Llama.generate: 21 prefix-match hit, remaining 648 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   56846.16 ms /   648 tokens (   87.73 ms per token,    11.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =   42325.77 ms /   198 runs   (  213.77 ms per token,     4.68 tokens per second)\n",
      "llama_perf_context_print:       total time =   99302.63 ms /   846 tokens\n",
      " 53%|███████████████████████████████████████▍                                   | 156/297 [5:43:34<4:26:48, 113.53s/it]Llama.generate: 21 prefix-match hit, remaining 496 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   43507.29 ms /   496 tokens (   87.72 ms per token,    11.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =   54427.62 ms /   255 runs   (  213.44 ms per token,     4.69 tokens per second)\n",
      "llama_perf_context_print:       total time =   98129.99 ms /   751 tokens\n",
      " 53%|███████████████████████████████████████▋                                   | 157/297 [5:45:12<4:14:11, 108.94s/it]Llama.generate: 21 prefix-match hit, remaining 579 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   51093.68 ms /   579 tokens (   88.24 ms per token,    11.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =   55069.37 ms /   255 runs   (  215.96 ms per token,     4.63 tokens per second)\n",
      "llama_perf_context_print:       total time =  106358.77 ms /   834 tokens\n",
      " 53%|███████████████████████████████████████▉                                   | 158/297 [5:46:58<4:10:39, 108.20s/it]Llama.generate: 21 prefix-match hit, remaining 709 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   62456.78 ms /   709 tokens (   88.09 ms per token,    11.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =   51788.75 ms /   241 runs   (  214.89 ms per token,     4.65 tokens per second)\n",
      "llama_perf_context_print:       total time =  114424.64 ms /   950 tokens\n",
      " 54%|████████████████████████████████████████▏                                  | 159/297 [5:48:53<4:13:12, 110.09s/it]Llama.generate: 22 prefix-match hit, remaining 823 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   74416.42 ms /   823 tokens (   90.42 ms per token,    11.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =   56120.88 ms /   255 runs   (  220.08 ms per token,     4.54 tokens per second)\n",
      "llama_perf_context_print:       total time =  130735.23 ms /  1078 tokens\n",
      " 54%|████████████████████████████████████████▍                                  | 160/297 [5:51:04<4:25:35, 116.32s/it]Llama.generate: 21 prefix-match hit, remaining 609 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   53283.88 ms /   609 tokens (   87.49 ms per token,    11.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =   34845.40 ms /   162 runs   (  215.10 ms per token,     4.65 tokens per second)\n",
      "llama_perf_context_print:       total time =   88231.18 ms /   771 tokens\n",
      " 54%|████████████████████████████████████████▋                                  | 161/297 [5:52:32<4:04:37, 107.92s/it]Llama.generate: 21 prefix-match hit, remaining 431 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   37862.70 ms /   431 tokens (   87.85 ms per token,    11.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =   54503.17 ms /   255 runs   (  213.74 ms per token,     4.68 tokens per second)\n",
      "llama_perf_context_print:       total time =   92568.58 ms /   686 tokens\n",
      " 55%|████████████████████████████████████████▉                                  | 162/297 [5:54:05<3:52:31, 103.34s/it]Llama.generate: 21 prefix-match hit, remaining 810 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   71665.13 ms /   810 tokens (   88.48 ms per token,    11.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =   44528.02 ms /   203 runs   (  219.35 ms per token,     4.56 tokens per second)\n",
      "llama_perf_context_print:       total time =  116329.83 ms /  1013 tokens\n",
      " 55%|█████████████████████████████████████████▏                                 | 163/297 [5:56:01<3:59:34, 107.27s/it]Llama.generate: 22 prefix-match hit, remaining 773 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   68182.25 ms /   773 tokens (   88.20 ms per token,    11.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =   52704.11 ms /   239 runs   (  220.52 ms per token,     4.53 tokens per second)\n",
      "llama_perf_context_print:       total time =  121073.95 ms /  1012 tokens\n",
      " 55%|█████████████████████████████████████████▍                                 | 164/297 [5:58:02<4:07:01, 111.44s/it]Llama.generate: 24 prefix-match hit, remaining 689 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   60510.83 ms /   689 tokens (   87.82 ms per token,    11.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =   55920.95 ms /   255 runs   (  219.30 ms per token,     4.56 tokens per second)\n",
      "llama_perf_context_print:       total time =  116631.86 ms /   944 tokens\n",
      " 56%|█████████████████████████████████████████▋                                 | 165/297 [5:59:59<4:08:39, 113.02s/it]Llama.generate: 21 prefix-match hit, remaining 387 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   33241.77 ms /   387 tokens (   85.90 ms per token,    11.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =   54422.28 ms /   253 runs   (  215.11 ms per token,     4.65 tokens per second)\n",
      "llama_perf_context_print:       total time =   87869.35 ms /   640 tokens\n",
      " 56%|█████████████████████████████████████████▉                                 | 166/297 [6:01:27<3:50:21, 105.51s/it]Llama.generate: 21 prefix-match hit, remaining 827 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   74610.20 ms /   827 tokens (   90.22 ms per token,    11.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =   56335.35 ms /   255 runs   (  220.92 ms per token,     4.53 tokens per second)\n",
      "llama_perf_context_print:       total time =  131149.27 ms /  1082 tokens\n",
      " 56%|██████████████████████████████████████████▏                                | 167/297 [6:03:38<4:05:19, 113.23s/it]Llama.generate: 21 prefix-match hit, remaining 411 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   35310.01 ms /   411 tokens (   85.91 ms per token,    11.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =   37584.87 ms /   179 runs   (  209.97 ms per token,     4.76 tokens per second)\n",
      "llama_perf_context_print:       total time =   73009.12 ms /   590 tokens\n",
      " 57%|██████████████████████████████████████████▍                                | 168/297 [6:04:51<3:37:33, 101.19s/it]Llama.generate: 21 prefix-match hit, remaining 337 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   28957.88 ms /   337 tokens (   85.93 ms per token,    11.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =   48184.13 ms /   226 runs   (  213.20 ms per token,     4.69 tokens per second)\n",
      "llama_perf_context_print:       total time =   77311.72 ms /   563 tokens\n",
      " 57%|███████████████████████████████████████████▏                                | 169/297 [6:06:09<3:20:39, 94.06s/it]Llama.generate: 21 prefix-match hit, remaining 495 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   43233.17 ms /   495 tokens (   87.34 ms per token,    11.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =   54896.02 ms /   255 runs   (  215.28 ms per token,     4.65 tokens per second)\n",
      "llama_perf_context_print:       total time =   98337.38 ms /   750 tokens\n",
      " 57%|███████████████████████████████████████████▌                                | 170/297 [6:07:47<3:21:51, 95.37s/it]Llama.generate: 21 prefix-match hit, remaining 582 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   51145.82 ms /   582 tokens (   87.88 ms per token,    11.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =   48882.30 ms /   226 runs   (  216.29 ms per token,     4.62 tokens per second)\n",
      "llama_perf_context_print:       total time =  100193.56 ms /   808 tokens\n",
      " 58%|███████████████████████████████████████████▊                                | 171/297 [6:09:28<3:23:22, 96.84s/it]Llama.generate: 21 prefix-match hit, remaining 389 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   33560.07 ms /   389 tokens (   86.27 ms per token,    11.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =   45137.78 ms /   211 runs   (  213.92 ms per token,     4.67 tokens per second)\n",
      "llama_perf_context_print:       total time =   78851.46 ms /   600 tokens\n",
      " 58%|████████████████████████████████████████████                                | 172/297 [6:10:47<3:10:33, 91.47s/it]Llama.generate: 21 prefix-match hit, remaining 501 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   44187.47 ms /   501 tokens (   88.20 ms per token,    11.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =   44990.39 ms /   212 runs   (  212.22 ms per token,     4.71 tokens per second)\n",
      "llama_perf_context_print:       total time =   89332.72 ms /   713 tokens\n",
      " 58%|████████████████████████████████████████████▎                               | 173/297 [6:12:16<3:07:46, 90.86s/it]Llama.generate: 21 prefix-match hit, remaining 408 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   35421.30 ms /   408 tokens (   86.82 ms per token,    11.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =   40499.33 ms /   190 runs   (  213.15 ms per token,     4.69 tokens per second)\n",
      "llama_perf_context_print:       total time =   76052.57 ms /   598 tokens\n",
      " 59%|████████████████████████████████████████████▌                               | 174/297 [6:13:32<2:57:12, 86.44s/it]Llama.generate: 21 prefix-match hit, remaining 284 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   24414.30 ms /   284 tokens (   85.97 ms per token,    11.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =   53467.90 ms /   255 runs   (  209.68 ms per token,     4.77 tokens per second)\n",
      "llama_perf_context_print:       total time =   78089.22 ms /   539 tokens\n",
      " 59%|████████████████████████████████████████████▊                               | 175/297 [6:14:50<2:50:43, 83.96s/it]Llama.generate: 21 prefix-match hit, remaining 513 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   45512.50 ms /   513 tokens (   88.72 ms per token,    11.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =   47190.79 ms /   223 runs   (  211.62 ms per token,     4.73 tokens per second)\n",
      "llama_perf_context_print:       total time =   92864.12 ms /   736 tokens\n",
      " 59%|█████████████████████████████████████████████                               | 176/297 [6:16:23<2:54:45, 86.66s/it]Llama.generate: 21 prefix-match hit, remaining 716 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   63285.72 ms /   716 tokens (   88.39 ms per token,    11.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =   55339.95 ms /   255 runs   (  217.02 ms per token,     4.61 tokens per second)\n",
      "llama_perf_context_print:       total time =  118826.16 ms /   971 tokens\n",
      " 60%|█████████████████████████████████████████████▎                              | 177/297 [6:18:22<3:12:40, 96.34s/it]Llama.generate: 25 prefix-match hit, remaining 735 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   65590.25 ms /   735 tokens (   89.24 ms per token,    11.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =   55898.82 ms /   255 runs   (  219.21 ms per token,     4.56 tokens per second)\n",
      "llama_perf_context_print:       total time =  121681.95 ms /   990 tokens\n",
      " 60%|████████████████████████████████████████████▉                              | 178/297 [6:20:24<3:26:11, 103.97s/it]Llama.generate: 21 prefix-match hit, remaining 836 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   74306.94 ms /   836 tokens (   88.88 ms per token,    11.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =   36296.70 ms /   163 runs   (  222.68 ms per token,     4.49 tokens per second)\n",
      "llama_perf_context_print:       total time =  110708.16 ms /   999 tokens\n",
      " 60%|█████████████████████████████████████████████▏                             | 179/297 [6:22:15<3:28:30, 106.02s/it]Llama.generate: 21 prefix-match hit, remaining 868 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   77306.62 ms /   868 tokens (   89.06 ms per token,    11.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =   42778.56 ms /   191 runs   (  223.97 ms per token,     4.46 tokens per second)\n",
      "llama_perf_context_print:       total time =  120215.37 ms /  1059 tokens\n",
      " 61%|█████████████████████████████████████████████▍                             | 180/297 [6:24:15<3:35:05, 110.31s/it]Llama.generate: 21 prefix-match hit, remaining 587 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   51238.02 ms /   587 tokens (   87.29 ms per token,    11.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =   55471.33 ms /   255 runs   (  217.53 ms per token,     4.60 tokens per second)\n",
      "llama_perf_context_print:       total time =  106909.24 ms /   842 tokens\n",
      " 61%|█████████████████████████████████████████████▋                             | 181/297 [6:26:02<3:31:20, 109.32s/it]Llama.generate: 21 prefix-match hit, remaining 754 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   66784.54 ms /   754 tokens (   88.57 ms per token,    11.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =   56198.48 ms /   255 runs   (  220.39 ms per token,     4.54 tokens per second)\n",
      "llama_perf_context_print:       total time =  123186.32 ms /  1009 tokens\n",
      " 61%|█████████████████████████████████████████████▉                             | 182/297 [6:28:05<3:37:33, 113.51s/it]Llama.generate: 21 prefix-match hit, remaining 439 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   38115.93 ms /   439 tokens (   86.82 ms per token,    11.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =   54300.46 ms /   255 runs   (  212.94 ms per token,     4.70 tokens per second)\n",
      "llama_perf_context_print:       total time =   92617.44 ms /   694 tokens\n",
      " 62%|██████████████████████████████████████████████▏                            | 183/297 [6:29:38<3:23:48, 107.26s/it]Llama.generate: 126 prefix-match hit, remaining 768 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   68296.88 ms /   768 tokens (   88.93 ms per token,    11.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =   42469.36 ms /   191 runs   (  222.35 ms per token,     4.50 tokens per second)\n",
      "llama_perf_context_print:       total time =  110899.23 ms /   959 tokens\n",
      " 62%|██████████████████████████████████████████████▍                            | 184/297 [6:31:29<3:24:07, 108.38s/it]Llama.generate: 21 prefix-match hit, remaining 594 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   53372.36 ms /   594 tokens (   89.85 ms per token,    11.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =   54690.29 ms /   255 runs   (  214.47 ms per token,     4.66 tokens per second)\n",
      "llama_perf_context_print:       total time =  108262.13 ms /   849 tokens\n",
      " 62%|██████████████████████████████████████████████▋                            | 185/297 [6:33:17<3:22:17, 108.37s/it]Llama.generate: 21 prefix-match hit, remaining 582 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   51217.50 ms /   582 tokens (   88.00 ms per token,    11.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =   39541.17 ms /   183 runs   (  216.07 ms per token,     4.63 tokens per second)\n",
      "llama_perf_context_print:       total time =   90884.05 ms /   765 tokens\n",
      " 63%|██████████████████████████████████████████████▉                            | 186/297 [6:34:48<3:10:49, 103.15s/it]Llama.generate: 21 prefix-match hit, remaining 716 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   63309.95 ms /   716 tokens (   88.42 ms per token,    11.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =   53804.92 ms /   247 runs   (  217.83 ms per token,     4.59 tokens per second)\n",
      "llama_perf_context_print:       total time =  117312.43 ms /   963 tokens\n",
      " 63%|███████████████████████████████████████████████▏                           | 187/297 [6:36:46<3:16:57, 107.43s/it]Llama.generate: 25 prefix-match hit, remaining 733 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   64586.44 ms /   733 tokens (   88.11 ms per token,    11.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =   55831.95 ms /   255 runs   (  218.95 ms per token,     4.57 tokens per second)\n",
      "llama_perf_context_print:       total time =  120621.83 ms /   988 tokens\n",
      " 63%|███████████████████████████████████████████████▍                           | 188/297 [6:38:46<3:22:24, 111.42s/it]Llama.generate: 21 prefix-match hit, remaining 1016 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   90947.75 ms /  1016 tokens (   89.52 ms per token,    11.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =   30084.30 ms /   134 runs   (  224.51 ms per token,     4.45 tokens per second)\n",
      "llama_perf_context_print:       total time =  121110.99 ms /  1150 tokens\n",
      " 64%|███████████████████████████████████████████████▋                           | 189/297 [6:40:48<3:25:49, 114.35s/it]Llama.generate: 21 prefix-match hit, remaining 866 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   76475.18 ms /   866 tokens (   88.31 ms per token,    11.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =   37787.21 ms /   172 runs   (  219.69 ms per token,     4.55 tokens per second)\n",
      "llama_perf_context_print:       total time =  114369.39 ms /  1038 tokens\n",
      " 64%|███████████████████████████████████████████████▉                           | 190/297 [6:42:42<3:23:59, 114.38s/it]Llama.generate: 21 prefix-match hit, remaining 666 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   58854.24 ms /   666 tokens (   88.37 ms per token,    11.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =   46251.74 ms /   212 runs   (  218.17 ms per token,     4.58 tokens per second)\n",
      "llama_perf_context_print:       total time =  105264.31 ms /   878 tokens\n",
      " 64%|████████████████████████████████████████████████▏                          | 191/297 [6:44:27<3:17:17, 111.67s/it]Llama.generate: 21 prefix-match hit, remaining 716 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   62957.94 ms /   716 tokens (   87.93 ms per token,    11.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =   49303.18 ms /   225 runs   (  219.13 ms per token,     4.56 tokens per second)\n",
      "llama_perf_context_print:       total time =  112428.68 ms /   941 tokens\n",
      " 65%|████████████████████████████████████████████████▍                          | 192/297 [6:46:20<3:15:52, 111.93s/it]Llama.generate: 21 prefix-match hit, remaining 418 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   38073.61 ms /   418 tokens (   91.09 ms per token,    10.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =   41217.08 ms /   196 runs   (  210.29 ms per token,     4.76 tokens per second)\n",
      "llama_perf_context_print:       total time =   79424.19 ms /   614 tokens\n",
      " 65%|████████████████████████████████████████████████▋                          | 193/297 [6:47:39<2:57:09, 102.20s/it]Llama.generate: 126 prefix-match hit, remaining 479 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   42308.55 ms /   479 tokens (   88.33 ms per token,    11.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =   44445.98 ms /   207 runs   (  214.71 ms per token,     4.66 tokens per second)\n",
      "llama_perf_context_print:       total time =   86901.34 ms /   686 tokens\n",
      " 65%|█████████████████████████████████████████████████▋                          | 194/297 [6:49:06<2:47:36, 97.64s/it]Llama.generate: 21 prefix-match hit, remaining 596 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   52880.23 ms /   596 tokens (   88.73 ms per token,    11.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =   54422.40 ms /   255 runs   (  213.42 ms per token,     4.69 tokens per second)\n",
      "llama_perf_context_print:       total time =  107499.91 ms /   851 tokens\n",
      " 66%|█████████████████████████████████████████████████▏                         | 195/297 [6:50:54<2:51:03, 100.62s/it]Llama.generate: 21 prefix-match hit, remaining 1205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  109255.22 ms /  1205 tokens (   90.67 ms per token,    11.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =   28573.24 ms /   124 runs   (  230.43 ms per token,     4.34 tokens per second)\n",
      "llama_perf_context_print:       total time =  137895.46 ms /  1329 tokens\n",
      " 66%|█████████████████████████████████████████████████▍                         | 196/297 [6:53:12<3:08:15, 111.84s/it]Llama.generate: 21 prefix-match hit, remaining 412 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   35880.42 ms /   412 tokens (   87.09 ms per token,    11.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =   52984.78 ms /   250 runs   (  211.94 ms per token,     4.72 tokens per second)\n",
      "llama_perf_context_print:       total time =   89055.93 ms /   662 tokens\n",
      " 66%|█████████████████████████████████████████████████▋                         | 197/297 [6:54:41<2:55:03, 105.03s/it]Llama.generate: 21 prefix-match hit, remaining 474 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   41379.22 ms /   474 tokens (   87.30 ms per token,    11.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =   52876.17 ms /   250 runs   (  211.50 ms per token,     4.73 tokens per second)\n",
      "llama_perf_context_print:       total time =   94447.79 ms /   724 tokens\n",
      " 67%|██████████████████████████████████████████████████                         | 198/297 [6:56:16<2:48:07, 101.89s/it]Llama.generate: 21 prefix-match hit, remaining 1207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  110347.79 ms /  1207 tokens (   91.42 ms per token,    10.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =   57854.12 ms /   255 runs   (  226.88 ms per token,     4.41 tokens per second)\n",
      "llama_perf_context_print:       total time =  168398.47 ms /  1462 tokens\n",
      " 67%|██████████████████████████████████████████████████▎                        | 199/297 [6:59:04<3:19:03, 121.87s/it]Llama.generate: 21 prefix-match hit, remaining 920 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   81991.44 ms /   920 tokens (   89.12 ms per token,    11.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =   42116.54 ms /   189 runs   (  222.84 ms per token,     4.49 tokens per second)\n",
      "llama_perf_context_print:       total time =  124232.42 ms /  1109 tokens\n",
      " 67%|██████████████████████████████████████████████████▌                        | 200/297 [7:01:09<3:18:13, 122.61s/it]Llama.generate: 21 prefix-match hit, remaining 513 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   45224.05 ms /   513 tokens (   88.16 ms per token,    11.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =   53967.42 ms /   255 runs   (  211.64 ms per token,     4.73 tokens per second)\n",
      "llama_perf_context_print:       total time =   99396.80 ms /   768 tokens\n",
      " 68%|██████████████████████████████████████████████████▊                        | 201/297 [7:02:48<3:05:04, 115.67s/it]Llama.generate: 22 prefix-match hit, remaining 531 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   46798.13 ms /   531 tokens (   88.13 ms per token,    11.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =   48242.38 ms /   224 runs   (  215.37 ms per token,     4.64 tokens per second)\n",
      "llama_perf_context_print:       total time =   95205.14 ms /   755 tokens\n",
      " 68%|███████████████████████████████████████████████████                        | 202/297 [7:04:23<2:53:28, 109.56s/it]Llama.generate: 21 prefix-match hit, remaining 1992 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  186897.51 ms /  1992 tokens (   93.82 ms per token,    10.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =   63285.08 ms /   255 runs   (  248.18 ms per token,     4.03 tokens per second)\n",
      "llama_perf_context_print:       total time =  250380.32 ms /  2247 tokens\n",
      " 68%|███████████████████████████████████████████████████▎                       | 203/297 [7:08:34<3:57:52, 151.83s/it]Llama.generate: 21 prefix-match hit, remaining 1311 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  118578.33 ms /  1311 tokens (   90.45 ms per token,    11.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =   59012.92 ms /   255 runs   (  231.42 ms per token,     4.32 tokens per second)\n",
      "llama_perf_context_print:       total time =  177786.50 ms /  1566 tokens\n",
      " 69%|███████████████████████████████████████████████████▌                       | 204/297 [7:11:32<4:07:27, 159.65s/it]Llama.generate: 21 prefix-match hit, remaining 712 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   63118.55 ms /   712 tokens (   88.65 ms per token,    11.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =   55521.72 ms /   255 runs   (  217.73 ms per token,     4.59 tokens per second)\n",
      "llama_perf_context_print:       total time =  118847.87 ms /   967 tokens\n",
      " 69%|███████████████████████████████████████████████████▊                       | 205/297 [7:13:31<3:46:03, 147.43s/it]Llama.generate: 21 prefix-match hit, remaining 520 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   45608.52 ms /   520 tokens (   87.71 ms per token,    11.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =   55648.47 ms /   255 runs   (  218.23 ms per token,     4.58 tokens per second)\n",
      "llama_perf_context_print:       total time =  101460.83 ms /   775 tokens\n",
      " 69%|████████████████████████████████████████████████████                       | 206/297 [7:15:12<3:22:43, 133.67s/it]Llama.generate: 21 prefix-match hit, remaining 709 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   62451.97 ms /   709 tokens (   88.08 ms per token,    11.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =   55341.69 ms /   255 runs   (  217.03 ms per token,     4.61 tokens per second)\n",
      "llama_perf_context_print:       total time =  117992.80 ms /   964 tokens\n",
      " 70%|████████████████████████████████████████████████████▎                      | 207/297 [7:17:10<3:13:29, 128.99s/it]Llama.generate: 21 prefix-match hit, remaining 1460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  134160.46 ms /  1460 tokens (   91.89 ms per token,    10.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =   60340.01 ms /   255 runs   (  236.63 ms per token,     4.23 tokens per second)\n",
      "llama_perf_context_print:       total time =  194703.02 ms /  1715 tokens\n",
      " 70%|████████████████████████████████████████████████████▌                      | 208/297 [7:20:25<3:40:37, 148.74s/it]Llama.generate: 21 prefix-match hit, remaining 784 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   71933.45 ms /   784 tokens (   91.75 ms per token,    10.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =   56448.11 ms /   255 runs   (  221.37 ms per token,     4.52 tokens per second)\n",
      "llama_perf_context_print:       total time =  128589.47 ms /  1039 tokens\n",
      " 70%|████████████████████████████████████████████████████▊                      | 209/297 [7:22:34<3:29:19, 142.72s/it]Llama.generate: 21 prefix-match hit, remaining 1580 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  146490.46 ms /  1580 tokens (   92.72 ms per token,    10.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =   31272.96 ms /   132 runs   (  236.92 ms per token,     4.22 tokens per second)\n",
      "llama_perf_context_print:       total time =  177839.46 ms /  1712 tokens\n",
      " 71%|█████████████████████████████████████████████████████                      | 210/297 [7:25:32<3:42:16, 153.29s/it]Llama.generate: 21 prefix-match hit, remaining 1240 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  113054.79 ms /  1240 tokens (   91.17 ms per token,    10.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =   59064.63 ms /   255 runs   (  231.63 ms per token,     4.32 tokens per second)\n",
      "llama_perf_context_print:       total time =  172324.16 ms /  1495 tokens\n",
      " 71%|█████████████████████████████████████████████████████▎                     | 211/297 [7:28:24<3:47:56, 159.03s/it]Llama.generate: 21 prefix-match hit, remaining 1267 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  115117.40 ms /  1267 tokens (   90.86 ms per token,    11.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =   43069.98 ms /   189 runs   (  227.88 ms per token,     4.39 tokens per second)\n",
      "llama_perf_context_print:       total time =  158318.14 ms /  1456 tokens\n",
      " 71%|█████████████████████████████████████████████████████▌                     | 212/297 [7:31:03<3:45:02, 158.85s/it]Llama.generate: 21 prefix-match hit, remaining 1660 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  155062.12 ms /  1660 tokens (   93.41 ms per token,    10.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =   58939.87 ms /   248 runs   (  237.66 ms per token,     4.21 tokens per second)\n",
      "llama_perf_context_print:       total time =  214194.75 ms /  1908 tokens\n",
      " 72%|█████████████████████████████████████████████████████▊                     | 213/297 [7:34:37<4:05:40, 175.48s/it]Llama.generate: 21 prefix-match hit, remaining 1446 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  133380.94 ms /  1446 tokens (   92.24 ms per token,    10.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =   59145.71 ms /   255 runs   (  231.94 ms per token,     4.31 tokens per second)\n",
      "llama_perf_context_print:       total time =  192723.34 ms /  1701 tokens\n",
      " 72%|██████████████████████████████████████████████████████                     | 214/297 [7:37:50<4:09:56, 180.68s/it]Llama.generate: 21 prefix-match hit, remaining 952 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   85145.16 ms /   952 tokens (   89.44 ms per token,    11.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =   56448.44 ms /   255 runs   (  221.37 ms per token,     4.52 tokens per second)\n",
      "llama_perf_context_print:       total time =  141792.98 ms /  1207 tokens\n",
      " 72%|██████████████████████████████████████████████████████▎                    | 215/297 [7:40:12<3:51:01, 169.05s/it]Llama.generate: 21 prefix-match hit, remaining 500 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   44588.86 ms /   500 tokens (   89.18 ms per token,    11.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =   54771.03 ms /   255 runs   (  214.79 ms per token,     4.66 tokens per second)\n",
      "llama_perf_context_print:       total time =   99562.13 ms /   755 tokens\n",
      " 73%|██████████████████████████████████████████████████████▌                    | 216/297 [7:41:51<3:20:06, 148.23s/it]Llama.generate: 21 prefix-match hit, remaining 1256 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  115672.86 ms /  1256 tokens (   92.10 ms per token,    10.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =   19271.82 ms /    85 runs   (  226.73 ms per token,     4.41 tokens per second)\n",
      "llama_perf_context_print:       total time =  134987.01 ms /  1341 tokens\n",
      " 73%|██████████████████████████████████████████████████████▊                    | 217/297 [7:44:06<3:12:23, 144.29s/it]Llama.generate: 21 prefix-match hit, remaining 1798 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  170041.93 ms /  1798 tokens (   94.57 ms per token,    10.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =   60822.92 ms /   255 runs   (  238.52 ms per token,     4.19 tokens per second)\n",
      "llama_perf_context_print:       total time =  231049.43 ms /  2053 tokens\n",
      " 73%|███████████████████████████████████████████████████████                    | 218/297 [7:47:58<3:44:17, 170.35s/it]Llama.generate: 21 prefix-match hit, remaining 1923 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  182050.22 ms /  1923 tokens (   94.67 ms per token,    10.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =   62112.20 ms /   255 runs   (  243.58 ms per token,     4.11 tokens per second)\n",
      "llama_perf_context_print:       total time =  244368.84 ms /  2178 tokens\n",
      " 74%|███████████████████████████████████████████████████████▎                   | 219/297 [7:52:02<4:10:21, 192.58s/it]Llama.generate: 21 prefix-match hit, remaining 779 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   69645.07 ms /   779 tokens (   89.40 ms per token,    11.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =   56570.53 ms /   255 runs   (  221.85 ms per token,     4.51 tokens per second)\n",
      "llama_perf_context_print:       total time =  126422.26 ms /  1034 tokens\n",
      " 74%|███████████████████████████████████████████████████████▌                   | 220/297 [7:54:09<3:41:43, 172.77s/it]Llama.generate: 21 prefix-match hit, remaining 374 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   32472.92 ms /   374 tokens (   86.83 ms per token,    11.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =   35585.70 ms /   171 runs   (  208.10 ms per token,     4.81 tokens per second)\n",
      "llama_perf_context_print:       total time =   68167.02 ms /   545 tokens\n",
      " 74%|███████████████████████████████████████████████████████▊                   | 221/297 [7:55:17<2:59:07, 141.41s/it]Llama.generate: 21 prefix-match hit, remaining 541 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   47819.15 ms /   541 tokens (   88.39 ms per token,    11.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =   42754.82 ms /   199 runs   (  214.85 ms per token,     4.65 tokens per second)\n",
      "llama_perf_context_print:       total time =   90713.79 ms /   740 tokens\n",
      " 75%|████████████████████████████████████████████████████████                   | 222/297 [7:56:48<2:37:47, 126.23s/it]Llama.generate: 21 prefix-match hit, remaining 507 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   45876.11 ms /   507 tokens (   90.49 ms per token,    11.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =   54054.74 ms /   255 runs   (  211.98 ms per token,     4.72 tokens per second)\n",
      "llama_perf_context_print:       total time =  100131.65 ms /   762 tokens\n",
      " 75%|████████████████████████████████████████████████████████▎                  | 223/297 [7:58:28<2:26:03, 118.43s/it]Llama.generate: 21 prefix-match hit, remaining 804 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   71808.92 ms /   804 tokens (   89.31 ms per token,    11.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =   55463.87 ms /   255 runs   (  217.51 ms per token,     4.60 tokens per second)\n",
      "llama_perf_context_print:       total time =  127463.68 ms /  1059 tokens\n",
      " 75%|████████████████████████████████████████████████████████▌                  | 224/297 [8:00:35<2:27:25, 121.17s/it]Llama.generate: 21 prefix-match hit, remaining 519 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   45528.40 ms /   519 tokens (   87.72 ms per token,    11.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =   54491.82 ms /   255 runs   (  213.69 ms per token,     4.68 tokens per second)\n",
      "llama_perf_context_print:       total time =  100222.64 ms /   774 tokens\n",
      " 76%|████████████████████████████████████████████████████████▊                  | 225/297 [8:02:16<2:17:53, 114.91s/it]Llama.generate: 21 prefix-match hit, remaining 468 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   41077.07 ms /   468 tokens (   87.77 ms per token,    11.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =   53895.44 ms /   254 runs   (  212.19 ms per token,     4.71 tokens per second)\n",
      "llama_perf_context_print:       total time =   95176.69 ms /   722 tokens\n",
      " 76%|█████████████████████████████████████████████████████████                  | 226/297 [8:03:51<2:09:00, 109.02s/it]Llama.generate: 21 prefix-match hit, remaining 505 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   44148.15 ms /   505 tokens (   87.42 ms per token,    11.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =   53620.99 ms /   255 runs   (  210.28 ms per token,     4.76 tokens per second)\n",
      "llama_perf_context_print:       total time =   97969.43 ms /   760 tokens\n",
      " 76%|█████████████████████████████████████████████████████████▎                 | 227/297 [8:05:29<2:03:21, 105.74s/it]Llama.generate: 22 prefix-match hit, remaining 643 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   56669.28 ms /   643 tokens (   88.13 ms per token,    11.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =   32157.53 ms /   149 runs   (  215.82 ms per token,     4.63 tokens per second)\n",
      "llama_perf_context_print:       total time =   88921.77 ms /   792 tokens\n",
      " 77%|█████████████████████████████████████████████████████████▌                 | 228/297 [8:06:58<1:55:49, 100.72s/it]Llama.generate: 21 prefix-match hit, remaining 908 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   81139.18 ms /   908 tokens (   89.36 ms per token,    11.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =   52898.94 ms /   239 runs   (  221.33 ms per token,     4.52 tokens per second)\n",
      "llama_perf_context_print:       total time =  134233.59 ms /  1147 tokens\n",
      " 77%|█████████████████████████████████████████████████████████▊                 | 229/297 [8:09:12<2:05:34, 110.80s/it]Llama.generate: 21 prefix-match hit, remaining 990 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   90006.05 ms /   990 tokens (   90.92 ms per token,    11.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =   35667.78 ms /   158 runs   (  225.75 ms per token,     4.43 tokens per second)\n",
      "llama_perf_context_print:       total time =  125773.42 ms /  1148 tokens\n",
      " 77%|██████████████████████████████████████████████████████████                 | 230/297 [8:11:18<2:08:46, 115.32s/it]Llama.generate: 21 prefix-match hit, remaining 1194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  108177.11 ms /  1194 tokens (   90.60 ms per token,    11.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =   38225.32 ms /   171 runs   (  223.54 ms per token,     4.47 tokens per second)\n",
      "llama_perf_context_print:       total time =  146507.18 ms /  1365 tokens\n",
      " 78%|██████████████████████████████████████████████████████████▎                | 231/297 [8:13:45<2:17:10, 124.71s/it]Llama.generate: 21 prefix-match hit, remaining 1295 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  117489.42 ms /  1295 tokens (   90.73 ms per token,    11.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =   55576.08 ms /   244 runs   (  227.77 ms per token,     4.39 tokens per second)\n",
      "llama_perf_context_print:       total time =  173250.58 ms /  1539 tokens\n",
      " 78%|██████████████████████████████████████████████████████████▌                | 232/297 [8:16:38<2:30:54, 139.30s/it]Llama.generate: 21 prefix-match hit, remaining 812 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   72319.45 ms /   812 tokens (   89.06 ms per token,    11.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =   54920.71 ms /   255 runs   (  215.38 ms per token,     4.64 tokens per second)\n",
      "llama_perf_context_print:       total time =  127434.55 ms /  1067 tokens\n",
      " 78%|██████████████████████████████████████████████████████████▊                | 233/297 [8:18:46<2:24:49, 135.77s/it]Llama.generate: 21 prefix-match hit, remaining 464 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   40470.33 ms /   464 tokens (   87.22 ms per token,    11.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =   53682.58 ms /   255 runs   (  210.52 ms per token,     4.75 tokens per second)\n",
      "llama_perf_context_print:       total time =   94345.74 ms /   719 tokens\n",
      " 79%|███████████████████████████████████████████████████████████                | 234/297 [8:20:20<2:09:32, 123.37s/it]Llama.generate: 22 prefix-match hit, remaining 539 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   47893.57 ms /   539 tokens (   88.86 ms per token,    11.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =   54756.99 ms /   255 runs   (  214.73 ms per token,     4.66 tokens per second)\n",
      "llama_perf_context_print:       total time =  102852.68 ms /   794 tokens\n",
      " 79%|███████████████████████████████████████████████████████████▎               | 235/297 [8:22:03<2:01:08, 117.24s/it]Llama.generate: 21 prefix-match hit, remaining 504 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   44300.41 ms /   504 tokens (   87.90 ms per token,    11.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =   53748.31 ms /   255 runs   (  210.78 ms per token,     4.74 tokens per second)\n",
      "llama_perf_context_print:       total time =   98241.91 ms /   759 tokens\n",
      " 79%|███████████████████████████████████████████████████████████▌               | 236/297 [8:23:41<1:53:25, 111.57s/it]Llama.generate: 21 prefix-match hit, remaining 644 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   57357.46 ms /   644 tokens (   89.06 ms per token,    11.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =   27938.83 ms /   132 runs   (  211.66 ms per token,     4.72 tokens per second)\n",
      "llama_perf_context_print:       total time =   85374.26 ms /   776 tokens\n",
      " 80%|███████████████████████████████████████████████████████████▊               | 237/297 [8:25:07<1:43:44, 103.74s/it]Llama.generate: 21 prefix-match hit, remaining 553 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   48908.46 ms /   553 tokens (   88.44 ms per token,    11.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =   41343.98 ms /   195 runs   (  212.02 ms per token,     4.72 tokens per second)\n",
      "llama_perf_context_print:       total time =   90383.70 ms /   748 tokens\n",
      " 80%|████████████████████████████████████████████████████████████▉               | 238/297 [8:26:37<1:38:06, 99.76s/it]Llama.generate: 21 prefix-match hit, remaining 586 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   52188.26 ms /   586 tokens (   89.06 ms per token,    11.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =   54411.93 ms /   255 runs   (  213.38 ms per token,     4.69 tokens per second)\n",
      "llama_perf_context_print:       total time =  106802.99 ms /   841 tokens\n",
      " 80%|████████████████████████████████████████████████████████████▎              | 239/297 [8:28:24<1:38:30, 101.91s/it]Llama.generate: 21 prefix-match hit, remaining 1079 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   96783.45 ms /  1079 tokens (   89.70 ms per token,    11.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =   57415.05 ms /   255 runs   (  225.16 ms per token,     4.44 tokens per second)\n",
      "llama_perf_context_print:       total time =  154400.57 ms /  1334 tokens\n",
      " 81%|████████████████████████████████████████████████████████████▌              | 240/297 [8:30:59<1:51:47, 117.68s/it]Llama.generate: 21 prefix-match hit, remaining 254 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   22046.80 ms /   254 tokens (   86.80 ms per token,    11.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =   53141.11 ms /   255 runs   (  208.40 ms per token,     4.80 tokens per second)\n",
      "llama_perf_context_print:       total time =   75390.87 ms /   509 tokens\n",
      " 81%|████████████████████████████████████████████████████████████▊              | 241/297 [8:32:14<1:38:01, 105.02s/it]Llama.generate: 21 prefix-match hit, remaining 837 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   74812.59 ms /   837 tokens (   89.38 ms per token,    11.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =   55597.31 ms /   255 runs   (  218.03 ms per token,     4.59 tokens per second)\n",
      "llama_perf_context_print:       total time =  130603.96 ms /  1092 tokens\n",
      " 81%|█████████████████████████████████████████████████████████████              | 242/297 [8:34:25<1:43:19, 112.72s/it]Llama.generate: 21 prefix-match hit, remaining 505 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   44071.20 ms /   505 tokens (   87.27 ms per token,    11.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =   53655.20 ms /   255 runs   (  210.41 ms per token,     4.75 tokens per second)\n",
      "llama_perf_context_print:       total time =   97921.70 ms /   760 tokens\n",
      " 82%|█████████████████████████████████████████████████████████████▎             | 243/297 [8:36:03<1:37:28, 108.31s/it]Llama.generate: 21 prefix-match hit, remaining 566 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   52533.10 ms /   566 tokens (   92.81 ms per token,    10.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =   43459.59 ms /   204 runs   (  213.04 ms per token,     4.69 tokens per second)\n",
      "llama_perf_context_print:       total time =   96139.11 ms /   770 tokens\n",
      " 82%|█████████████████████████████████████████████████████████████▌             | 244/297 [8:37:39<1:32:28, 104.69s/it]Llama.generate: 21 prefix-match hit, remaining 828 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   73527.42 ms /   828 tokens (   88.80 ms per token,    11.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =   55914.29 ms /   255 runs   (  219.27 ms per token,     4.56 tokens per second)\n",
      "llama_perf_context_print:       total time =  129641.52 ms /  1083 tokens\n",
      " 82%|█████████████████████████████████████████████████████████████▊             | 245/297 [8:39:49<1:37:14, 112.21s/it]Llama.generate: 21 prefix-match hit, remaining 1616 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  149791.61 ms /  1616 tokens (   92.69 ms per token,    10.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =   32247.35 ms /   138 runs   (  233.68 ms per token,     4.28 tokens per second)\n",
      "llama_perf_context_print:       total time =  182120.48 ms /  1754 tokens\n",
      " 83%|██████████████████████████████████████████████████████████████             | 246/297 [8:42:51<1:53:13, 133.21s/it]Llama.generate: 21 prefix-match hit, remaining 669 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   60187.67 ms /   669 tokens (   89.97 ms per token,    11.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =   36421.67 ms /   172 runs   (  211.75 ms per token,     4.72 tokens per second)\n",
      "llama_perf_context_print:       total time =   96716.87 ms /   841 tokens\n",
      " 83%|██████████████████████████████████████████████████████████████▎            | 247/297 [8:44:28<1:41:54, 122.29s/it]Llama.generate: 21 prefix-match hit, remaining 574 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   50675.09 ms /   574 tokens (   88.28 ms per token,    11.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =   54814.39 ms /   255 runs   (  214.96 ms per token,     4.65 tokens per second)\n",
      "llama_perf_context_print:       total time =  105686.72 ms /   829 tokens\n",
      " 84%|██████████████████████████████████████████████████████████████▋            | 248/297 [8:46:14<1:35:49, 117.34s/it]Llama.generate: 21 prefix-match hit, remaining 862 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   77059.49 ms /   862 tokens (   89.40 ms per token,    11.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =   55834.89 ms /   255 runs   (  218.96 ms per token,     4.57 tokens per second)\n",
      "llama_perf_context_print:       total time =  133092.97 ms /  1117 tokens\n",
      " 84%|██████████████████████████████████████████████████████████████▉            | 249/297 [8:48:27<1:37:40, 122.09s/it]Llama.generate: 21 prefix-match hit, remaining 655 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   59682.82 ms /   655 tokens (   91.12 ms per token,    10.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =   55449.74 ms /   255 runs   (  217.45 ms per token,     4.60 tokens per second)\n",
      "llama_perf_context_print:       total time =  115337.23 ms /   910 tokens\n",
      " 84%|███████████████████████████████████████████████████████████████▏           | 250/297 [8:50:22<1:34:04, 120.09s/it]Llama.generate: 21 prefix-match hit, remaining 831 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   74220.29 ms /   831 tokens (   89.31 ms per token,    11.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =   30076.70 ms /   139 runs   (  216.38 ms per token,     4.62 tokens per second)\n",
      "llama_perf_context_print:       total time =  104376.51 ms /   970 tokens\n",
      " 85%|███████████████████████████████████████████████████████████████▍           | 251/297 [8:52:07<1:28:28, 115.41s/it]Llama.generate: 21 prefix-match hit, remaining 1328 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  122172.65 ms /  1328 tokens (   92.00 ms per token,    10.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =   54360.08 ms /   238 runs   (  228.40 ms per token,     4.38 tokens per second)\n",
      "llama_perf_context_print:       total time =  176704.11 ms /  1566 tokens\n",
      " 85%|███████████████████████████████████████████████████████████████▋           | 252/297 [8:55:04<1:40:22, 133.82s/it]Llama.generate: 21 prefix-match hit, remaining 664 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   58679.47 ms /   664 tokens (   88.37 ms per token,    11.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =   44181.55 ms /   192 runs   (  230.11 ms per token,     4.35 tokens per second)\n",
      "llama_perf_context_print:       total time =  102990.66 ms /   856 tokens\n",
      " 85%|███████████████████████████████████████████████████████████████▉           | 253/297 [8:56:47<1:31:22, 124.60s/it]Llama.generate: 21 prefix-match hit, remaining 574 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   53996.50 ms /   574 tokens (   94.07 ms per token,    10.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =   53247.16 ms /   255 runs   (  208.81 ms per token,     4.79 tokens per second)\n",
      "llama_perf_context_print:       total time =  107430.10 ms /   829 tokens\n",
      " 86%|████████████████████████████████████████████████████████████████▏          | 254/297 [8:58:34<1:25:37, 119.48s/it]Llama.generate: 21 prefix-match hit, remaining 796 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   72248.07 ms /   796 tokens (   90.76 ms per token,    11.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =   56467.35 ms /   255 runs   (  221.44 ms per token,     4.52 tokens per second)\n",
      "llama_perf_context_print:       total time =  128917.75 ms /  1051 tokens\n",
      " 86%|████████████████████████████████████████████████████████████████▍          | 255/297 [9:00:43<1:25:38, 122.34s/it]Llama.generate: 21 prefix-match hit, remaining 1145 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  105758.56 ms /  1145 tokens (   92.37 ms per token,    10.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =   18186.27 ms /    80 runs   (  227.33 ms per token,     4.40 tokens per second)\n",
      "llama_perf_context_print:       total time =  123983.61 ms /  1225 tokens\n",
      " 86%|████████████████████████████████████████████████████████████████▋          | 256/297 [9:02:47<1:23:57, 122.86s/it]Llama.generate: 21 prefix-match hit, remaining 667 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   59985.33 ms /   667 tokens (   89.93 ms per token,    11.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =   56290.72 ms /   255 runs   (  220.75 ms per token,     4.53 tokens per second)\n",
      "llama_perf_context_print:       total time =  116487.94 ms /   922 tokens\n",
      " 87%|████████████████████████████████████████████████████████████████▉          | 257/297 [9:04:44<1:20:39, 120.98s/it]Llama.generate: 21 prefix-match hit, remaining 840 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   75960.76 ms /   840 tokens (   90.43 ms per token,    11.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =   45622.40 ms /   211 runs   (  216.22 ms per token,     4.62 tokens per second)\n",
      "llama_perf_context_print:       total time =  121728.24 ms /  1051 tokens\n",
      " 87%|█████████████████████████████████████████████████████████████████▏         | 258/297 [9:06:46<1:18:48, 121.24s/it]Llama.generate: 21 prefix-match hit, remaining 177 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   15059.40 ms /   177 tokens (   85.08 ms per token,    11.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8568.13 ms /    42 runs   (  204.00 ms per token,     4.90 tokens per second)\n",
      "llama_perf_context_print:       total time =   23645.11 ms /   219 tokens\n",
      " 87%|████████████████████████████████████████████████████████████████████          | 259/297 [9:07:10<58:15, 91.99s/it]Llama.generate: 21 prefix-match hit, remaining 1112 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  103203.38 ms /  1112 tokens (   92.81 ms per token,    10.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =   39511.06 ms /   172 runs   (  229.72 ms per token,     4.35 tokens per second)\n",
      "llama_perf_context_print:       total time =  142828.58 ms /  1284 tokens\n",
      " 88%|█████████████████████████████████████████████████████████████████▋         | 260/297 [9:09:32<1:06:09, 107.27s/it]Llama.generate: 21 prefix-match hit, remaining 238 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   21130.73 ms /   238 tokens (   88.78 ms per token,    11.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =   31307.83 ms /   152 runs   (  205.97 ms per token,     4.86 tokens per second)\n",
      "llama_perf_context_print:       total time =   52531.44 ms /   390 tokens\n",
      " 88%|████████████████████████████████████████████████████████████████████▌         | 261/297 [9:10:25<54:31, 90.88s/it]Llama.generate: 21 prefix-match hit, remaining 1087 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   99222.87 ms /  1087 tokens (   91.28 ms per token,    10.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =   29154.83 ms /   130 runs   (  224.27 ms per token,     4.46 tokens per second)\n",
      "llama_perf_context_print:       total time =  128451.15 ms /  1217 tokens\n",
      " 88%|███████████████████████████████████████████████████████████████████▉         | 262/297 [9:12:34<59:36, 102.18s/it]Llama.generate: 21 prefix-match hit, remaining 500 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   44408.31 ms /   500 tokens (   88.82 ms per token,    11.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =   39450.11 ms /   186 runs   (  212.10 ms per token,     4.71 tokens per second)\n",
      "llama_perf_context_print:       total time =   83988.19 ms /   686 tokens\n",
      " 89%|█████████████████████████████████████████████████████████████████████         | 263/297 [9:13:58<54:49, 96.75s/it]Llama.generate: 21 prefix-match hit, remaining 675 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   59904.34 ms /   675 tokens (   88.75 ms per token,    11.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =   55527.54 ms /   255 runs   (  217.76 ms per token,     4.59 tokens per second)\n",
      "llama_perf_context_print:       total time =  115628.37 ms /   930 tokens\n",
      " 89%|████████████████████████████████████████████████████████████████████▍        | 264/297 [9:15:53<56:20, 102.44s/it]Llama.generate: 21 prefix-match hit, remaining 1306 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  120996.54 ms /  1306 tokens (   92.65 ms per token,    10.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =   58167.64 ms /   255 runs   (  228.11 ms per token,     4.38 tokens per second)\n",
      "llama_perf_context_print:       total time =  179365.02 ms /  1561 tokens\n",
      " 89%|██████████████████████████████████████████████████████████████████▉        | 265/297 [9:18:53<1:06:57, 125.55s/it]Llama.generate: 21 prefix-match hit, remaining 527 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   46849.60 ms /   527 tokens (   88.90 ms per token,    11.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =   24446.88 ms /   114 runs   (  214.45 ms per token,     4.66 tokens per second)\n",
      "llama_perf_context_print:       total time =   71357.38 ms /   641 tokens\n",
      " 90%|████████████████████████████████████████████████████████████████████▉        | 266/297 [9:20:04<56:28, 109.32s/it]Llama.generate: 21 prefix-match hit, remaining 840 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   76776.62 ms /   840 tokens (   91.40 ms per token,    10.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =   55711.55 ms /   255 runs   (  218.48 ms per token,     4.58 tokens per second)\n",
      "llama_perf_context_print:       total time =  132682.77 ms /  1095 tokens\n",
      " 90%|█████████████████████████████████████████████████████████████████████▏       | 267/297 [9:22:17<58:10, 116.36s/it]Llama.generate: 21 prefix-match hit, remaining 584 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   51931.34 ms /   584 tokens (   88.92 ms per token,    11.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =   54545.55 ms /   255 runs   (  213.90 ms per token,     4.67 tokens per second)\n",
      "llama_perf_context_print:       total time =  106676.17 ms /   839 tokens\n",
      " 90%|█████████████████████████████████████████████████████████████████████▍       | 268/297 [9:24:04<54:51, 113.49s/it]Llama.generate: 21 prefix-match hit, remaining 308 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   26517.92 ms /   308 tokens (   86.10 ms per token,    11.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =   43366.63 ms /   206 runs   (  210.52 ms per token,     4.75 tokens per second)\n",
      "llama_perf_context_print:       total time =   70024.42 ms /   514 tokens\n",
      " 91%|█████████████████████████████████████████████████████████████████████▋       | 269/297 [9:25:14<46:53, 100.47s/it]Llama.generate: 21 prefix-match hit, remaining 475 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   41943.74 ms /   475 tokens (   88.30 ms per token,    11.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =   44076.13 ms /   206 runs   (  213.96 ms per token,     4.67 tokens per second)\n",
      "llama_perf_context_print:       total time =   86165.06 ms /   681 tokens\n",
      " 91%|██████████████████████████████████████████████████████████████████████▉       | 270/297 [9:26:40<43:17, 96.21s/it]Llama.generate: 21 prefix-match hit, remaining 408 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   35516.57 ms /   408 tokens (   87.05 ms per token,    11.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =   53884.79 ms /   255 runs   (  211.31 ms per token,     4.73 tokens per second)\n",
      "llama_perf_context_print:       total time =   89601.43 ms /   663 tokens\n",
      " 91%|███████████████████████████████████████████████████████████████████████▏      | 271/297 [9:28:10<40:50, 94.25s/it]Llama.generate: 21 prefix-match hit, remaining 510 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   46949.97 ms /   510 tokens (   92.06 ms per token,    10.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =   45494.33 ms /   214 runs   (  212.59 ms per token,     4.70 tokens per second)\n",
      "llama_perf_context_print:       total time =   92597.53 ms /   724 tokens\n",
      " 92%|███████████████████████████████████████████████████████████████████████▍      | 272/297 [9:29:43<39:04, 93.79s/it]Llama.generate: 22 prefix-match hit, remaining 576 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   51688.37 ms /   576 tokens (   89.74 ms per token,    11.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =   54499.28 ms /   255 runs   (  213.72 ms per token,     4.68 tokens per second)\n",
      "llama_perf_context_print:       total time =  106390.57 ms /   831 tokens\n",
      " 92%|███████████████████████████████████████████████████████████████████████▋      | 273/297 [9:31:29<39:02, 97.60s/it]Llama.generate: 21 prefix-match hit, remaining 828 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   73543.94 ms /   828 tokens (   88.82 ms per token,    11.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =   55969.37 ms /   255 runs   (  219.49 ms per token,     4.56 tokens per second)\n",
      "llama_perf_context_print:       total time =  129712.34 ms /  1083 tokens\n",
      " 92%|███████████████████████████████████████████████████████████████████████      | 274/297 [9:33:39<41:07, 107.26s/it]Llama.generate: 21 prefix-match hit, remaining 390 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   33977.91 ms /   390 tokens (   87.12 ms per token,    11.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =   32175.72 ms /   154 runs   (  208.93 ms per token,     4.79 tokens per second)\n",
      "llama_perf_context_print:       total time =   66246.68 ms /   544 tokens\n",
      " 93%|████████████████████████████████████████████████████████████████████████▏     | 275/297 [9:34:45<34:49, 94.99s/it]Llama.generate: 21 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   39367.76 ms /   450 tokens (   87.48 ms per token,    11.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =   31510.21 ms /   150 runs   (  210.07 ms per token,     4.76 tokens per second)\n",
      "llama_perf_context_print:       total time =   70970.13 ms /   600 tokens\n",
      " 93%|████████████████████████████████████████████████████████████████████████▍     | 276/297 [9:35:56<30:43, 87.81s/it]Llama.generate: 21 prefix-match hit, remaining 585 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   51936.30 ms /   585 tokens (   88.78 ms per token,    11.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =   54661.80 ms /   255 runs   (  214.36 ms per token,     4.67 tokens per second)\n",
      "llama_perf_context_print:       total time =  106793.24 ms /   840 tokens\n",
      " 93%|████████████████████████████████████████████████████████████████████████▋     | 277/297 [9:37:43<31:10, 93.53s/it]Llama.generate: 21 prefix-match hit, remaining 597 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   52567.93 ms /   597 tokens (   88.05 ms per token,    11.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =   48614.88 ms /   230 runs   (  211.37 ms per token,     4.73 tokens per second)\n",
      "llama_perf_context_print:       total time =  101351.54 ms /   827 tokens\n",
      " 94%|█████████████████████████████████████████████████████████████████████████     | 278/297 [9:39:25<30:22, 95.90s/it]Llama.generate: 21 prefix-match hit, remaining 525 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   48686.04 ms /   525 tokens (   92.74 ms per token,    10.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =   49749.48 ms /   228 runs   (  218.20 ms per token,     4.58 tokens per second)\n",
      "llama_perf_context_print:       total time =   98601.13 ms /   753 tokens\n",
      " 94%|█████████████████████████████████████████████████████████████████████████▎    | 279/297 [9:41:03<29:01, 96.74s/it]Llama.generate: 21 prefix-match hit, remaining 1265 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  117492.52 ms /  1265 tokens (   92.88 ms per token,    10.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =   70445.42 ms /   255 runs   (  276.26 ms per token,     3.62 tokens per second)\n",
      "llama_perf_context_print:       total time =  188200.67 ms /  1520 tokens\n",
      " 94%|████████████████████████████████████████████████████████████████████████▌    | 280/297 [9:44:12<35:11, 124.21s/it]Llama.generate: 21 prefix-match hit, remaining 993 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  294100.09 ms /   993 tokens (  296.17 ms per token,     3.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =  102786.60 ms /   192 runs   (  535.35 ms per token,     1.87 tokens per second)\n",
      "llama_perf_context_print:       total time =  397244.87 ms /  1185 tokens\n",
      " 95%|████████████████████████████████████████████████████████████████████████▊    | 281/297 [9:50:49<54:58, 206.19s/it]Llama.generate: 21 prefix-match hit, remaining 750 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  230816.39 ms /   750 tokens (  307.76 ms per token,     3.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =  117751.91 ms /   227 runs   (  518.73 ms per token,     1.93 tokens per second)\n",
      "llama_perf_context_print:       total time =  349015.88 ms /   977 tokens\n",
      " 95%|███████████████████████████████████████████████████████████████████████▏   | 282/297 [9:56:38<1:02:16, 249.11s/it]Llama.generate: 21 prefix-match hit, remaining 991 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  146157.56 ms /   991 tokens (  147.48 ms per token,     6.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =   33007.04 ms /   150 runs   (  220.05 ms per token,     4.54 tokens per second)\n",
      "llama_perf_context_print:       total time =  179261.09 ms /  1141 tokens\n",
      " 95%|█████████████████████████████████████████████████████████████████████████▎   | 283/297 [9:59:38<53:15, 228.22s/it]Llama.generate: 21 prefix-match hit, remaining 425 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   36648.28 ms /   425 tokens (   86.23 ms per token,    11.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =   53707.97 ms /   255 runs   (  210.62 ms per token,     4.75 tokens per second)\n",
      "llama_perf_context_print:       total time =   90551.21 ms /   680 tokens\n",
      " 96%|████████████████████████████████████████████████████████████████████████▋   | 284/297 [10:01:09<40:30, 186.95s/it]Llama.generate: 21 prefix-match hit, remaining 696 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   62796.49 ms /   696 tokens (   90.22 ms per token,    11.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =   28101.87 ms /   128 runs   (  219.55 ms per token,     4.55 tokens per second)\n",
      "llama_perf_context_print:       total time =   90976.82 ms /   824 tokens\n",
      " 96%|████████████████████████████████████████████████████████████████████████▉   | 285/297 [10:02:40<31:38, 158.18s/it]Llama.generate: 21 prefix-match hit, remaining 750 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   68942.65 ms /   750 tokens (   91.92 ms per token,    10.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =   57655.41 ms /   255 runs   (  226.10 ms per token,     4.42 tokens per second)\n",
      "llama_perf_context_print:       total time =  126801.33 ms /  1005 tokens\n",
      " 96%|█████████████████████████████████████████████████████████████████████████▏  | 286/297 [10:04:47<27:16, 148.80s/it]Llama.generate: 21 prefix-match hit, remaining 510 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   43736.31 ms /   510 tokens (   85.76 ms per token,    11.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =   32581.56 ms /   150 runs   (  217.21 ms per token,     4.60 tokens per second)\n",
      "llama_perf_context_print:       total time =   76411.49 ms /   660 tokens\n",
      " 97%|█████████████████████████████████████████████████████████████████████████▍  | 287/297 [10:06:03<21:11, 127.11s/it]Llama.generate: 21 prefix-match hit, remaining 1311 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  124561.83 ms /  1311 tokens (   95.01 ms per token,    10.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =   62122.49 ms /   255 runs   (  243.62 ms per token,     4.10 tokens per second)\n",
      "llama_perf_context_print:       total time =  186886.19 ms /  1566 tokens\n",
      " 97%|█████████████████████████████████████████████████████████████████████████▋  | 288/297 [10:09:10<21:45, 145.08s/it]Llama.generate: 21 prefix-match hit, remaining 1327 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =  122218.81 ms /  1327 tokens (   92.10 ms per token,    10.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =   58396.62 ms /   255 runs   (  229.01 ms per token,     4.37 tokens per second)\n",
      "llama_perf_context_print:       total time =  180811.74 ms /  1582 tokens\n",
      " 97%|█████████████████████████████████████████████████████████████████████████▉  | 289/297 [10:12:11<20:46, 155.87s/it]Llama.generate: 21 prefix-match hit, remaining 364 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   32904.16 ms /   364 tokens (   90.40 ms per token,    11.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =   33448.80 ms /   151 runs   (  221.52 ms per token,     4.51 tokens per second)\n",
      "llama_perf_context_print:       total time =   66445.66 ms /   515 tokens\n",
      " 98%|██████████████████████████████████████████████████████████████████████████▏ | 290/297 [10:13:18<15:03, 129.07s/it]Llama.generate: 21 prefix-match hit, remaining 495 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   45871.01 ms /   495 tokens (   92.67 ms per token,    10.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =   58849.86 ms /   255 runs   (  230.78 ms per token,     4.33 tokens per second)\n",
      "llama_perf_context_print:       total time =  104923.05 ms /   750 tokens\n",
      " 98%|██████████████████████████████████████████████████████████████████████████▍ | 291/297 [10:15:03<12:11, 121.85s/it]Llama.generate: 23 prefix-match hit, remaining 670 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   61213.88 ms /   670 tokens (   91.36 ms per token,    10.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =   26621.41 ms /   118 runs   (  225.61 ms per token,     4.43 tokens per second)\n",
      "llama_perf_context_print:       total time =   87898.22 ms /   788 tokens\n",
      " 98%|██████████████████████████████████████████████████████████████████████████▋ | 292/297 [10:16:31<09:18, 111.69s/it]Llama.generate: 21 prefix-match hit, remaining 372 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   34670.46 ms /   372 tokens (   93.20 ms per token,    10.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =   42416.12 ms /   193 runs   (  219.77 ms per token,     4.55 tokens per second)\n",
      "llama_perf_context_print:       total time =   77218.11 ms /   565 tokens\n",
      " 99%|██████████████████████████████████████████████████████████████████████████▉ | 293/297 [10:17:48<06:45, 101.38s/it]Llama.generate: 27 prefix-match hit, remaining 430 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   39593.34 ms /   430 tokens (   92.08 ms per token,    10.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =   48215.53 ms /   222 runs   (  217.19 ms per token,     4.60 tokens per second)\n",
      "llama_perf_context_print:       total time =   87968.66 ms /   652 tokens\n",
      " 99%|████████████████████████████████████████████████████████████████████████████▏| 294/297 [10:19:16<04:52, 97.38s/it]Llama.generate: 22 prefix-match hit, remaining 340 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   29950.28 ms /   340 tokens (   88.09 ms per token,    11.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =   52908.00 ms /   255 runs   (  207.48 ms per token,     4.82 tokens per second)\n",
      "llama_perf_context_print:       total time =   83052.71 ms /   595 tokens\n",
      " 99%|████████████████████████████████████████████████████████████████████████████▍| 295/297 [10:20:39<03:06, 93.11s/it]Llama.generate: 21 prefix-match hit, remaining 781 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   72112.00 ms /   781 tokens (   92.33 ms per token,    10.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =   55279.46 ms /   255 runs   (  216.78 ms per token,     4.61 tokens per second)\n",
      "llama_perf_context_print:       total time =  127588.54 ms /  1036 tokens\n",
      "100%|███████████████████████████████████████████████████████████████████████████▋| 296/297 [10:22:47<01:43, 103.49s/it]Llama.generate: 21 prefix-match hit, remaining 881 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    7022.50 ms\n",
      "llama_perf_context_print: prompt eval time =   79818.65 ms /   881 tokens (   90.60 ms per token,    11.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =   46765.50 ms /   212 runs   (  220.59 ms per token,     4.53 tokens per second)\n",
      "llama_perf_context_print:       total time =  126740.38 ms /  1093 tokens\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 297/297 [10:24:54<00:00, 126.24s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for item in tqdm.tqdm(data):\n",
    "    query = item[\"question\"]\n",
    "    # Step 1: Get relevant chunks\n",
    "    top_chunks = retrieve_relevant_chunks_cosine(query,model_2,index_2,loaded_chunks, top_k=5)\n",
    "\n",
    "    # Step 2: Build prompt\n",
    "    prompt = build_prompt(query, top_chunks)\n",
    "\n",
    "    # Step 3: Token count (optional)\n",
    "    token_ids = llm.tokenize(prompt.encode(\"utf-8\"))\n",
    "    # print(f\"Token count for question: '{query[:50]}...':\", len(token_ids))\n",
    "\n",
    "    # Step 4: Run inference\n",
    "    response = llm(prompt, max_tokens=256, temperature=0.7, stop=[\"###\"])\n",
    "    answer = response[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "    # Step 5: Save updated QA\n",
    "    updated_qa_data_v2.append({\n",
    "        \"question\": query,\n",
    "        \"answer\": answer\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cabd8faf-699b-4dc1-bfb0-6e490b603fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Answers generated and saved to 'updated_qa_data_v2.json'.\n"
     ]
    }
   ],
   "source": [
    "with open(\"updated_qa_pairs_v2.json\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "    json.dump(updated_qa_data_v2, out_file, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\" Answers generated and saved to 'updated_qa_data_v2.json'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9e762fb8-b1a4-4746-b6f4-389c413769d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total QA pairs: 297\n",
      "Sample QA pair: {'question': 'What collaborative initiative was announced by the European Microfinance Network (EMN) and the Microfinance Centre (MFC) in April 2024, and what are its main objectives for supporting the European microfinance sector?', 'answer': \"The European Microfinance Network (EMN) and the Microfinance Centre (MFC) announced a collaborative initiative in April 2024 to capture data from the vast majority of European microfinance institutions. The main objectives of this initiative are to provide the most comprehensive dataset available on the sector today and to remain the leading source of data and analysis on the microfinance sector in Europe. This evaluation is undertaken as part of the Commission's commitment to evidence-based policy making under the Better Regulation policy.\"}\n"
     ]
    }
   ],
   "source": [
    "with open(\"updated_qa_pairs_v2.json\", \"r\", encoding=\"utf-8\") as in_file:\n",
    "    updated_qa_data_v2 = json.load(in_file)\n",
    "\n",
    "print(\"Total QA pairs:\", len(updated_qa_data))\n",
    "print(\"Sample QA pair:\", updated_qa_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "99e42cb5-c019-439c-a426-f4e3a35565ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔢 Pair 1\n",
      "❓ Question:\n",
      "What collaborative initiative was announced by the European Microfinance Network (EMN) and the Microfinance Centre (MFC) in April 2024, and what are its main objectives for supporting the European microfinance sector?\n",
      "\n",
      "✅ Reference Answer (Original ChatGPT):\n",
      "In April 2024, the European Microfinance Network (EMN) and the Microfinance Centre (MFC) announced a joint strategy aimed at strengthening the European microfinance sector. This collaborative initiative focuses on promoting financial inclusion, developing capacity-building resources, and creating a unified voice to influence policy-making at the European level. The partnership aims to better support microfinance institutions and expand access to responsible finance for underserved populations across Europe.\n",
      "\n",
      "🤖 Generated Answer (Model 1):\n",
      "The European Microfinance Network (EMN) and the Microfinance Centre (MFC) announced a collaborative initiative in April 2024 to capture data from the vast majority of European microfinance institutions. The main objectives of this initiative are to provide the most comprehensive dataset available on the sector today and to remain the leading source of data and analysis on the microfinance sector in Europe. This evaluation is undertaken as part of the Commission's commitment to evidence-based policy making under the Better Regulation policy.\n",
      "\n",
      "🧠 Generated Answer (Model 2):\n",
      "The European Microfinance Network (EMN) and the Microfinance Centre (MFC) announced a collaborative initiative in April 2024 to capture data from the vast majority of European microfinance institutions. The main objective of this initiative is to provide the most comprehensive dataset available on the European microfinance sector. The survey remains the leading source of data and analysis on the microfinance sector in Europe, and for the sixth consecutive survey edition, EMN and MFC have joined forces to capture data from the majority of European microfinance institutions. The focus of this edition is on the types of businesses reached by microfinance and highlights the social performance of business loans, along with the impact measurement approaches adopted by MFIs.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "🔢 Pair 2\n",
      "❓ Question:\n",
      "How did the AccessibleEU initiative exceed its yearly objectives in promoting accessibility for persons with disabilities across Europe?\n",
      "\n",
      "✅ Reference Answer (Original ChatGPT):\n",
      "The AccessibleEU initiative surpassed its yearly objectives by significantly advancing accessibility measures across European Union member states. It achieved this through extensive stakeholder engagement, implementation of accessibility audits, development of best practice toolkits, and organization of awareness campaigns. The initiative focused on inclusive design, policy harmonization, and improved accessibility in public infrastructure, thereby contributing to a more inclusive environment for persons with disabilities throughout Europe.\n",
      "\n",
      "🤖 Generated Answer (Model 1):\n",
      "The AccessibleEU initiative exceeded its yearly objectives in promoting accessibility for persons with disabilities across Europe by successfully hosting 88 events, growing its Community of Practitioners to over 3,400 members, establishing a Moodle forum, and keeping building its digital library with 163 new Good Practices and 136 new references on accessibility. Additionally, the entry into force of the European Accessibility Act (EU Directive 2019/882) marks an important step towards achieving a truly accessible Europe.\n",
      "\n",
      "🧠 Generated Answer (Model 2):\n",
      "The AccessibleEU initiative exceeded its yearly objectives in promoting accessibility for persons with disabilities across Europe by achieving several significant milestones in 2024. Some of these achievements include:\n",
      "\n",
      "1. Refinement and implementation of a set of indicators monitoring accessibility initiatives across all Member States.\n",
      "2. Hosting 88 events, including two online courses, and growing the Community of Practitioners to over 3,400 members.\n",
      "3. Establishing a Moodle forum and keeping on building its digital library with 163 new Good Practices and 136 new references on accessibility.\n",
      "4. Analyzing the European Accessibility Act and emphasizing the need to comply with it at a high-level event in Brussels.\n",
      "5. Launching the third edition of its online course “Accessible Technology Design”.\n",
      "\n",
      "Overall, these achievements demonstrate the initiative's commitment to promoting accessibility for persons with disabilities across Europe and its success in achieving its yearly objectives.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "🔢 Pair 3\n",
      "❓ Question:\n",
      "What key policy development took place on July 3, 2025, involving the European Commission and its implications for AI regulation in the EU?\n",
      "\n",
      "✅ Reference Answer (Original ChatGPT):\n",
      "On July 3, 2025, the European Commission and the European Parliament reached a political agreement on the AI Act, a landmark piece of legislation aimed at regulating artificial intelligence technologies across the EU. This act sets out a risk-based framework, classifying AI systems by their potential impact on fundamental rights and safety. It introduces strict obligations for high-risk AI applications, including requirements for transparency, accountability, and human oversight. The policy aims to foster trust in AI while supporting innovation and ensuring compliance with EU values.\n",
      "\n",
      "🤖 Generated Answer (Model 1):\n",
      "On July 3, 2025, the European Commission published its first AI strategy, which aimed to position Europe as a global leader in AI research and development. The strategy included several key policy developments, including the establishment of an AI ethics committee to ensure that AI is developed and used in a responsible and ethical manner. This committee would be tasked with developing guidelines and standards for AI, as well as addressing concerns around privacy, security, and bias.\n",
      "\n",
      "The strategy also included plans to invest in AI research and development, as well as to create new jobs in the field. The European Commission committed to investing €20 billion in AI research and development over the next seven years, and to creating 200,000 new jobs in the field.\n",
      "\n",
      "Overall, the European Commission's first AI strategy represents a significant effort to address the challenges and opportunities presented by AI, and to position Europe as a global leader in the field.\n",
      "\n",
      "🧠 Generated Answer (Model 2):\n",
      "On July 3, 2025, the European Commission published its European Strategy for AI in Science, which aims to accelerate the adoption of AI by scientists and researchers in the EU. The strategy focuses on improving access to data, computational power, and talent, as well as addressing science-specific AI challenges such as preserving scientific integrity and methodological rigour. The policy development is significant as it marks the EU's commitment to leveraging AI to drive innovation in scientific research and tackle societal challenges. The implications of this policy development for AI regulation in the EU are that it could lead to the development of more advanced and integrated AI tools and technologies in the scientific research sector, which could have significant impacts on various industries and sectors. However, it is important to note that the EU has already been working on AI regulation, with the ongoing development of the EU's AI strategy and the upcoming Apply AI Strategy, and the Technology Roadmap on encryption.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "🔢 Pair 4\n",
      "❓ Question:\n",
      "What environmentally sustainable measures has Hadag, Hamburg’s ferry service operator, implemented to modernize its fleet and reduce emissions?\n",
      "\n",
      "✅ Reference Answer (Original ChatGPT):\n",
      "Hadag, the ferry service operator in Hamburg, has undertaken a modernization program that includes the introduction of hybrid and fully electric ferries to its fleet. These environmentally sustainable measures are designed to reduce greenhouse gas emissions and improve energy efficiency in public transport. The initiative aligns with Hamburg’s broader climate strategy and emphasizes the shift towards low-emission urban mobility solutions in maritime transportation.\n",
      "\n",
      "🤖 Generated Answer (Model 1):\n",
      "Hadag, Hamburg's ferry service operator, has implemented the delivery of three electric ferries in 2024 and has more on the way, which will enable it to operate a fully zero-emissions fleet. Additionally, the company is utilizing optimisation algorithms and cloud-based planning tools to position itself as a leader in sustainable, water-based public transport. The PM² Methodologies are also being highlighted as a valuable addition to the agenda, emphasizing the importance of project management skills in delivering impactful solutions across all sectors.\n",
      "\n",
      "🧠 Generated Answer (Model 2):\n",
      "Hadag, Hamburg’s ferry service operator, has implemented several environmentally sustainable measures to modernize its fleet and reduce emissions. The company has delivered three electric ferries in 2024 and has plans to deliver more in the future, making its fleet fully zero-emissions. The new electric ferries are being powered by advanced technology and optimized by an AI-powered platform to streamline scheduling and operation. The platform also improves real-time passenger information across Hamburg's busy waterways.\n",
      "\n",
      "In addition, Hadag has adopted an innovative AI-powered platform to optimize the scheduling and operation of its electric ferry fleet. The new system will streamline ferry timetables and charging routines, while also improving real-time passenger information across Hamburg's busy waterways.\n",
      "\n",
      "Overall, Hadag's efforts to reduce emissions are part of the city's goal to reduce transport-related emissions significantly. As a member of the Carbon Neutral Cities Alliance, Hamburg aims to cut emissions by 55% by 2030 and by 95% by 2050.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "🔢 Pair 5\n",
      "❓ Question:\n",
      "What actions did the Israeli government undertake following recent Cabinet resolutions concerning the security situation in the Gaza Strip?\n",
      "\n",
      "✅ Reference Answer (Original ChatGPT):\n",
      "Following recent Cabinet resolutions, the Israeli government implemented heightened security operations targeting militant infrastructure in the Gaza Strip. These actions included airstrikes on weapon manufacturing sites and border surveillance enhancements. The operations were positioned as a response to ongoing threats and aimed at restoring stability and deterring further escalation in the region.\n",
      "\n",
      "🤖 Generated Answer (Model 1):\n",
      "The Israeli government undertook several actions following recent Cabinet resolutions concerning the security situation in the Gaza Strip. These actions include an increase in daily trucks for food and non-food items to enter Gaza, the opening of several other crossing points in both the northern and southern areas, the reopening of the Jordanian and Egyptian aid routes, enabling the distribution of food supplies through bakeries and public kitchens throughout the Gaza strip, the resumption of fuel deliveries for use by humanitarian facilities up to an operational level, the protection of aid workers, and the repair and facilitation of works on vital infrastructure like the resumption of the power supply to the water desalination facility.\n",
      "\n",
      "🧠 Generated Answer (Model 2):\n",
      "The Israeli government undertook several actions following recent Cabinet resolutions concerning the security situation in the Gaza Strip. These actions include:\n",
      "\n",
      "1. A substantial increase of daily trucks for food and non-food items to enter Gaza.\n",
      "2. The opening of several other crossing points in both the northern and southern areas.\n",
      "3. The reopening of the Jordanian and Egyptian aid routes.\n",
      "4. Enabling the distribution of food supplies through bakeries and public kitchens throughout the Gaza strip.\n",
      "5. The resumption of fuel deliveries for use by humanitarian facilities, up to an operational level.\n",
      "6. The protection of aid workers.\n",
      "7. The repair and facilitation of works on vital infrastructure like the resumption of the power supply to the water desalination facility.\n",
      "\n",
      "In addition to these actions, the Israeli government has also called for an immediate and permanent ceasefire in Gaza, the release of all hostages, and the unimpeded access of humanitarian aid. The EU stands ready to coordinate with all relevant humanitarian stakeholders, UN agencies and NGOs on the ground, to ensure swift implementation of these urgent steps.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "🔢 Pair 6\n",
      "❓ Question:\n",
      "What is the purpose of the EU Flight Emissions Label (FEL) initiative involving the European Union Aviation Safety Agency (EASA) and Air France-KLM, and how does it aim to enhance transparency and sustainability in European aviation?\n",
      "\n",
      "✅ Reference Answer (Original ChatGPT):\n",
      "The EU Flight Emissions Label (FEL) initiative is a collaborative effort between the European Union Aviation Safety Agency (EASA) and Air France-KLM aimed at enhancing transparency in flight emissions reporting. Under a Memorandum of Cooperation signed in 2024, Air France and KLM will pilot the FEL scheme to provide reliable, harmonized data on flight emissions based on factors like aircraft type, passenger load, freight volume, and fuel type. The goal is to allow passengers to make more informed choices, protect them from greenwashing, and promote the adoption of sustainable aviation fuels, thereby reinforcing decarbonization and fair competition within the EU aviation market.\n",
      "\n",
      "🤖 Generated Answer (Model 1):\n",
      "The EU Flight Emissions Label (FEL) initiative involves the European Union Aviation Safety Agency (EASA) and Air France-KLM, and its purpose is to enhance transparency and sustainability in European aviation. The FEL scheme allows airlines to display their flight emissions data and use these estimates when, for example, offering sustainable aviation fuel or other emissions offsets to passengers. This initiative encourages airlines to be more transparent about their carbon footprint and to take steps towards reducing their emissions. The Memorandum of Cooperation signed by EASA and Air France-KLM includes support from EASA experts to guide the airline group through the onboarding process.\n",
      "\n",
      "🧠 Generated Answer (Model 2):\n",
      "The EU Flight Emissions Label (FEL) initiative involving the European Union Aviation Safety Agency (EASA) and Air France-KLM aims to enhance transparency and sustainability in European aviation by providing a reliable and harmonised methodology for estimating flight emissions. The FEL sets out operational factors like aircraft type, passenger numbers, and freight volume on board, as well as the amount and type of aviation fuels uplifted per airport. This allows airlines to share flight emissions data with passengers in a trustworthy framework. The initiative encourages fair competition, transparency, and trust among airlines, and helps to accelerate the uptake of sustainable aviation fuels to cut flight emissions. It is important for airlines to join this initiative if they want to display flight emissions and use these estimates when, for example, offering sustainable aviation fuel or other emissions offsets to passengers.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "🔢 Pair 7\n",
      "❓ Question:\n",
      "What were the key findings and contributions of the two-year research initiative on circular economy monitoring, and how many indicators were ultimately selected and tested?\n",
      "\n",
      "✅ Reference Answer (Original ChatGPT):\n",
      "The two-year research initiative on circular economy monitoring resulted in a comprehensive set of options for tracking circularity across multiple sectors, including batteries, vehicles, electronics, bioeconomy, food, construction, plastics, and textiles. Out of over 730 potential indicators catalogued, 60 were selected and tested through 19 detailed case studies. These indicators support policy development by offering scalable monitoring tools at regional, national, and international levels, and across households, companies, and industries. The project aimed to strengthen understanding and implementation of circular economy policies and tools to address resource use and climate challenges.\n",
      "\n",
      "🤖 Generated Answer (Model 1):\n",
      "The two-year research initiative on circular economy monitoring concluded with a set of key findings and contributions that advanced our understanding of effective circular economy policies and support tools. The initiative focused on complementary and expanding aspects of existing monitoring efforts, such as the Circular Economy Monitoring Framework, to develop a comprehensive set of options for monitoring circularity across critical policy areas and sectors. Ultimately, 42 indicators were selected and tested as a result of the research project.\n",
      "\n",
      "🧠 Generated Answer (Model 2):\n",
      "The two-year research initiative on circular economy monitoring concluded by presenting a comprehensive set of options for monitoring circularity across critical policy areas and sectors. The project catalogued over 730 potential indicators, which were accessible through the Circular Economy Indicators Tool. After considering existing monitoring efforts such as the Circular Economy Monitoring Framework, the project focused on complementary and expanding aspects such as product-service systems. Ultimately, the project selected and tested a number of indicators, although the exact number is not specified in the provided context.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "🔢 Pair 8\n",
      "❓ Question:\n",
      "Why does the European Union continue to commemorate the genocide in Srebrenica, and what is its stance on historical denial and reconciliation in the Western Balkans?\n",
      "\n",
      "✅ Reference Answer (Original ChatGPT):\n",
      "The European Union commemorates the genocide in Srebrenica to honor the memory of over 8,300 Bosniak men and boys who were killed in July 1995 and to support the survivors and families still affected by the tragedy. The EU emphasizes the importance of preserving historical truth, recognizing its past responsibility in failing to prevent the atrocity, and firmly condemns any denial, distortion, or glorification of war crimes. The EU urges political leaders in Bosnia and Herzegovina and the Western Balkans to acknowledge established facts, support genuine reconciliation, and work towards a peaceful and unified future. It reaffirms its commitment to Bosnia and Herzegovina's path toward EU membership.\n",
      "\n",
      "🤖 Generated Answer (Model 1):\n",
      "The European Union continues to commemorate the genocide in Srebrenica because it was a significant event in the history of the Western Balkans and it represents one of the darkest moments in European history. The European Union firmly rejects and condemns any denial, distortion, or minimisation of the Srebrenica genocide, as well as the glorification of war criminals.\n",
      "\n",
      "In terms of historical denial and reconciliation in the Western Balkans, the European Union believes that it is essential to confront the roots of hatred that led to these atrocities in order to promote reconciliation and peace in the region. The European Union stands with the people of Bosnia and Herzegovina and supports their efforts to promote reconciliation and peace. The European Union also encourages dialogue and cooperation between different communities in the region to promote a shared future for all. The European Union recognises that historical denial and hatred are major barriers to reconciliation and peace and believes that it is essential to address these issues in order to promote a stable and prosperous future for the Western Balkans.\n",
      "\n",
      "🧠 Generated Answer (Model 2):\n",
      "The European Union continues to commemorate the genocide in Srebrenica as a way to remember and pay tribute to the more than 8,300 victims and their families, who continue to grieve after three decades. The EU also acknowledges its responsibility for failing to prevent and stop the genocide and firmly rejects any denial, distortion, or minimisation of the Srebrenica genocide, as well as the glorification of war criminals. The EU stands among the darkest chapters in Europe's collective memory and recognises the importance of historical reconciliation in the Western Balkans.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "🔢 Pair 9\n",
      "❓ Question:\n",
      "What changes were introduced in the European Commission's 'quick fix' amendments to the European Sustainability Reporting Standards (ESRS), and how do these impact wave one companies reporting for financial years 2025 and 2026?\n",
      "\n",
      "✅ Reference Answer (Original ChatGPT):\n",
      "The European Commission introduced 'quick fix' amendments to the European Sustainability Reporting Standards (ESRS) to ease the burden on 'wave one' companies that began reporting in financial year 2024. The amendments allow these companies to omit certain disclosures, such as anticipated financial impacts of sustainability-related risks, not only for 2024 but also for 2025 and 2026. The changes align wave one companies with the phase-in provisions granted to smaller firms and address the fact that they were excluded from the 'stop-the-clock' Directive that deferred requirements for 'wave two' and 'wave three' companies. A broader ESRS revision is underway, aiming for simplification and consistency by 2027.\n",
      "\n",
      "🤖 Generated Answer (Model 1):\n",
      "The European Commission has adopted targeted \"quick fix\" amendments to the first set of European Sustainability Reporting Standards (ESRS), which impact wave one companies reporting for financial years 2025 and 2026. These amendments aim to clarify certain provisions and reduce the number of data requirements, which will help simplify reporting requirements for companies. Additionally, the Commission is working on a broader revision of the ESRS, with the goal of improving consistency with other legislation and reducing the number of data requirements further. This revision will be implemented in the future, but it is not clear when it will be finalized and implemented. Overall, the changes introduced in the \"quick fix\" amendments and future revisions to the ESRS aim to improve the clarity and simplicity of sustainability reporting requirements for companies.\n",
      "\n",
      "🧠 Generated Answer (Model 2):\n",
      "The European Commission introduced targeted \"quick fix\" amendments to the European Sustainability Reporting Standards (ESRS) to allow wave one companies, which report on financial year 2024, to omit certain sustainability-related information. These amendments apply from financial year 2025 and will allow wave one companies to omit the same information for financial years 2025 and 2026. This means that wave one companies will not have to report any additional information compared to financial year 2024. The Commission is also working on a broader revision of the ESRS, with the aim of substantially reducing the number of data requirements, clarifying provisions deemed unclear, and improving consistency with other pieces of legislation.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "🔢 Pair 10\n",
      "❓ Question:\n",
      "What led to the imposition of anti-dumping duties on lysine imports from China, and what are the expected effects on the EU industry and environmental sustainability?\n",
      "\n",
      "✅ Reference Answer (Original ChatGPT):\n",
      "The EU imposed anti-dumping duties on lysine imports from China after an investigation revealed that these imports were being sold at unfairly low prices, harming European producers. Lysine, a critical amino acid used in animal feed, pharmaceuticals, and dietary supplements, plays an essential role in improving animal nutrition and reducing environmental impacts such as nitrogen pollution. The duties are expected to level the playing field for EU lysine manufacturers and promote fair competition while supporting environmentally sustainable agricultural practices in the EU.\n",
      "\n",
      "🤖 Generated Answer (Model 1):\n",
      "The imposition of anti-dumping duties on lysine imports from China was the result of an investigation that found that these imports were harming EU industry. The investigation revealed that the Chinese companies were selling lysine at artificially low prices, which made it difficult for EU lysine makers to compete. The duties imposed today were expected to help EU lysine makers to compete on a more equal footing with their Chinese counterparts.\n",
      "\n",
      "The decision to impose duties on lysine imports from China also had implications for environmental sustainability. Dumping of lysine from China led to environmental pollution in the EU, and the imposition of duties could help to protect the environment by reducing the amount of imported lysine on the market.\n",
      "\n",
      "However, it is important to note that the imposition of duties could also have unintended consequences. It is possible that the reduced availability of imported lysine could lead to higher prices for EU consumers, and it is also possible that the reduction in imports could lead to a decrease in competition, which could be harmful to innovation and technological progress.\n",
      "\n",
      "Overall, the imposition of anti-dumping duties on lysine imports from China reflects the\n",
      "\n",
      "🧠 Generated Answer (Model 2):\n",
      "The imposition of anti-dumping duties on lysine imports from China was a result of an investigation that found that dumped imports were harming EU industry. The expected effects on the EU industry are that it should help EU lysine makers to compete on a more equal footing with their Chinese counterparts. As for environmental sustainability, no information was provided in the context regarding the impact of these duties on the environment.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i, (original, model1, model2) in enumerate(zip(data, updated_qa_data, updated_qa_data_v2)):\n",
    "    print(f\"🔢 Pair {i+1}\")\n",
    "    print(f\"❓ Question:\\n{original['question']}\\n\")\n",
    "    print(f\"✅ Reference Answer (Original ChatGPT):\\n{original['answer']}\\n\")\n",
    "    print(f\"🤖 Generated Answer (Model 1):\\n{model1['answer']}\\n\")\n",
    "    print(f\"🧠 Generated Answer (Model 2):\\n{model2['answer']}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    if i == 9:\n",
    "        break  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b5a2e9fa-2dd4-439a-ad90-9042e6fa56f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 297/297 [02:14<00:00,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Average BLEU Score (1-gram): 0.1933\n",
      "🔹 Average Semantic Similarity: 0.7523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "assert len(data) == len(updated_qa_data_v2), \"Mismatch in number of QA pairs\"\n",
    "\n",
    "bleu_scores_2 = []\n",
    "similarity_scores_2 = []\n",
    "\n",
    "for gpt_item, my_item in tqdm.tqdm(zip(data, updated_qa_data_v2), total=len(data)):\n",
    "    reference = gpt_item['answer'].strip()\n",
    "    hypothesis = my_item['answer'].strip()\n",
    "\n",
    "    # BLEU Score (use 1-gram BLEU for QA relevance)\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    bleu = sentence_bleu([reference.split()], hypothesis.split(), weights=(1, 0, 0, 0), smoothing_function=smoothie)\n",
    "    bleu_scores_2.append(bleu)\n",
    "\n",
    "    # Semantic Similarity (cosine similarity)\n",
    "    emb_ref = model_2.encode(reference, convert_to_tensor=True)\n",
    "    emb_hyp = model_2.encode(hypothesis, convert_to_tensor=True)\n",
    "    sim_score = util.pytorch_cos_sim(emb_ref, emb_hyp).item()\n",
    "    similarity_scores_2.append(sim_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "af2d2cbe-40ae-4d84-b703-8c64d1bccf3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Average BLEU Score (1-gram): 0.2225\n",
      "🔹 Average Semantic Similarity: 0.8782\n"
     ]
    }
   ],
   "source": [
    "# --- Summary ---\n",
    "average_bleu = sum(bleu_scores_2) / len(bleu_scores_2)\n",
    "average_similarity = sum(similarity_scores_2) / len(similarity_scores_2)\n",
    "\n",
    "print(f\"🔹 Average BLEU Score (1-gram): {average_bleu:.4f}\")\n",
    "print(f\"🔹 Average Semantic Similarity: {average_similarity:.4f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "7ec29f55-3924-4aa0-8041-35cf7f488c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 297/297 [00:07<00:00, 37.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Average METEOR: 0.3542\n",
      "🔹 Average ROUGE-1: 0.3756\n",
      "🔹 Average ROUGE-2: 0.1348\n",
      "🔹 Average ROUGE-L: 0.2385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "meteor_scores_2 = []\n",
    "rouge1_scores_2 = []\n",
    "rouge2_scores_2 = []\n",
    "rougeL_scores_2 = []\n",
    "\n",
    "for gpt, mine in tqdm.tqdm(zip(data, updated_qa_data_v2), total=len(data)):\n",
    "    ref = gpt[\"answer\"].strip()\n",
    "    hyp = mine[\"answer\"].strip()\n",
    "\n",
    "    # METEOR\n",
    "    meteor = single_meteor_score(word_tokenize(ref), word_tokenize(hyp))\n",
    "    meteor_scores_2.append(meteor)\n",
    "\n",
    "    # ROUGE\n",
    "    scores = rouge.score(ref, hyp)\n",
    "    rouge1_scores_2.append(scores[\"rouge1\"].fmeasure)\n",
    "    rouge2_scores_2.append(scores[\"rouge2\"].fmeasure)\n",
    "    rougeL_scores_2.append(scores[\"rougeL\"].fmeasure)\n",
    "\n",
    "print(\"🔹 Average METEOR:\", round(sum(meteor_scores_2) / len(meteor_scores_2), 4))\n",
    "print(\"🔹 Average ROUGE-1:\", round(sum(rouge1_scores_2) / len(rouge1_scores_2), 4))\n",
    "print(\"🔹 Average ROUGE-2:\", round(sum(rouge2_scores_2) / len(rouge2_scores_2), 4))\n",
    "print(\"🔹 Average ROUGE-L:\", round(sum(rougeL_scores_2) / len(rougeL_scores_2), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad9f3c1-7fff-413a-ac07-f7dee644c3dd",
   "metadata": {},
   "source": [
    "#### Third attempt with index_3, which has chunks with sliding window, max len 300 and without similarity merging. The embedding model is same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b38668c7-691f-4c8f-840d-ebdfdd462fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the FAISS index\n",
    "index_3 = faiss.read_index(\"chunk_index_3.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9739e88d-9650-4dcc-8413-ab6592500992",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sliding_sentance_chunks_.pkl\", \"rb\") as f:\n",
    "    data_2 = pickle.load(f)\n",
    "\n",
    "all_chunks_3 = data_2[\"chunks\"]\n",
    "chunk_index_map_3 = data_2[\"index_map\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1042d280-afed-4c0e-861d-cff160d880da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_qa_v3 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8fcc7732-864d-4188-b076-d4b55a7971d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/297 [00:00<?, ?it/s]Llama.generate: 11 prefix-match hit, remaining 1145 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =  288953.33 ms /  1145 tokens (  252.36 ms per token,     3.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =   64533.94 ms /   133 runs   (  485.22 ms per token,     2.06 tokens per second)\n",
      "llama_perf_context_print:       total time =  353701.24 ms /  1278 tokens\n",
      "  0%|▎                                                                             | 1/297 [05:54<29:07:04, 354.14s/it]Llama.generate: 21 prefix-match hit, remaining 1431 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =  362701.48 ms /  1431 tokens (  253.46 ms per token,     3.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =  112382.02 ms /   225 runs   (  499.48 ms per token,     2.00 tokens per second)\n",
      "llama_perf_context_print:       total time =  475496.91 ms /  1656 tokens\n",
      "  1%|▌                                                                             | 2/297 [13:49<34:52:52, 425.67s/it]Llama.generate: 21 prefix-match hit, remaining 747 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =  187148.94 ms /   747 tokens (  250.53 ms per token,     3.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =   18939.34 ms /    41 runs   (  461.94 ms per token,     2.16 tokens per second)\n",
      "llama_perf_context_print:       total time =  206135.78 ms /   788 tokens\n",
      "  1%|▊                                                                             | 3/297 [17:16<26:35:07, 325.54s/it]Llama.generate: 21 prefix-match hit, remaining 1322 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =  324626.48 ms /  1322 tokens (  245.56 ms per token,     4.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =  106535.13 ms /   207 runs   (  514.66 ms per token,     1.94 tokens per second)\n",
      "llama_perf_context_print:       total time =  431530.02 ms /  1529 tokens\n",
      "  1%|█                                                                             | 4/297 [24:28<29:54:31, 367.48s/it]Llama.generate: 21 prefix-match hit, remaining 1113 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =  251195.09 ms /  1113 tokens (  225.69 ms per token,     4.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =   91550.35 ms /   191 runs   (  479.32 ms per token,     2.09 tokens per second)\n",
      "llama_perf_context_print:       total time =  343050.41 ms /  1304 tokens\n",
      "  2%|█▎                                                                            | 5/297 [30:11<29:05:54, 358.75s/it]Llama.generate: 21 prefix-match hit, remaining 1804 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =  440296.77 ms /  1804 tokens (  244.07 ms per token,     4.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =  108772.82 ms /   191 runs   (  569.49 ms per token,     1.76 tokens per second)\n",
      "llama_perf_context_print:       total time =  549381.34 ms /  1995 tokens\n",
      "  2%|█▌                                                                            | 6/297 [39:20<34:14:44, 423.66s/it]Llama.generate: 21 prefix-match hit, remaining 835 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =  196195.06 ms /   835 tokens (  234.96 ms per token,     4.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =  139473.81 ms /   255 runs   (  546.96 ms per token,     1.83 tokens per second)\n",
      "llama_perf_context_print:       total time =  336144.98 ms /  1090 tokens\n",
      "  2%|█▊                                                                            | 7/297 [44:57<31:49:47, 395.13s/it]Llama.generate: 21 prefix-match hit, remaining 1662 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =  409233.83 ms /  1662 tokens (  246.23 ms per token,     4.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =  115708.07 ms /   226 runs   (  511.98 ms per token,     1.95 tokens per second)\n",
      "llama_perf_context_print:       total time =  525339.40 ms /  1888 tokens\n",
      "  3%|██                                                                            | 8/297 [53:43<35:03:21, 436.68s/it]Llama.generate: 21 prefix-match hit, remaining 1599 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =  402488.80 ms /  1599 tokens (  251.71 ms per token,     3.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =  125718.18 ms /   255 runs   (  493.01 ms per token,     2.03 tokens per second)\n",
      "llama_perf_context_print:       total time =  528666.94 ms /  1854 tokens\n",
      "  3%|██▎                                                                         | 9/297 [1:02:31<37:14:32, 465.53s/it]Llama.generate: 22 prefix-match hit, remaining 527 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =  127820.74 ms /   527 tokens (  242.54 ms per token,     4.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =   61187.28 ms /   133 runs   (  460.05 ms per token,     2.17 tokens per second)\n",
      "llama_perf_context_print:       total time =  189187.60 ms /   660 tokens\n",
      "  3%|██▌                                                                        | 10/297 [1:05:41<30:19:02, 380.29s/it]Llama.generate: 22 prefix-match hit, remaining 1943 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =  477845.27 ms /  1943 tokens (  245.93 ms per token,     4.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =   57396.13 ms /   229 runs   (  250.64 ms per token,     3.99 tokens per second)\n",
      "llama_perf_context_print:       total time =  535416.58 ms /  2172 tokens\n",
      "  4%|██▊                                                                        | 11/297 [1:14:37<33:59:18, 427.83s/it]Llama.generate: 21 prefix-match hit, remaining 1799 prompt tokens to eval\n",
      "  4%|██▊                                                                        | 11/297 [1:17:36<33:37:39, 423.28s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 14\u001b[0m\n\u001b[0;32m     10\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mtokenize(prompt\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# print(f\"Token count for question: '{query[:50]}...':\", len(token_ids))\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Step 4: Run inference\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m response \u001b[38;5;241m=\u001b[39m llm(prompt, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m, stop\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m###\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     15\u001b[0m answer \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Step 5: Save updated QA\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\llama_cpp\\llama.py:1904\u001b[0m, in \u001b[0;36mLlama.__call__\u001b[1;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[0;32m   1840\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m   1841\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1842\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1866\u001b[0m     logit_bias: Optional[Dict[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1867\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[CreateCompletionResponse, Iterator[CreateCompletionStreamResponse]]:\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate text from a prompt.\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \n\u001b[0;32m   1870\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1902\u001b[0m \u001b[38;5;124;03m        Response object containing the generated text.\u001b[39;00m\n\u001b[0;32m   1903\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1904\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_completion(\n\u001b[0;32m   1905\u001b[0m         prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[0;32m   1906\u001b[0m         suffix\u001b[38;5;241m=\u001b[39msuffix,\n\u001b[0;32m   1907\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39mmax_tokens,\n\u001b[0;32m   1908\u001b[0m         temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[0;32m   1909\u001b[0m         top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   1910\u001b[0m         min_p\u001b[38;5;241m=\u001b[39mmin_p,\n\u001b[0;32m   1911\u001b[0m         typical_p\u001b[38;5;241m=\u001b[39mtypical_p,\n\u001b[0;32m   1912\u001b[0m         logprobs\u001b[38;5;241m=\u001b[39mlogprobs,\n\u001b[0;32m   1913\u001b[0m         echo\u001b[38;5;241m=\u001b[39mecho,\n\u001b[0;32m   1914\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m   1915\u001b[0m         frequency_penalty\u001b[38;5;241m=\u001b[39mfrequency_penalty,\n\u001b[0;32m   1916\u001b[0m         presence_penalty\u001b[38;5;241m=\u001b[39mpresence_penalty,\n\u001b[0;32m   1917\u001b[0m         repeat_penalty\u001b[38;5;241m=\u001b[39mrepeat_penalty,\n\u001b[0;32m   1918\u001b[0m         top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[0;32m   1919\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1920\u001b[0m         seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[0;32m   1921\u001b[0m         tfs_z\u001b[38;5;241m=\u001b[39mtfs_z,\n\u001b[0;32m   1922\u001b[0m         mirostat_mode\u001b[38;5;241m=\u001b[39mmirostat_mode,\n\u001b[0;32m   1923\u001b[0m         mirostat_tau\u001b[38;5;241m=\u001b[39mmirostat_tau,\n\u001b[0;32m   1924\u001b[0m         mirostat_eta\u001b[38;5;241m=\u001b[39mmirostat_eta,\n\u001b[0;32m   1925\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   1926\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1927\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[0;32m   1928\u001b[0m         grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[0;32m   1929\u001b[0m         logit_bias\u001b[38;5;241m=\u001b[39mlogit_bias,\n\u001b[0;32m   1930\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\llama_cpp\\llama.py:1837\u001b[0m, in \u001b[0;36mLlama.create_completion\u001b[1;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[0;32m   1835\u001b[0m     chunks: Iterator[CreateCompletionStreamResponse] \u001b[38;5;241m=\u001b[39m completion_or_chunks\n\u001b[0;32m   1836\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n\u001b[1;32m-> 1837\u001b[0m completion: Completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(completion_or_chunks)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   1838\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m completion\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\llama_cpp\\llama.py:1322\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[1;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[0;32m   1320\u001b[0m finish_reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1321\u001b[0m multibyte_fix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1322\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m   1323\u001b[0m     prompt_tokens,\n\u001b[0;32m   1324\u001b[0m     top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[0;32m   1325\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   1326\u001b[0m     min_p\u001b[38;5;241m=\u001b[39mmin_p,\n\u001b[0;32m   1327\u001b[0m     typical_p\u001b[38;5;241m=\u001b[39mtypical_p,\n\u001b[0;32m   1328\u001b[0m     temp\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[0;32m   1329\u001b[0m     tfs_z\u001b[38;5;241m=\u001b[39mtfs_z,\n\u001b[0;32m   1330\u001b[0m     mirostat_mode\u001b[38;5;241m=\u001b[39mmirostat_mode,\n\u001b[0;32m   1331\u001b[0m     mirostat_tau\u001b[38;5;241m=\u001b[39mmirostat_tau,\n\u001b[0;32m   1332\u001b[0m     mirostat_eta\u001b[38;5;241m=\u001b[39mmirostat_eta,\n\u001b[0;32m   1333\u001b[0m     frequency_penalty\u001b[38;5;241m=\u001b[39mfrequency_penalty,\n\u001b[0;32m   1334\u001b[0m     presence_penalty\u001b[38;5;241m=\u001b[39mpresence_penalty,\n\u001b[0;32m   1335\u001b[0m     repeat_penalty\u001b[38;5;241m=\u001b[39mrepeat_penalty,\n\u001b[0;32m   1336\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1337\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[0;32m   1338\u001b[0m     grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[0;32m   1339\u001b[0m ):\n\u001b[0;32m   1340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m llama_cpp\u001b[38;5;241m.\u001b[39mllama_token_is_eog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mvocab, token):\n\u001b[0;32m   1341\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetokenize(completion_tokens, prev_tokens\u001b[38;5;241m=\u001b[39mprompt_tokens)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\llama_cpp\\llama.py:914\u001b[0m, in \u001b[0;36mLlama.generate\u001b[1;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;66;03m# Eval and sample\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 914\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval(tokens)\n\u001b[0;32m    915\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m sample_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_tokens:\n\u001b[0;32m    916\u001b[0m         token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[0;32m    917\u001b[0m             top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[0;32m    918\u001b[0m             top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    932\u001b[0m             idx\u001b[38;5;241m=\u001b[39msample_idx,\n\u001b[0;32m    933\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\llama_cpp\\llama.py:648\u001b[0m, in \u001b[0;36mLlama.eval\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    644\u001b[0m n_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[0;32m    645\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch\u001b[38;5;241m.\u001b[39mset_batch(\n\u001b[0;32m    646\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch, n_past\u001b[38;5;241m=\u001b[39mn_past, logits_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logits_all\n\u001b[0;32m    647\u001b[0m )\n\u001b[1;32m--> 648\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ctx\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch)\n\u001b[0;32m    649\u001b[0m \u001b[38;5;66;03m# Save tokens\u001b[39;00m\n\u001b[0;32m    650\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_ids[n_past : n_past \u001b[38;5;241m+\u001b[39m n_tokens] \u001b[38;5;241m=\u001b[39m batch\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\llama_cpp\\_internals.py:316\u001b[0m, in \u001b[0;36mLlamaContext.decode\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: LlamaBatch):\n\u001b[1;32m--> 316\u001b[0m     return_code \u001b[38;5;241m=\u001b[39m llama_cpp\u001b[38;5;241m.\u001b[39mllama_decode(\n\u001b[0;32m    317\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx,\n\u001b[0;32m    318\u001b[0m         batch\u001b[38;5;241m.\u001b[39mbatch,\n\u001b[0;32m    319\u001b[0m     )\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_decode returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for item in tqdm.tqdm(data):\n",
    "    query = item[\"question\"]\n",
    "    # Step 1: Get relevant chunks\n",
    "    top_chunks = retrieve_relevant_chunks_cosine(query,model_2,index_3,all_chunks_3, top_k=5)\n",
    "\n",
    "    # Step 2: Build prompt\n",
    "    prompt = build_prompt(query, top_chunks)\n",
    "\n",
    "    # Step 3: Token count (optional)\n",
    "    token_ids = llm.tokenize(prompt.encode(\"utf-8\"))\n",
    "    # print(f\"Token count for question: '{query[:50]}...':\", len(token_ids))\n",
    "\n",
    "    # Step 4: Run inference\n",
    "    response = llm(prompt, max_tokens=256, temperature=0.7, stop=[\"###\"])\n",
    "    answer = response[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "    # Step 5: Save updated QA\n",
    "    model_qa_v3.append({\n",
    "        \"question\": query,\n",
    "        \"answer\": answer\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ea6ed997-657d-48c7-a003-a2c6ce088f97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔢 Pair 1\n",
      "❓ Question:\n",
      "What collaborative initiative was announced by the European Microfinance Network (EMN) and the Microfinance Centre (MFC) in April 2024, and what are its main objectives for supporting the European microfinance sector?\n",
      "\n",
      "✅ Reference Answer (Original ChatGPT):\n",
      "In April 2024, the European Microfinance Network (EMN) and the Microfinance Centre (MFC) announced a joint strategy aimed at strengthening the European microfinance sector. This collaborative initiative focuses on promoting financial inclusion, developing capacity-building resources, and creating a unified voice to influence policy-making at the European level. The partnership aims to better support microfinance institutions and expand access to responsible finance for underserved populations across Europe.\n",
      "\n",
      "🤖 Generated Answer (Model 1):\n",
      "The European Microfinance Network (EMN) and the Microfinance Centre (MFC) announced a collaborative initiative in April 2024 to capture data from the vast majority of European microfinance institutions. The main objectives of this initiative are to provide the most comprehensive dataset available on the sector today and to remain the leading source of data and analysis on the microfinance sector in Europe. This evaluation is undertaken as part of the Commission's commitment to evidence-based policy making under the Better Regulation policy.\n",
      "\n",
      "🧠 Generated Answer (Model 2):\n",
      "The European Microfinance Network (EMN) and the Microfinance Centre (MFC) announced a collaborative initiative in April 2024 to capture data from the vast majority of European microfinance institutions. The main objective of this initiative is to provide the most comprehensive dataset available on the European microfinance sector. The survey remains the leading source of data and analysis on the microfinance sector in Europe, and for the sixth consecutive survey edition, EMN and MFC have joined forces to capture data from the majority of European microfinance institutions. The focus of this edition is on the types of businesses reached by microfinance and highlights the social performance of business loans, along with the impact measurement approaches adopted by MFIs.\n",
      "🧠 Generated Answer (Model 3):\n",
      "The European Microfinance Network (EMN) and the Microfinance Centre (MFC) announced a collaborative initiative in April 2024 called \"Microfinance in Europe: Survey Report\". The main objectives of this initiative are to provide valuable insights into the contribution of microfinance institutions to social inclusion, entrepreneurship, and local development, and to serve as an important policy tool supporting evidence-based decision-making for policymakers working to strengthen financial inclusion and the social economy. The initiative also functions as a benchmarking reference for MFIs, helping them evaluate their performance and position within the wider European landscape.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "🔢 Pair 2\n",
      "❓ Question:\n",
      "How did the AccessibleEU initiative exceed its yearly objectives in promoting accessibility for persons with disabilities across Europe?\n",
      "\n",
      "✅ Reference Answer (Original ChatGPT):\n",
      "The AccessibleEU initiative surpassed its yearly objectives by significantly advancing accessibility measures across European Union member states. It achieved this through extensive stakeholder engagement, implementation of accessibility audits, development of best practice toolkits, and organization of awareness campaigns. The initiative focused on inclusive design, policy harmonization, and improved accessibility in public infrastructure, thereby contributing to a more inclusive environment for persons with disabilities throughout Europe.\n",
      "\n",
      "🤖 Generated Answer (Model 1):\n",
      "The AccessibleEU initiative exceeded its yearly objectives in promoting accessibility for persons with disabilities across Europe by successfully hosting 88 events, growing its Community of Practitioners to over 3,400 members, establishing a Moodle forum, and keeping building its digital library with 163 new Good Practices and 136 new references on accessibility. Additionally, the entry into force of the European Accessibility Act (EU Directive 2019/882) marks an important step towards achieving a truly accessible Europe.\n",
      "\n",
      "🧠 Generated Answer (Model 2):\n",
      "The AccessibleEU initiative exceeded its yearly objectives in promoting accessibility for persons with disabilities across Europe by achieving several significant milestones in 2024. Some of these achievements include:\n",
      "\n",
      "1. Refinement and implementation of a set of indicators monitoring accessibility initiatives across all Member States.\n",
      "2. Hosting 88 events, including two online courses, and growing the Community of Practitioners to over 3,400 members.\n",
      "3. Establishing a Moodle forum and keeping on building its digital library with 163 new Good Practices and 136 new references on accessibility.\n",
      "4. Analyzing the European Accessibility Act and emphasizing the need to comply with it at a high-level event in Brussels.\n",
      "5. Launching the third edition of its online course “Accessible Technology Design”.\n",
      "\n",
      "Overall, these achievements demonstrate the initiative's commitment to promoting accessibility for persons with disabilities across Europe and its success in achieving its yearly objectives.\n",
      "🧠 Generated Answer (Model 3):\n",
      "The AccessibleEU initiative exceeded its yearly objectives in promoting accessibility for persons with disabilities across Europe in several ways. One of the key achievements was the refinement and implementation of the set of indicators monitoring accessibility initiatives across all Member States, which helped to collect a large amount of information on commitments and efforts made by each Member State to implement accessibility legislation and standards. Additionally, the initiative successfully hosted 88 events, including two online courses, grew the Community of Practiceto over 3,400 members, established aMoodle forum, and kept building its digital library with 163 new Good Practices and 136 new references on accessibility. Furthermore, the inititative held a high-level event in Brussels on 3rdJuly, bringing together European institutions, national governments, leading companies, disability organisations and accessibility experts to analyse the European Accessibility Act and emphasise the need to comply with it. All these achievements demonstrate the initiative's success in promoting accessibility for persons with disabilities across Europe.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "🔢 Pair 3\n",
      "❓ Question:\n",
      "What key policy development took place on July 3, 2025, involving the European Commission and its implications for AI regulation in the EU?\n",
      "\n",
      "✅ Reference Answer (Original ChatGPT):\n",
      "On July 3, 2025, the European Commission and the European Parliament reached a political agreement on the AI Act, a landmark piece of legislation aimed at regulating artificial intelligence technologies across the EU. This act sets out a risk-based framework, classifying AI systems by their potential impact on fundamental rights and safety. It introduces strict obligations for high-risk AI applications, including requirements for transparency, accountability, and human oversight. The policy aims to foster trust in AI while supporting innovation and ensuring compliance with EU values.\n",
      "\n",
      "🤖 Generated Answer (Model 1):\n",
      "On July 3, 2025, the European Commission published its first AI strategy, which aimed to position Europe as a global leader in AI research and development. The strategy included several key policy developments, including the establishment of an AI ethics committee to ensure that AI is developed and used in a responsible and ethical manner. This committee would be tasked with developing guidelines and standards for AI, as well as addressing concerns around privacy, security, and bias.\n",
      "\n",
      "The strategy also included plans to invest in AI research and development, as well as to create new jobs in the field. The European Commission committed to investing €20 billion in AI research and development over the next seven years, and to creating 200,000 new jobs in the field.\n",
      "\n",
      "Overall, the European Commission's first AI strategy represents a significant effort to address the challenges and opportunities presented by AI, and to position Europe as a global leader in the field.\n",
      "\n",
      "🧠 Generated Answer (Model 2):\n",
      "On July 3, 2025, the European Commission published its European Strategy for AI in Science, which aims to accelerate the adoption of AI by scientists and researchers in the EU. The strategy focuses on improving access to data, computational power, and talent, as well as addressing science-specific AI challenges such as preserving scientific integrity and methodological rigour. The policy development is significant as it marks the EU's commitment to leveraging AI to drive innovation in scientific research and tackle societal challenges. The implications of this policy development for AI regulation in the EU are that it could lead to the development of more advanced and integrated AI tools and technologies in the scientific research sector, which could have significant impacts on various industries and sectors. However, it is important to note that the EU has already been working on AI regulation, with the ongoing development of the EU's AI strategy and the upcoming Apply AI Strategy, and the Technology Roadmap on encryption.\n",
      "🧠 Generated Answer (Model 3):\n",
      "It is not specified in the provided context what key policy development took place on July 3, 2025, involving the European Commission and its implications for AI regulation in the EU.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "🔢 Pair 4\n",
      "❓ Question:\n",
      "What environmentally sustainable measures has Hadag, Hamburg’s ferry service operator, implemented to modernize its fleet and reduce emissions?\n",
      "\n",
      "✅ Reference Answer (Original ChatGPT):\n",
      "Hadag, the ferry service operator in Hamburg, has undertaken a modernization program that includes the introduction of hybrid and fully electric ferries to its fleet. These environmentally sustainable measures are designed to reduce greenhouse gas emissions and improve energy efficiency in public transport. The initiative aligns with Hamburg’s broader climate strategy and emphasizes the shift towards low-emission urban mobility solutions in maritime transportation.\n",
      "\n",
      "🤖 Generated Answer (Model 1):\n",
      "Hadag, Hamburg's ferry service operator, has implemented the delivery of three electric ferries in 2024 and has more on the way, which will enable it to operate a fully zero-emissions fleet. Additionally, the company is utilizing optimisation algorithms and cloud-based planning tools to position itself as a leader in sustainable, water-based public transport. The PM² Methodologies are also being highlighted as a valuable addition to the agenda, emphasizing the importance of project management skills in delivering impactful solutions across all sectors.\n",
      "\n",
      "🧠 Generated Answer (Model 2):\n",
      "Hadag, Hamburg’s ferry service operator, has implemented several environmentally sustainable measures to modernize its fleet and reduce emissions. The company has delivered three electric ferries in 2024 and has plans to deliver more in the future, making its fleet fully zero-emissions. The new electric ferries are being powered by advanced technology and optimized by an AI-powered platform to streamline scheduling and operation. The platform also improves real-time passenger information across Hamburg's busy waterways.\n",
      "\n",
      "In addition, Hadag has adopted an innovative AI-powered platform to optimize the scheduling and operation of its electric ferry fleet. The new system will streamline ferry timetables and charging routines, while also improving real-time passenger information across Hamburg's busy waterways.\n",
      "\n",
      "Overall, Hadag's efforts to reduce emissions are part of the city's goal to reduce transport-related emissions significantly. As a member of the Carbon Neutral Cities Alliance, Hamburg aims to cut emissions by 55% by 2030 and by 95% by 2050.\n",
      "🧠 Generated Answer (Model 3):\n",
      "Hadag, Hamburg's ferry service operator, has implemented an AI-powered platform to optimize the scheduling and operation of its electric ferry fleet. The platform addresses key challenges in electric ferry operations, such as battery range and fluctuating energy costs. The initiative supports the city's goal to reduce transport-related emissions significantly. As a member of the Carbon Neutral Cities Alliance, Hamburg aims to cut emissions by 55% by 2030 and by 95% by 2050. Electric ferries play a key role in reaching these targets. The AI system will support daily operations by tailoring charging strategies, managing energy demand, and enhancing communication with passengers via digital displays and mobile apps. With three electric ferries already delivered in 2024 and more on the way, Hadag is on track to operate a fully zero-emissions fleet - demonstrating how advanced technology can drive real climate action in public transport.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "🔢 Pair 5\n",
      "❓ Question:\n",
      "What actions did the Israeli government undertake following recent Cabinet resolutions concerning the security situation in the Gaza Strip?\n",
      "\n",
      "✅ Reference Answer (Original ChatGPT):\n",
      "Following recent Cabinet resolutions, the Israeli government implemented heightened security operations targeting militant infrastructure in the Gaza Strip. These actions included airstrikes on weapon manufacturing sites and border surveillance enhancements. The operations were positioned as a response to ongoing threats and aimed at restoring stability and deterring further escalation in the region.\n",
      "\n",
      "🤖 Generated Answer (Model 1):\n",
      "The Israeli government undertook several actions following recent Cabinet resolutions concerning the security situation in the Gaza Strip. These actions include an increase in daily trucks for food and non-food items to enter Gaza, the opening of several other crossing points in both the northern and southern areas, the reopening of the Jordanian and Egyptian aid routes, enabling the distribution of food supplies through bakeries and public kitchens throughout the Gaza strip, the resumption of fuel deliveries for use by humanitarian facilities up to an operational level, the protection of aid workers, and the repair and facilitation of works on vital infrastructure like the resumption of the power supply to the water desalination facility.\n",
      "\n",
      "🧠 Generated Answer (Model 2):\n",
      "The Israeli government undertook several actions following recent Cabinet resolutions concerning the security situation in the Gaza Strip. These actions include:\n",
      "\n",
      "1. A substantial increase of daily trucks for food and non-food items to enter Gaza.\n",
      "2. The opening of several other crossing points in both the northern and southern areas.\n",
      "3. The reopening of the Jordanian and Egyptian aid routes.\n",
      "4. Enabling the distribution of food supplies through bakeries and public kitchens throughout the Gaza strip.\n",
      "5. The resumption of fuel deliveries for use by humanitarian facilities, up to an operational level.\n",
      "6. The protection of aid workers.\n",
      "7. The repair and facilitation of works on vital infrastructure like the resumption of the power supply to the water desalination facility.\n",
      "\n",
      "In addition to these actions, the Israeli government has also called for an immediate and permanent ceasefire in Gaza, the release of all hostages, and the unimpeded access of humanitarian aid. The EU stands ready to coordinate with all relevant humanitarian stakeholders, UN agencies and NGOs on the ground, to ensure swift implementation of these urgent steps.\n",
      "🧠 Generated Answer (Model 3):\n",
      "The Israeli government undertook several actions following recent Cabinet resolutions concerning the security situation in the Gaza Strip. These actions include, among other things, a substantial increase of daily trucks for food and non-food items to enter Gaza, the opening of several other crossing points in both the northern and southern areas; the reopening of the Jordanian and Egyptian aid routes; enabling the distribution of food supplies through bakeries and public kitchens throughout the Gaza strip; the resumption of fuel deliveries for use by humanitarian facilities, up to an operational level; the protection of aid workers;  the repair and facilitation of works on vital infrastructure like the resumption of the power supply to the water desalination facility. Additionally, the EU stands ready to coordinate with all relevant humanitarian stakeholders, UN agencies and NGOs on the ground, to ensure swift implementation of those urgent steps.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "🔢 Pair 6\n",
      "❓ Question:\n",
      "What is the purpose of the EU Flight Emissions Label (FEL) initiative involving the European Union Aviation Safety Agency (EASA) and Air France-KLM, and how does it aim to enhance transparency and sustainability in European aviation?\n",
      "\n",
      "✅ Reference Answer (Original ChatGPT):\n",
      "The EU Flight Emissions Label (FEL) initiative is a collaborative effort between the European Union Aviation Safety Agency (EASA) and Air France-KLM aimed at enhancing transparency in flight emissions reporting. Under a Memorandum of Cooperation signed in 2024, Air France and KLM will pilot the FEL scheme to provide reliable, harmonized data on flight emissions based on factors like aircraft type, passenger load, freight volume, and fuel type. The goal is to allow passengers to make more informed choices, protect them from greenwashing, and promote the adoption of sustainable aviation fuels, thereby reinforcing decarbonization and fair competition within the EU aviation market.\n",
      "\n",
      "🤖 Generated Answer (Model 1):\n",
      "The EU Flight Emissions Label (FEL) initiative involves the European Union Aviation Safety Agency (EASA) and Air France-KLM, and its purpose is to enhance transparency and sustainability in European aviation. The FEL scheme allows airlines to display their flight emissions data and use these estimates when, for example, offering sustainable aviation fuel or other emissions offsets to passengers. This initiative encourages airlines to be more transparent about their carbon footprint and to take steps towards reducing their emissions. The Memorandum of Cooperation signed by EASA and Air France-KLM includes support from EASA experts to guide the airline group through the onboarding process.\n",
      "\n",
      "🧠 Generated Answer (Model 2):\n",
      "The EU Flight Emissions Label (FEL) initiative involving the European Union Aviation Safety Agency (EASA) and Air France-KLM aims to enhance transparency and sustainability in European aviation by providing a reliable and harmonised methodology for estimating flight emissions. The FEL sets out operational factors like aircraft type, passenger numbers, and freight volume on board, as well as the amount and type of aviation fuels uplifted per airport. This allows airlines to share flight emissions data with passengers in a trustworthy framework. The initiative encourages fair competition, transparency, and trust among airlines, and helps to accelerate the uptake of sustainable aviation fuels to cut flight emissions. It is important for airlines to join this initiative if they want to display flight emissions and use these estimates when, for example, offering sustainable aviation fuel or other emissions offsets to passengers.\n",
      "🧠 Generated Answer (Model 3):\n",
      "The purpose of the EU Flight Emissions Label (FEL) initiative involving the European Union Aviation Safety Agency (EASA) and Air France-KLM is to enhance transparency and sustainability in European aviation by providing a trustworthy framework for airlines to share flight emissions data with passengers. The initiative aims to provide a reliable and harmonised methodology for estimating flight emissions, considering operational factors like aircraft type, passenger numbers, and freight volume on board, as well as the amount and type of aviation fuels uplifted per airport. This will help passengers make more informed decisions when comparing and booking flights among competitors and protect them from misleading greenwashing claims. Ultimately, this initiative will strengthen consumer protection laws within the EU. The agreement marks a major advance in making the Flight Emissions Label more widely available, setting out a reliable and harmonised methodology for estimating flight emissions.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "🔢 Pair 7\n",
      "❓ Question:\n",
      "What were the key findings and contributions of the two-year research initiative on circular economy monitoring, and how many indicators were ultimately selected and tested?\n",
      "\n",
      "✅ Reference Answer (Original ChatGPT):\n",
      "The two-year research initiative on circular economy monitoring resulted in a comprehensive set of options for tracking circularity across multiple sectors, including batteries, vehicles, electronics, bioeconomy, food, construction, plastics, and textiles. Out of over 730 potential indicators catalogued, 60 were selected and tested through 19 detailed case studies. These indicators support policy development by offering scalable monitoring tools at regional, national, and international levels, and across households, companies, and industries. The project aimed to strengthen understanding and implementation of circular economy policies and tools to address resource use and climate challenges.\n",
      "\n",
      "🤖 Generated Answer (Model 1):\n",
      "The two-year research initiative on circular economy monitoring concluded with a set of key findings and contributions that advanced our understanding of effective circular economy policies and support tools. The initiative focused on complementary and expanding aspects of existing monitoring efforts, such as the Circular Economy Monitoring Framework, to develop a comprehensive set of options for monitoring circularity across critical policy areas and sectors. Ultimately, 42 indicators were selected and tested as a result of the research project.\n",
      "\n",
      "🧠 Generated Answer (Model 2):\n",
      "The two-year research initiative on circular economy monitoring concluded by presenting a comprehensive set of options for monitoring circularity across critical policy areas and sectors. The project catalogued over 730 potential indicators, which were accessible through the Circular Economy Indicators Tool. After considering existing monitoring efforts such as the Circular Economy Monitoring Framework, the project focused on complementary and expanding aspects such as product-service systems. Ultimately, the project selected and tested a number of indicators, although the exact number is not specified in the provided context.\n",
      "🧠 Generated Answer (Model 3):\n",
      "The two-year research initiative on circular economy monitoring concluded by presenting a comprehensive set of options for monitoring circularity across critical policy areas and sectors. The project catalogued over 730 potential indicators, and after taking existing monitoring efforts and focusing on complementary and expanding aspects such as product-service systems, 60 indicators were ultimately selected and tested via 19 detailed case studies. The project advanced our understanding of effective circular economy policies and support tools, laying the foundation for future efforts to monitor and drive the circular economy. It tackles resource use and climate change challenges, fostering the transition to a more sustainable and equitable society.\n",
      "\n",
      "The project's key findings and contributions include:\n",
      "\n",
      "1. Cataloging over 730 potential indicators accessible through the Circular Economy Indicators Tool, which provides a comprehensive resource for monitoring circularity.\n",
      "2. Taking existing monitoring efforts, such as the Circular Economy Monitoring Framework, into consideration and focusing on complementary and expanding aspects such as product-service systems.\n",
      "3. Selecting and testing 60 indicators via 19 detailed case studies, which provides valuable insights into effective circular economy policies and support tools.\n",
      "4.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "🔢 Pair 8\n",
      "❓ Question:\n",
      "Why does the European Union continue to commemorate the genocide in Srebrenica, and what is its stance on historical denial and reconciliation in the Western Balkans?\n",
      "\n",
      "✅ Reference Answer (Original ChatGPT):\n",
      "The European Union commemorates the genocide in Srebrenica to honor the memory of over 8,300 Bosniak men and boys who were killed in July 1995 and to support the survivors and families still affected by the tragedy. The EU emphasizes the importance of preserving historical truth, recognizing its past responsibility in failing to prevent the atrocity, and firmly condemns any denial, distortion, or glorification of war crimes. The EU urges political leaders in Bosnia and Herzegovina and the Western Balkans to acknowledge established facts, support genuine reconciliation, and work towards a peaceful and unified future. It reaffirms its commitment to Bosnia and Herzegovina's path toward EU membership.\n",
      "\n",
      "🤖 Generated Answer (Model 1):\n",
      "The European Union continues to commemorate the genocide in Srebrenica because it was a significant event in the history of the Western Balkans and it represents one of the darkest moments in European history. The European Union firmly rejects and condemns any denial, distortion, or minimisation of the Srebrenica genocide, as well as the glorification of war criminals.\n",
      "\n",
      "In terms of historical denial and reconciliation in the Western Balkans, the European Union believes that it is essential to confront the roots of hatred that led to these atrocities in order to promote reconciliation and peace in the region. The European Union stands with the people of Bosnia and Herzegovina and supports their efforts to promote reconciliation and peace. The European Union also encourages dialogue and cooperation between different communities in the region to promote a shared future for all. The European Union recognises that historical denial and hatred are major barriers to reconciliation and peace and believes that it is essential to address these issues in order to promote a stable and prosperous future for the Western Balkans.\n",
      "\n",
      "🧠 Generated Answer (Model 2):\n",
      "The European Union continues to commemorate the genocide in Srebrenica as a way to remember and pay tribute to the more than 8,300 victims and their families, who continue to grieve after three decades. The EU also acknowledges its responsibility for failing to prevent and stop the genocide and firmly rejects any denial, distortion, or minimisation of the Srebrenica genocide, as well as the glorification of war criminals. The EU stands among the darkest chapters in Europe's collective memory and recognises the importance of historical reconciliation in the Western Balkans.\n",
      "🧠 Generated Answer (Model 3):\n",
      "The European Union continues to commemorate the genocide in Srebrenica because it stands among the darkest chapters in Europe's collective memory. The Union acknowledges that victims were systematically executed and buried in mass graves within the UN-designated 'safe area' of Srebrenica, and that this was a failure in preventing and stopping the genocide. The Union firmly rejects and condemns any denial, distortion, or minimisation of the Srebrenica genocide, as well as the glorification of war criminals. Political leaders in Bosnia and Herzegovina and across the Western Balkans have a great responsibility in acknowledging the established facts and genuinely honouring the victims. They must actively work towards reconciliation by confronting the roots of hatred that led to these atrocities. This is the only way towards a brighter and prosperous future for the next generation. The European Union stands with the people of Bosnia and Herzegovina, and is fully committed to supporting its country on its path towards EU membership.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "🔢 Pair 9\n",
      "❓ Question:\n",
      "What changes were introduced in the European Commission's 'quick fix' amendments to the European Sustainability Reporting Standards (ESRS), and how do these impact wave one companies reporting for financial years 2025 and 2026?\n",
      "\n",
      "✅ Reference Answer (Original ChatGPT):\n",
      "The European Commission introduced 'quick fix' amendments to the European Sustainability Reporting Standards (ESRS) to ease the burden on 'wave one' companies that began reporting in financial year 2024. The amendments allow these companies to omit certain disclosures, such as anticipated financial impacts of sustainability-related risks, not only for 2024 but also for 2025 and 2026. The changes align wave one companies with the phase-in provisions granted to smaller firms and address the fact that they were excluded from the 'stop-the-clock' Directive that deferred requirements for 'wave two' and 'wave three' companies. A broader ESRS revision is underway, aiming for simplification and consistency by 2027.\n",
      "\n",
      "🤖 Generated Answer (Model 1):\n",
      "The European Commission has adopted targeted \"quick fix\" amendments to the first set of European Sustainability Reporting Standards (ESRS), which impact wave one companies reporting for financial years 2025 and 2026. These amendments aim to clarify certain provisions and reduce the number of data requirements, which will help simplify reporting requirements for companies. Additionally, the Commission is working on a broader revision of the ESRS, with the goal of improving consistency with other legislation and reducing the number of data requirements further. This revision will be implemented in the future, but it is not clear when it will be finalized and implemented. Overall, the changes introduced in the \"quick fix\" amendments and future revisions to the ESRS aim to improve the clarity and simplicity of sustainability reporting requirements for companies.\n",
      "\n",
      "🧠 Generated Answer (Model 2):\n",
      "The European Commission introduced targeted \"quick fix\" amendments to the European Sustainability Reporting Standards (ESRS) to allow wave one companies, which report on financial year 2024, to omit certain sustainability-related information. These amendments apply from financial year 2025 and will allow wave one companies to omit the same information for financial years 2025 and 2026. This means that wave one companies will not have to report any additional information compared to financial year 2024. The Commission is also working on a broader revision of the ESRS, with the aim of substantially reducing the number of data requirements, clarifying provisions deemed unclear, and improving consistency with other pieces of legislation.\n",
      "🧠 Generated Answer (Model 3):\n",
      "The European Commission introduced targeted \"quick fix\" amendments to the first set of European Sustainability Reporting Standards (ESRS). These changes aim to reduce the burden and increase certainty for wave one companies that had to start reporting for financial year 2024. According to the current ESRS, companies reporting on financial year 2024 can omit information on, amongst other things, the anticipated financial effects of certain sustainability-related risks. The \"quick fix\" amendment, which applies from financial year 2025, allows wave one companies to omit that same information for financial years 2025 and 2026. This means wave one companies will not have to report additional information compared to financial year 2024. Moreover, for financial years 2025 and 2026, wave one companies with more than 750 employees will benefit from most of the same phase-in provisions that currently apply to companies with up to 750 employees. These changes were necessary because wave one companies were not captured by the \"stop-the-clock\" Directive, which delayed by two years the sustainability reporting requirements for companies that report\n",
      "----------------------------------------------------------------------------------------------------\n",
      "🔢 Pair 10\n",
      "❓ Question:\n",
      "What led to the imposition of anti-dumping duties on lysine imports from China, and what are the expected effects on the EU industry and environmental sustainability?\n",
      "\n",
      "✅ Reference Answer (Original ChatGPT):\n",
      "The EU imposed anti-dumping duties on lysine imports from China after an investigation revealed that these imports were being sold at unfairly low prices, harming European producers. Lysine, a critical amino acid used in animal feed, pharmaceuticals, and dietary supplements, plays an essential role in improving animal nutrition and reducing environmental impacts such as nitrogen pollution. The duties are expected to level the playing field for EU lysine manufacturers and promote fair competition while supporting environmentally sustainable agricultural practices in the EU.\n",
      "\n",
      "🤖 Generated Answer (Model 1):\n",
      "The imposition of anti-dumping duties on lysine imports from China was the result of an investigation that found that these imports were harming EU industry. The investigation revealed that the Chinese companies were selling lysine at artificially low prices, which made it difficult for EU lysine makers to compete. The duties imposed today were expected to help EU lysine makers to compete on a more equal footing with their Chinese counterparts.\n",
      "\n",
      "The decision to impose duties on lysine imports from China also had implications for environmental sustainability. Dumping of lysine from China led to environmental pollution in the EU, and the imposition of duties could help to protect the environment by reducing the amount of imported lysine on the market.\n",
      "\n",
      "However, it is important to note that the imposition of duties could also have unintended consequences. It is possible that the reduced availability of imported lysine could lead to higher prices for EU consumers, and it is also possible that the reduction in imports could lead to a decrease in competition, which could be harmful to innovation and technological progress.\n",
      "\n",
      "Overall, the imposition of anti-dumping duties on lysine imports from China reflects the\n",
      "\n",
      "🧠 Generated Answer (Model 2):\n",
      "The imposition of anti-dumping duties on lysine imports from China was a result of an investigation that found that dumped imports were harming EU industry. The expected effects on the EU industry are that it should help EU lysine makers to compete on a more equal footing with their Chinese counterparts. As for environmental sustainability, no information was provided in the context regarding the impact of these duties on the environment.\n",
      "🧠 Generated Answer (Model 3):\n",
      "The imposition of anti-dumping duties on lysine imports from China was due to an investigation that showed that dumped imports of lysine were harming EU industry. The expected effects on the EU industry and environmental sustainability are that the duties imposed today should help EU lysine makers to compete on a more equal footing with their Chinese counterparts. The measures aim to protect EU producers of lysine from unfair trading practices, which enables farmers to meet their animals’ nutritional requirements more accurately, saving land and reducing nitrogen pollution – such as nitrates and ammonia – on livestock farms.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i, (original, model1, model2, model3) in enumerate(zip(data, updated_qa_data, updated_qa_data_v2,model_qa_v3)):\n",
    "    print(f\"🔢 Pair {i+1}\")\n",
    "    print(f\"❓ Question:\\n{original['question']}\\n\")\n",
    "    print(f\"✅ Reference Answer (Original ChatGPT):\\n{original['answer']}\\n\")\n",
    "    print(f\"🤖 Generated Answer (Model 1):\\n{model1['answer']}\\n\")\n",
    "    print(f\"🧠 Generated Answer (Model 2):\\n{model2['answer']}\")\n",
    "    print(f\"🧠 Generated Answer (Model 3):\\n{model3['answer']}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    if i == 9:\n",
    "        break  # Show only the top 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36891bb6-000c-4819-a2d3-dfefa1687cb4",
   "metadata": {},
   "source": [
    "Let's numerically evaluate the 3rd model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b93468ce-af99-4032-84b7-aaa8130307aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_qa_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b1bfcc0e-ea53-44df-bfaa-3756534a83ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  8.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Average BLEU Score (1-gram): 0.2736\n",
      "🔹 Average Semantic Similarity: 0.8424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "subset_data = data[:10] \n",
    "subset_model_qa_v3 = model_qa_v3[:10]\n",
    "\n",
    "assert len(subset_data) == len(subset_model_qa_v3), \"Mismatch in number of QA pairs\"\n",
    "\n",
    "bleu_scores = []\n",
    "similarity_scores = []\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "for gpt_item, my_item in tqdm.tqdm(zip(subset_data, subset_model_qa_v3), total=10):\n",
    "    reference = gpt_item['answer'].strip()\n",
    "    hypothesis = my_item['answer'].strip()\n",
    "\n",
    "    # BLEU\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    bleu = sentence_bleu([reference.split()], hypothesis.split(), weights=(1, 0, 0, 0), smoothing_function=smoothie)\n",
    "    bleu_scores.append(bleu)\n",
    "\n",
    "    # Semantic similarity\n",
    "    emb_ref = model.encode(reference, convert_to_tensor=True)\n",
    "    emb_hyp = model.encode(hypothesis, convert_to_tensor=True)\n",
    "    sim_score = util.pytorch_cos_sim(emb_ref, emb_hyp).item()\n",
    "    similarity_scores.append(sim_score)\n",
    "\n",
    "# Averages\n",
    "average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "average_similarity = sum(similarity_scores) / len(similarity_scores)\n",
    "\n",
    "print(f\"🔹 Average BLEU Score (1-gram): {average_bleu:.4f}\")\n",
    "print(f\"🔹 Average Semantic Similarity: {average_similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2a38f77d-e258-43b6-8d54-8082f31f20e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 32.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Average METEOR: 0.391\n",
      "🔹 Average ROUGE-1: 0.4342\n",
      "🔹 Average ROUGE-2: 0.174\n",
      "🔹 Average ROUGE-L: 0.2913\n"
     ]
    }
   ],
   "source": [
    "rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "meteor_scores = []\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougeL_scores = []\n",
    "\n",
    "for gpt, mine in tqdm.tqdm(zip(subset_data, subset_model_qa_v3), total=len(subset_data)):\n",
    "    ref = gpt[\"answer\"].strip()\n",
    "    hyp = mine[\"answer\"].strip()\n",
    "\n",
    "    # METEOR\n",
    "    meteor = single_meteor_score(word_tokenize(ref), word_tokenize(hyp))\n",
    "    meteor_scores.append(meteor)\n",
    "\n",
    "    # ROUGE\n",
    "    scores = rouge.score(ref, hyp)\n",
    "    rouge1_scores.append(scores[\"rouge1\"].fmeasure)\n",
    "    rouge2_scores.append(scores[\"rouge2\"].fmeasure)\n",
    "    rougeL_scores.append(scores[\"rougeL\"].fmeasure)\n",
    "\n",
    "print(\"🔹 Average METEOR:\", round(sum(meteor_scores) / len(meteor_scores), 4))\n",
    "print(\"🔹 Average ROUGE-1:\", round(sum(rouge1_scores) / len(rouge1_scores), 4))\n",
    "print(\"🔹 Average ROUGE-2:\", round(sum(rouge2_scores) / len(rouge2_scores), 4))\n",
    "print(\"🔹 Average ROUGE-L:\", round(sum(rougeL_scores) / len(rougeL_scores), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07569ee2-1645-4860-a807-071c1f69dc16",
   "metadata": {},
   "source": [
    "The Performance of this approach is better than all previous approaches. The only problem with this is that it takes a lot of time even 5 minutes to generate a single answer, which is very bad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4c805c-69f8-4758-8862-83b507d4c64b",
   "metadata": {},
   "source": [
    "The similar chunk retrieval is very fast the prompt creation is very fast, the thing which is taking so much time is Mistral 7B which I am using as my chatbot. Let's try for once Gemini 1.5 Flash and see if the latency improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "78814e89-139b-4c34-bda9-259ca65a3784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-generativeai\n",
      "  Downloading google_generativeai-0.8.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting google-ai-generativelanguage==0.6.15 (from google-generativeai)\n",
      "  Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting google-api-core (from google-generativeai)\n",
      "  Downloading google_api_core-2.25.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-api-python-client (from google-generativeai)\n",
      "  Downloading google_api_python_client-2.176.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting google-auth>=2.15.0 (from google-generativeai)\n",
      "  Downloading google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: protobuf in c:\\users\\shri\\anaconda3\\lib\\site-packages (from google-generativeai) (4.25.3)\n",
      "Requirement already satisfied: pydantic in c:\\users\\shri\\anaconda3\\lib\\site-packages (from google-generativeai) (2.8.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\shri\\anaconda3\\lib\\site-packages (from google-generativeai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\shri\\anaconda3\\lib\\site-packages (from google-generativeai) (4.14.1)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core->google-generativeai)\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.73.0)\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading grpcio_status-1.73.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading grpcio_status-1.73.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.72.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.72.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.71.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting protobuf (from google-generativeai)\n",
      "  Downloading protobuf-5.29.5-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.7.9)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth>=2.15.0->google-generativeai) (0.4.8)\n",
      "Collecting httplib2<1.0.0,>=0.19.0 (from google-api-python-client->google-generativeai)\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google-generativeai)\n",
      "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google-generativeai)\n",
      "  Downloading uritemplate-4.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.1.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from pydantic->google-generativeai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from pydantic->google-generativeai) (2.20.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\shri\\anaconda3\\lib\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Downloading google_generativeai-0.8.5-py3-none-any.whl (155 kB)\n",
      "Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.3 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 0.5/1.3 MB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.0/1.3 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 2.1 MB/s eta 0:00:00\n",
      "Downloading google_api_core-2.25.1-py3-none-any.whl (160 kB)\n",
      "Downloading google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Downloading grpcio_status-1.71.2-py3-none-any.whl (14 kB)\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading protobuf-5.29.5-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading google_api_python_client-2.176.0-py3-none-any.whl (13.7 MB)\n",
      "   ---------------------------------------- 0.0/13.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/13.7 MB 3.4 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 1.0/13.7 MB 3.1 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 1.8/13.7 MB 3.0 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 2.4/13.7 MB 3.0 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 3.1/13.7 MB 3.0 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 3.7/13.7 MB 3.1 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 4.2/13.7 MB 3.0 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 4.7/13.7 MB 2.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 5.0/13.7 MB 2.8 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 5.5/13.7 MB 2.7 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 6.3/13.7 MB 2.7 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 6.8/13.7 MB 2.7 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 7.3/13.7 MB 2.7 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 7.6/13.7 MB 2.6 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 8.1/13.7 MB 2.6 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 8.4/13.7 MB 2.5 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 8.9/13.7 MB 2.5 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 9.4/13.7 MB 2.5 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 9.7/13.7 MB 2.5 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 10.0/13.7 MB 2.4 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 10.2/13.7 MB 2.4 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 10.5/13.7 MB 2.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 11.0/13.7 MB 2.3 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 11.5/13.7 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 11.8/13.7 MB 2.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 12.1/13.7 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 12.3/13.7 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 12.6/13.7 MB 2.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 13.1/13.7 MB 2.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 13.1/13.7 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.4/13.7 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.7/13.7 MB 2.1 MB/s eta 0:00:00\n",
      "Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Downloading uritemplate-4.2.0-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: uritemplate, rsa, protobuf, httplib2, proto-plus, googleapis-common-protos, google-auth, grpcio-status, google-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google-generativeai\n",
      "\n",
      "  Attempting uninstall: protobuf\n",
      "\n",
      "    Found existing installation: protobuf 4.25.3\n",
      "\n",
      "    Uninstalling protobuf-4.25.3:\n",
      "\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "      Successfully uninstalled protobuf-4.25.3\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   ------ ---------------------------------  2/13 [protobuf]\n",
      "   --------------- ------------------------  5/13 [googleapis-common-protos]\n",
      "   --------------- ------------------------  5/13 [googleapis-common-protos]\n",
      "   ------------------ ---------------------  6/13 [google-auth]\n",
      "   --------------------------- ------------  9/13 [google-api-core]\n",
      "   --------------------------- ------------  9/13 [google-api-core]\n",
      "   ------------------------------ --------- 10/13 [google-api-python-client]\n",
      "   ------------------------------ --------- 10/13 [google-api-python-client]\n",
      "   ------------------------------ --------- 10/13 [google-api-python-client]\n",
      "   ------------------------------ --------- 10/13 [google-api-python-client]\n",
      "   --------------------------------- ----- 11/13 [google-ai-generativelanguage]\n",
      "   --------------------------------- ----- 11/13 [google-ai-generativelanguage]\n",
      "   --------------------------------- ----- 11/13 [google-ai-generativelanguage]\n",
      "   --------------------------------- ----- 11/13 [google-ai-generativelanguage]\n",
      "   --------------------------------- ----- 11/13 [google-ai-generativelanguage]\n",
      "   --------------------------------- ----- 11/13 [google-ai-generativelanguage]\n",
      "   --------------------------------- ----- 11/13 [google-ai-generativelanguage]\n",
      "   --------------------------------- ----- 11/13 [google-ai-generativelanguage]\n",
      "   --------------------------------- ----- 11/13 [google-ai-generativelanguage]\n",
      "   --------------------------------- ----- 11/13 [google-ai-generativelanguage]\n",
      "   --------------------------------- ----- 11/13 [google-ai-generativelanguage]\n",
      "   --------------------------------- ----- 11/13 [google-ai-generativelanguage]\n",
      "   ------------------------------------ --- 12/13 [google-generativeai]\n",
      "   ------------------------------------ --- 12/13 [google-generativeai]\n",
      "   ---------------------------------------- 13/13 [google-generativeai]\n",
      "\n",
      "Successfully installed google-ai-generativelanguage-0.6.15 google-api-core-2.25.1 google-api-python-client-2.176.0 google-auth-2.40.3 google-auth-httplib2-0.2.0 google-generativeai-0.8.5 googleapis-common-protos-1.70.0 grpcio-status-1.71.2 httplib2-0.22.0 proto-plus-1.26.1 protobuf-5.29.5 rsa-4.9.1 uritemplate-4.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "afb828c1-af43-4535-88aa-67985178037c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2205997c-38a3-4337-bca0-07b1ccbc6e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_API_KEY\"] = \"***********\" \n",
    "\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "\n",
    "#  Gemini 1.5 Flash model\n",
    "gemini_model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "100c97ed-5c46-41f0-88da-c41c2a444e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_qa = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c9f055e5-da1d-4cce-aca1-b277cb0eae05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▍                                                                               | 9/297 [00:33<17:56,  3.74s/it]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for item in tqdm.tqdm(data):\n",
    "    query = item[\"question\"]\n",
    "    # Get relevant chunks\n",
    "    top_chunks = retrieve_relevant_chunks_cosine(query,model_2,index_3,all_chunks_3, top_k=5)\n",
    "\n",
    "    #  Build prompt\n",
    "    prompt = build_prompt(query, top_chunks)\n",
    "\n",
    "    response = gemini_model.generate_content(prompt)\n",
    "    answer = response.text\n",
    "\n",
    "    # Save updated QA\n",
    "    gemini_qa.append({\n",
    "        \"question\": query,\n",
    "        \"answer\": answer\n",
    "    })\n",
    "    i+=1\n",
    "    if(i>9):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "35a1d659-318d-4912-b134-28f2ef0d9312",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔢 Pair 1\n",
      "❓ Question:\n",
      "What collaborative initiative was announced by the European Microfinance Network (EMN) and the Microfinance Centre (MFC) in April 2024, and what are its main objectives for supporting the European microfinance sector?\n",
      "\n",
      "✅ Reference Answer (Original ChatGPT):\n",
      "In April 2024, the European Microfinance Network (EMN) and the Microfinance Centre (MFC) announced a joint strategy aimed at strengthening the European microfinance sector. This collaborative initiative focuses on promoting financial inclusion, developing capacity-building resources, and creating a unified voice to influence policy-making at the European level. The partnership aims to better support microfinance institutions and expand access to responsible finance for underserved populations across Europe.\n",
      "\n",
      "🤖 Generated Answer (Model 1):\n",
      "The European Microfinance Network (EMN) and the Microfinance Centre (MFC) announced a collaborative initiative in April 2024 called \"Microfinance in Europe: Survey Report\". The main objectives of this initiative are to provide valuable insights into the contribution of microfinance institutions to social inclusion, entrepreneurship, and local development, and to serve as an important policy tool supporting evidence-based decision-making for policymakers working to strengthen financial inclusion and the social economy. The initiative also functions as a benchmarking reference for MFIs, helping them evaluate their performance and position within the wider European landscape.\n",
      "\n",
      "🧠 Gemini Answer (Model 2):\n",
      "The provided text announces the 12th edition of the \"Microfinance in Europe: Survey Report,\" a collaborative initiative by the European Microfinance Network (EMN) and the Microfinance Centre (MFC).  This report's main objectives are to:\n",
      "\n",
      "1.  Provide the most comprehensive dataset available on the European microfinance sector, capturing data from the vast majority of European microfinance institutions (MFIs).\n",
      "2.  Offer valuable insights into how MFIs contribute to social inclusion, entrepreneurship, and local development.\n",
      "3.  Serve as a policy tool supporting evidence-based decision-making for policymakers working to strengthen financial inclusion and the social economy.\n",
      "4.  Function as a benchmarking reference for MFIs, helping them evaluate their performance and position within the wider European landscape.  The 2024 edition specifically focuses on the types of businesses reached by microfinance, the social performance of business loans, and the impact measurement approaches adopted by MFIs.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "🔢 Pair 2\n",
      "❓ Question:\n",
      "How did the AccessibleEU initiative exceed its yearly objectives in promoting accessibility for persons with disabilities across Europe?\n",
      "\n",
      "✅ Reference Answer (Original ChatGPT):\n",
      "The AccessibleEU initiative surpassed its yearly objectives by significantly advancing accessibility measures across European Union member states. It achieved this through extensive stakeholder engagement, implementation of accessibility audits, development of best practice toolkits, and organization of awareness campaigns. The initiative focused on inclusive design, policy harmonization, and improved accessibility in public infrastructure, thereby contributing to a more inclusive environment for persons with disabilities throughout Europe.\n",
      "\n",
      "🤖 Generated Answer (Model 1):\n",
      "The AccessibleEU initiative exceeded its yearly objectives in promoting accessibility for persons with disabilities across Europe in several ways. One of the key achievements was the refinement and implementation of the set of indicators monitoring accessibility initiatives across all Member States, which helped to collect a large amount of information on commitments and efforts made by each Member State to implement accessibility legislation and standards. Additionally, the initiative successfully hosted 88 events, including two online courses, grew the Community of Practiceto over 3,400 members, established aMoodle forum, and kept building its digital library with 163 new Good Practices and 136 new references on accessibility. Furthermore, the inititative held a high-level event in Brussels on 3rdJuly, bringing together European institutions, national governments, leading companies, disability organisations and accessibility experts to analyse the European Accessibility Act and emphasise the need to comply with it. All these achievements demonstrate the initiative's success in promoting accessibility for persons with disabilities across Europe.\n",
      "\n",
      "🧠 Gemini Answer (Model 2):\n",
      "AccessibleEU surpassed its 2024 objectives by:\n",
      "\n",
      "* **Bringing together stakeholders:** Successfully connecting policymakers, experts, and persons with disabilities.\n",
      "* **Providing guidance:** Offering training, awareness-raising, and technical publications to implement accessibility standards.\n",
      "* **Refining and implementing monitoring indicators:**  Collecting extensive data on Member States' commitments and efforts to implement accessibility legislation and standards, accessible via the AccessibleEU - Accessibility Monitoring website.\n",
      "* **Extensive outreach and community building:** Hosting 88 events (including two online courses), growing its Community of Practice to over 3,400 members, establishing a Moodle forum, and expanding its digital library with 163 new Good Practices and 136 new references on accessibility.  \n",
      "* **High-level event in Brussels:**  Analyzing the European Accessibility Act and emphasizing compliance, bringing together European institutions, national governments, companies, disability organizations, and accessibility experts.  The event covered various sectors including ICT, transport, digital services, telecommunications, public procurement, and the built environment.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "🔢 Pair 3\n",
      "❓ Question:\n",
      "What key policy development took place on July 3, 2025, involving the European Commission and its implications for AI regulation in the EU?\n",
      "\n",
      "✅ Reference Answer (Original ChatGPT):\n",
      "On July 3, 2025, the European Commission and the European Parliament reached a political agreement on the AI Act, a landmark piece of legislation aimed at regulating artificial intelligence technologies across the EU. This act sets out a risk-based framework, classifying AI systems by their potential impact on fundamental rights and safety. It introduces strict obligations for high-risk AI applications, including requirements for transparency, accountability, and human oversight. The policy aims to foster trust in AI while supporting innovation and ensuring compliance with EU values.\n",
      "\n",
      "🤖 Generated Answer (Model 1):\n",
      "It is not specified in the provided context what key policy development took place on July 3, 2025, involving the European Commission and its implications for AI regulation in the EU.\n",
      "\n",
      "🧠 Gemini Answer (Model 2):\n",
      "The provided text does not contain information about any key policy development on July 3, 2025, regarding AI regulation in the EU.  The latest date mentioned is January 24, 2024, when the Commission adopted five initiatives to strengthen the EU's economic security, including a legislative proposal for AI regulation.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "🔢 Pair 4\n",
      "❓ Question:\n",
      "What environmentally sustainable measures has Hadag, Hamburg’s ferry service operator, implemented to modernize its fleet and reduce emissions?\n",
      "\n",
      "✅ Reference Answer (Original ChatGPT):\n",
      "Hadag, the ferry service operator in Hamburg, has undertaken a modernization program that includes the introduction of hybrid and fully electric ferries to its fleet. These environmentally sustainable measures are designed to reduce greenhouse gas emissions and improve energy efficiency in public transport. The initiative aligns with Hamburg’s broader climate strategy and emphasizes the shift towards low-emission urban mobility solutions in maritime transportation.\n",
      "\n",
      "🤖 Generated Answer (Model 1):\n",
      "Hadag, Hamburg's ferry service operator, has implemented an AI-powered platform to optimize the scheduling and operation of its electric ferry fleet. The platform addresses key challenges in electric ferry operations, such as battery range and fluctuating energy costs. The initiative supports the city's goal to reduce transport-related emissions significantly. As a member of the Carbon Neutral Cities Alliance, Hamburg aims to cut emissions by 55% by 2030 and by 95% by 2050. Electric ferries play a key role in reaching these targets. The AI system will support daily operations by tailoring charging strategies, managing energy demand, and enhancing communication with passengers via digital displays and mobile apps. With three electric ferries already delivered in 2024 and more on the way, Hadag is on track to operate a fully zero-emissions fleet - demonstrating how advanced technology can drive real climate action in public transport.\n",
      "\n",
      "🧠 Gemini Answer (Model 2):\n",
      "Hadag has implemented an AI-powered platform to optimize the scheduling and operation of its electric ferry fleet.  This system streamlines ferry timetables and charging routines, addresses challenges like battery range and fluctuating energy costs, and improves passenger information.  The initiative is part of Hamburg's broader goal to reduce transport-related emissions, aiming for a 55% reduction by 2030 and 95% by 2050.  Hadag is transitioning to a fully zero-emissions fleet, with three electric ferries already delivered in 2024 and more planned.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "🔢 Pair 5\n",
      "❓ Question:\n",
      "What actions did the Israeli government undertake following recent Cabinet resolutions concerning the security situation in the Gaza Strip?\n",
      "\n",
      "✅ Reference Answer (Original ChatGPT):\n",
      "Following recent Cabinet resolutions, the Israeli government implemented heightened security operations targeting militant infrastructure in the Gaza Strip. These actions included airstrikes on weapon manufacturing sites and border surveillance enhancements. The operations were positioned as a response to ongoing threats and aimed at restoring stability and deterring further escalation in the region.\n",
      "\n",
      "🤖 Generated Answer (Model 1):\n",
      "The Israeli government undertook several actions following recent Cabinet resolutions concerning the security situation in the Gaza Strip. These actions include, among other things, a substantial increase of daily trucks for food and non-food items to enter Gaza, the opening of several other crossing points in both the northern and southern areas; the reopening of the Jordanian and Egyptian aid routes; enabling the distribution of food supplies through bakeries and public kitchens throughout the Gaza strip; the resumption of fuel deliveries for use by humanitarian facilities, up to an operational level; the protection of aid workers;  the repair and facilitation of works on vital infrastructure like the resumption of the power supply to the water desalination facility. Additionally, the EU stands ready to coordinate with all relevant humanitarian stakeholders, UN agencies and NGOs on the ground, to ensure swift implementation of those urgent steps.\n",
      "\n",
      "🧠 Gemini Answer (Model 2):\n",
      "Following cabinet resolutions and dialogue with the EU, the Israeli government agreed to several measures to improve the humanitarian situation in Gaza.  These include substantially increasing the daily number of trucks carrying food and non-food items into Gaza, opening additional crossing points in the north and south, reopening Jordanian and Egyptian aid routes, enabling food distribution via bakeries and public kitchens, resuming fuel deliveries for humanitarian facilities, protecting aid workers, and repairing vital infrastructure such as the water desalination plant's power supply.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "🔢 Pair 6\n",
      "❓ Question:\n",
      "What is the purpose of the EU Flight Emissions Label (FEL) initiative involving the European Union Aviation Safety Agency (EASA) and Air France-KLM, and how does it aim to enhance transparency and sustainability in European aviation?\n",
      "\n",
      "✅ Reference Answer (Original ChatGPT):\n",
      "The EU Flight Emissions Label (FEL) initiative is a collaborative effort between the European Union Aviation Safety Agency (EASA) and Air France-KLM aimed at enhancing transparency in flight emissions reporting. Under a Memorandum of Cooperation signed in 2024, Air France and KLM will pilot the FEL scheme to provide reliable, harmonized data on flight emissions based on factors like aircraft type, passenger load, freight volume, and fuel type. The goal is to allow passengers to make more informed choices, protect them from greenwashing, and promote the adoption of sustainable aviation fuels, thereby reinforcing decarbonization and fair competition within the EU aviation market.\n",
      "\n",
      "🤖 Generated Answer (Model 1):\n",
      "The purpose of the EU Flight Emissions Label (FEL) initiative involving the European Union Aviation Safety Agency (EASA) and Air France-KLM is to enhance transparency and sustainability in European aviation by providing a trustworthy framework for airlines to share flight emissions data with passengers. The initiative aims to provide a reliable and harmonised methodology for estimating flight emissions, considering operational factors like aircraft type, passenger numbers, and freight volume on board, as well as the amount and type of aviation fuels uplifted per airport. This will help passengers make more informed decisions when comparing and booking flights among competitors and protect them from misleading greenwashing claims. Ultimately, this initiative will strengthen consumer protection laws within the EU. The agreement marks a major advance in making the Flight Emissions Label more widely available, setting out a reliable and harmonised methodology for estimating flight emissions.\n",
      "\n",
      "🧠 Gemini Answer (Model 2):\n",
      "The EU Flight Emissions Label (FEL) initiative, involving EASA and Air France-KLM, aims to create a trustworthy framework for airlines to share flight emissions data with passengers.  This enhances transparency by allowing passengers to see the emissions associated with their flights, enabling more informed decisions when booking and comparing flights.  It also combats misleading greenwashing claims.  The initiative promotes sustainability by accelerating the uptake of sustainable aviation fuels (SAFs) and other emissions offsets, as airlines can use the emission estimates to offer these options to passengers.  The voluntary participation of airlines, initially exemplified by Air France-KLM, allows for a pre-assessment of emission values and testing of digital tools, ultimately contributing to a more sustainable and transparent future for European aviation.  Furthermore,  FEL provides a harmonized methodology for estimating flight emissions, considering factors like aircraft type, passenger numbers, freight volume, and fuel type.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "🔢 Pair 7\n",
      "❓ Question:\n",
      "What were the key findings and contributions of the two-year research initiative on circular economy monitoring, and how many indicators were ultimately selected and tested?\n",
      "\n",
      "✅ Reference Answer (Original ChatGPT):\n",
      "The two-year research initiative on circular economy monitoring resulted in a comprehensive set of options for tracking circularity across multiple sectors, including batteries, vehicles, electronics, bioeconomy, food, construction, plastics, and textiles. Out of over 730 potential indicators catalogued, 60 were selected and tested through 19 detailed case studies. These indicators support policy development by offering scalable monitoring tools at regional, national, and international levels, and across households, companies, and industries. The project aimed to strengthen understanding and implementation of circular economy policies and tools to address resource use and climate challenges.\n",
      "\n",
      "🤖 Generated Answer (Model 1):\n",
      "The two-year research initiative on circular economy monitoring concluded by presenting a comprehensive set of options for monitoring circularity across critical policy areas and sectors. The project catalogued over 730 potential indicators, and after taking existing monitoring efforts and focusing on complementary and expanding aspects such as product-service systems, 60 indicators were ultimately selected and tested via 19 detailed case studies. The project advanced our understanding of effective circular economy policies and support tools, laying the foundation for future efforts to monitor and drive the circular economy. It tackles resource use and climate change challenges, fostering the transition to a more sustainable and equitable society.\n",
      "\n",
      "The project's key findings and contributions include:\n",
      "\n",
      "1. Cataloging over 730 potential indicators accessible through the Circular Economy Indicators Tool, which provides a comprehensive resource for monitoring circularity.\n",
      "2. Taking existing monitoring efforts, such as the Circular Economy Monitoring Framework, into consideration and focusing on complementary and expanding aspects such as product-service systems.\n",
      "3. Selecting and testing 60 indicators via 19 detailed case studies, which provides valuable insights into effective circular economy policies and support tools.\n",
      "4.\n",
      "\n",
      "🧠 Gemini Answer (Model 2):\n",
      "The two-year research initiative developed a comprehensive set of indicators for monitoring circularity across various sectors (batteries, vehicles, electronics, bioeconomy, food, construction, plastics, packaging, textiles) and levels (regional, national, international, household, company, sectoral).  It built upon existing frameworks like the Circular Economy Monitoring Framework, adding complementary aspects such as product-service systems.  The project started with over 730 potential indicators, ultimately selecting and testing 60 of them through 19 detailed case studies.  The initiative advances understanding of effective circular economy policies and tools, contributing to a more sustainable and equitable society by addressing resource use and climate change challenges.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "🔢 Pair 8\n",
      "❓ Question:\n",
      "Why does the European Union continue to commemorate the genocide in Srebrenica, and what is its stance on historical denial and reconciliation in the Western Balkans?\n",
      "\n",
      "✅ Reference Answer (Original ChatGPT):\n",
      "The European Union commemorates the genocide in Srebrenica to honor the memory of over 8,300 Bosniak men and boys who were killed in July 1995 and to support the survivors and families still affected by the tragedy. The EU emphasizes the importance of preserving historical truth, recognizing its past responsibility in failing to prevent the atrocity, and firmly condemns any denial, distortion, or glorification of war crimes. The EU urges political leaders in Bosnia and Herzegovina and the Western Balkans to acknowledge established facts, support genuine reconciliation, and work towards a peaceful and unified future. It reaffirms its commitment to Bosnia and Herzegovina's path toward EU membership.\n",
      "\n",
      "🤖 Generated Answer (Model 1):\n",
      "The European Union continues to commemorate the genocide in Srebrenica because it stands among the darkest chapters in Europe's collective memory. The Union acknowledges that victims were systematically executed and buried in mass graves within the UN-designated 'safe area' of Srebrenica, and that this was a failure in preventing and stopping the genocide. The Union firmly rejects and condemns any denial, distortion, or minimisation of the Srebrenica genocide, as well as the glorification of war criminals. Political leaders in Bosnia and Herzegovina and across the Western Balkans have a great responsibility in acknowledging the established facts and genuinely honouring the victims. They must actively work towards reconciliation by confronting the roots of hatred that led to these atrocities. This is the only way towards a brighter and prosperous future for the next generation. The European Union stands with the people of Bosnia and Herzegovina, and is fully committed to supporting its country on its path towards EU membership.\n",
      "\n",
      "🧠 Gemini Answer (Model 2):\n",
      "The European Union commemorates the Srebrenica genocide to preserve the truth for future generations, ensuring they understand the systematic execution and burial of victims in a UN-designated safe area during the summer of 1995.  The EU acknowledges its past failure to prevent the genocide and recognizes its responsibility.\n",
      "\n",
      "The EU firmly rejects and condemns any denial, distortion, or minimization of the Srebrenica genocide, as well as the glorification of war criminals.  It believes that political leaders in Bosnia and Herzegovina and the Western Balkans must acknowledge the established facts, honor the victims, and actively work towards reconciliation by addressing the roots of hatred.  This, the EU believes, is the path to a brighter future.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "🔢 Pair 9\n",
      "❓ Question:\n",
      "What changes were introduced in the European Commission's 'quick fix' amendments to the European Sustainability Reporting Standards (ESRS), and how do these impact wave one companies reporting for financial years 2025 and 2026?\n",
      "\n",
      "✅ Reference Answer (Original ChatGPT):\n",
      "The European Commission introduced 'quick fix' amendments to the European Sustainability Reporting Standards (ESRS) to ease the burden on 'wave one' companies that began reporting in financial year 2024. The amendments allow these companies to omit certain disclosures, such as anticipated financial impacts of sustainability-related risks, not only for 2024 but also for 2025 and 2026. The changes align wave one companies with the phase-in provisions granted to smaller firms and address the fact that they were excluded from the 'stop-the-clock' Directive that deferred requirements for 'wave two' and 'wave three' companies. A broader ESRS revision is underway, aiming for simplification and consistency by 2027.\n",
      "\n",
      "🤖 Generated Answer (Model 1):\n",
      "The European Commission introduced targeted \"quick fix\" amendments to the first set of European Sustainability Reporting Standards (ESRS). These changes aim to reduce the burden and increase certainty for wave one companies that had to start reporting for financial year 2024. According to the current ESRS, companies reporting on financial year 2024 can omit information on, amongst other things, the anticipated financial effects of certain sustainability-related risks. The \"quick fix\" amendment, which applies from financial year 2025, allows wave one companies to omit that same information for financial years 2025 and 2026. This means wave one companies will not have to report additional information compared to financial year 2024. Moreover, for financial years 2025 and 2026, wave one companies with more than 750 employees will benefit from most of the same phase-in provisions that currently apply to companies with up to 750 employees. These changes were necessary because wave one companies were not captured by the \"stop-the-clock\" Directive, which delayed by two years the sustainability reporting requirements for companies that report\n",
      "\n",
      "🧠 Gemini Answer (Model 2):\n",
      "The European Commission's \"quick fix\" amendments to the ESRS allow wave one companies (those reporting for financial year 2024) to omit information on the anticipated financial effects of certain sustainability-related risks for financial years 2025 and 2026, maintaining the same reporting requirements as in 2024.  Additionally, for 2025 and 2026, wave one companies with over 750 employees will receive most of the same phase-in provisions currently available to companies with up to 750 employees.  This means these larger wave one companies will face a reduced reporting burden compared to what might have been expected without the amendment.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "🔢 Pair 10\n",
      "❓ Question:\n",
      "What led to the imposition of anti-dumping duties on lysine imports from China, and what are the expected effects on the EU industry and environmental sustainability?\n",
      "\n",
      "✅ Reference Answer (Original ChatGPT):\n",
      "The EU imposed anti-dumping duties on lysine imports from China after an investigation revealed that these imports were being sold at unfairly low prices, harming European producers. Lysine, a critical amino acid used in animal feed, pharmaceuticals, and dietary supplements, plays an essential role in improving animal nutrition and reducing environmental impacts such as nitrogen pollution. The duties are expected to level the playing field for EU lysine manufacturers and promote fair competition while supporting environmentally sustainable agricultural practices in the EU.\n",
      "\n",
      "🤖 Generated Answer (Model 1):\n",
      "The imposition of anti-dumping duties on lysine imports from China was due to an investigation that showed that dumped imports of lysine were harming EU industry. The expected effects on the EU industry and environmental sustainability are that the duties imposed today should help EU lysine makers to compete on a more equal footing with their Chinese counterparts. The measures aim to protect EU producers of lysine from unfair trading practices, which enables farmers to meet their animals’ nutritional requirements more accurately, saving land and reducing nitrogen pollution – such as nitrates and ammonia – on livestock farms.\n",
      "\n",
      "🧠 Gemini Answer (Model 2):\n",
      "The imposition of anti-dumping duties on lysine imports from China stemmed from an investigation revealing that dumped imports were harming the EU lysine industry.  The expected effect is that EU lysine producers will be able to compete more equally with their Chinese counterparts.  Regarding environmental sustainability, the text notes that using synthetic lysine in animal feed allows for more accurate meeting of animals' nutritional needs, thus saving land and reducing nitrogen pollution (nitrates and ammonia) on livestock farms.  The anti-dumping duties are therefore indirectly expected to support this environmental benefit by protecting the EU lysine industry.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i, (original, model3,gemini) in enumerate(zip(data,model_qa_v3 , gemini_qa)):\n",
    "    print(f\"🔢 Pair {i+1}\")\n",
    "    print(f\"❓ Question:\\n{original['question']}\\n\")\n",
    "    print(f\"✅ Reference Answer (Original ChatGPT):\\n{original['answer']}\\n\")\n",
    "    print(f\"🤖 Generated Answer (Model 1):\\n{model3['answer']}\\n\")\n",
    "    print(f\"🧠 Gemini Answer (Model 2):\\n{gemini['answer']}\")\n",
    "    # print(f\"🧠 Generated Answer (Model 3):\\n{model3['answer']}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    if i == 9:\n",
    "        break  # Show only the top 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "74a97207-7f3f-43f2-b25c-2c62e6951699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  8.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Average BLEU Score (1-gram): 0.3110\n",
      "🔹 Average Semantic Similarity: 0.8437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# subset_data = data[:10] \n",
    "# subset_model_qa_v3 = model_qa_v3[:10]\n",
    "\n",
    "assert len(subset_data) == len(gemini_qa), \"Mismatch in number of QA pairs\"\n",
    "\n",
    "bleu_scores = []\n",
    "similarity_scores = []\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "for gpt_item, my_item in tqdm.tqdm(zip(subset_data, gemini_qa), total=10):\n",
    "    reference = gpt_item['answer'].strip()\n",
    "    hypothesis = my_item['answer'].strip()\n",
    "\n",
    "    # BLEU\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    bleu = sentence_bleu([reference.split()], hypothesis.split(), weights=(1, 0, 0, 0), smoothing_function=smoothie)\n",
    "    bleu_scores.append(bleu)\n",
    "\n",
    "    # Semantic similarity\n",
    "    emb_ref = model.encode(reference, convert_to_tensor=True)\n",
    "    emb_hyp = model.encode(hypothesis, convert_to_tensor=True)\n",
    "    sim_score = util.pytorch_cos_sim(emb_ref, emb_hyp).item()\n",
    "    similarity_scores.append(sim_score)\n",
    "\n",
    "# Averages\n",
    "average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "average_similarity = sum(similarity_scores) / len(similarity_scores)\n",
    "\n",
    "print(f\"🔹 Average BLEU Score (1-gram): {average_bleu:.4f}\")\n",
    "print(f\"🔹 Average Semantic Similarity: {average_similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c9f80705-1426-4515-85e9-5d38a515fbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 29.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Average METEOR: 0.3483\n",
      "🔹 Average ROUGE-1: 0.4587\n",
      "🔹 Average ROUGE-2: 0.1757\n",
      "🔹 Average ROUGE-L: 0.3038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "meteor_scores = []\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougeL_scores = []\n",
    "\n",
    "for gpt, mine in tqdm.tqdm(zip(subset_data, gemini_qa), total=len(subset_data)):\n",
    "    ref = gpt[\"answer\"].strip()\n",
    "    hyp = mine[\"answer\"].strip()\n",
    "\n",
    "    # METEOR\n",
    "    meteor = single_meteor_score(word_tokenize(ref), word_tokenize(hyp))\n",
    "    meteor_scores.append(meteor)\n",
    "\n",
    "    # ROUGE\n",
    "    scores = rouge.score(ref, hyp)\n",
    "    rouge1_scores.append(scores[\"rouge1\"].fmeasure)\n",
    "    rouge2_scores.append(scores[\"rouge2\"].fmeasure)\n",
    "    rougeL_scores.append(scores[\"rougeL\"].fmeasure)\n",
    "\n",
    "print(\"🔹 Average METEOR:\", round(sum(meteor_scores) / len(meteor_scores), 4))\n",
    "print(\"🔹 Average ROUGE-1:\", round(sum(rouge1_scores) / len(rouge1_scores), 4))\n",
    "print(\"🔹 Average ROUGE-2:\", round(sum(rouge2_scores) / len(rouge2_scores), 4))\n",
    "print(\"🔹 Average ROUGE-L:\", round(sum(rougeL_scores) / len(rougeL_scores), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14675378-2c87-4185-a72e-5d260ae39ffb",
   "metadata": {},
   "source": [
    "Let's try with our second prompt for better reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "640a7803-2c9e-4c53-b003-c09eea99896e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▍                                                                               | 9/297 [00:22<12:00,  2.50s/it]\n"
     ]
    }
   ],
   "source": [
    "gemini_qa_prompt2 = []\n",
    "i = 0\n",
    "for item in tqdm.tqdm(data):\n",
    "    query = item[\"question\"]\n",
    "    # Step 1: Get relevant chunks\n",
    "    top_chunks = retrieve_relevant_chunks_cosine(query,model_2,index_3,all_chunks_3, top_k=5)\n",
    "\n",
    "    # Step 2: Build prompt\n",
    "    prompt = build_prompt_2(query, top_chunks)\n",
    "\n",
    "    response = gemini_model.generate_content(prompt)\n",
    "    answer = response.text\n",
    "\n",
    "    # Step 5: Save updated QA\n",
    "    gemini_qa_prompt2.append({\n",
    "        \"question\": query,\n",
    "        \"answer\": answer\n",
    "    })\n",
    "    i+=1\n",
    "    if(i>9):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "422264a8-77e0-4b99-8fe5-452492c91f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 13.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Average BLEU Score (1-gram): 0.2719\n",
      "🔹 Average Semantic Similarity: 0.7523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "assert len(subset_data) == len(gemini_qa_prompt2), \"Mismatch in number of QA pairs\"\n",
    "\n",
    "bleu_scores = []\n",
    "similarity_scores = []\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "for gpt_item, my_item in tqdm.tqdm(zip(subset_data, gemini_qa_prompt2), total=10):\n",
    "    reference = gpt_item['answer'].strip()\n",
    "    hypothesis = my_item['answer'].strip()\n",
    "\n",
    "    # BLEU\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    bleu = sentence_bleu([reference.split()], hypothesis.split(), weights=(1, 0, 0, 0), smoothing_function=smoothie)\n",
    "    bleu_scores.append(bleu)\n",
    "\n",
    "    # Semantic similarity\n",
    "    emb_ref = model.encode(reference, convert_to_tensor=True)\n",
    "    emb_hyp = model.encode(hypothesis, convert_to_tensor=True)\n",
    "    sim_score = util.pytorch_cos_sim(emb_ref, emb_hyp).item()\n",
    "    similarity_scores.append(sim_score)\n",
    "\n",
    "# Averages\n",
    "average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "average_similarity = sum(similarity_scores) / len(similarity_scores)\n",
    "\n",
    "print(f\"🔹 Average BLEU Score (1-gram): {average_bleu:.4f}\")\n",
    "print(f\"🔹 Average Semantic Similarity: {average_similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e2d5f684-e8ec-488e-8f34-8f336f0d86da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 42.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Average METEOR: 0.2816\n",
      "🔹 Average ROUGE-1: 0.4031\n",
      "🔹 Average ROUGE-2: 0.1343\n",
      "🔹 Average ROUGE-L: 0.252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "meteor_scores = []\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougeL_scores = []\n",
    "\n",
    "for gpt, mine in tqdm.tqdm(zip(subset_data, gemini_qa_prompt2), total=len(subset_data)):\n",
    "    ref = gpt[\"answer\"].strip()\n",
    "    hyp = mine[\"answer\"].strip()\n",
    "\n",
    "    # METEOR\n",
    "    meteor = single_meteor_score(word_tokenize(ref), word_tokenize(hyp))\n",
    "    meteor_scores.append(meteor)\n",
    "\n",
    "    # ROUGE\n",
    "    scores = rouge.score(ref, hyp)\n",
    "    rouge1_scores.append(scores[\"rouge1\"].fmeasure)\n",
    "    rouge2_scores.append(scores[\"rouge2\"].fmeasure)\n",
    "    rougeL_scores.append(scores[\"rougeL\"].fmeasure)\n",
    "\n",
    "print(\"🔹 Average METEOR:\", round(sum(meteor_scores) / len(meteor_scores), 4))\n",
    "print(\"🔹 Average ROUGE-1:\", round(sum(rouge1_scores) / len(rouge1_scores), 4))\n",
    "print(\"🔹 Average ROUGE-2:\", round(sum(rouge2_scores) / len(rouge2_scores), 4))\n",
    "print(\"🔹 Average ROUGE-L:\", round(sum(rougeL_scores) / len(rougeL_scores), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2914fe95-6295-406c-81d7-2939c19dcd7c",
   "metadata": {},
   "source": [
    "When I change the prompt to strict prompt, the performance matrix drop instantly. Let's read few answers side by side for human comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d46ef6e7-7c98-488a-bd54-9aee8e5b15db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔢 Pair 1\n",
      "❓ Question:\n",
      "What collaborative initiative was announced by the European Microfinance Network (EMN) and the Microfinance Centre (MFC) in April 2024, and what are its main objectives for supporting the European microfinance sector?\n",
      "\n",
      "✅ Reference Answer (Original ChatGPT):\n",
      "In April 2024, the European Microfinance Network (EMN) and the Microfinance Centre (MFC) announced a joint strategy aimed at strengthening the European microfinance sector. This collaborative initiative focuses on promoting financial inclusion, developing capacity-building resources, and creating a unified voice to influence policy-making at the European level. The partnership aims to better support microfinance institutions and expand access to responsible finance for underserved populations across Europe.\n",
      "\n",
      "🤖 Generated Answer (Model 1):\n",
      "The European Microfinance Network (EMN) and the Microfinance Centre (MFC) announced a collaborative initiative in April 2024 called \"Microfinance in Europe: Survey Report\". The main objectives of this initiative are to provide valuable insights into the contribution of microfinance institutions to social inclusion, entrepreneurship, and local development, and to serve as an important policy tool supporting evidence-based decision-making for policymakers working to strengthen financial inclusion and the social economy. The initiative also functions as a benchmarking reference for MFIs, helping them evaluate their performance and position within the wider European landscape.\n",
      "\n",
      "🧠 Gemini Answer (Model 2):\n",
      "The provided text announces the 12th edition of the \"Microfinance in Europe: Survey Report,\" a collaborative initiative by the European Microfinance Network (EMN) and the Microfinance Centre (MFC).  This report's main objectives are to:\n",
      "\n",
      "1.  Provide the most comprehensive dataset available on the European microfinance sector, capturing data from the vast majority of European microfinance institutions (MFIs).\n",
      "2.  Offer valuable insights into how MFIs contribute to social inclusion, entrepreneurship, and local development.\n",
      "3.  Serve as a policy tool supporting evidence-based decision-making for policymakers working to strengthen financial inclusion and the social economy.\n",
      "4.  Function as a benchmarking reference for MFIs, helping them evaluate their performance and position within the wider European landscape.  The 2024 edition specifically focuses on the types of businesses reached by microfinance, the social performance of business loans, and the impact measurement approaches adopted by MFIs.\n",
      "\n",
      "🧠 Gemini Answer Prompt 2(Model 3):\n",
      "The provided text mentions the 12th edition of the EMN and MFC's flagship publication, \"Microfinance in Europe: Survey Report (2024 Edition),\"  but doesn't state it was announced in April 2024.  The report's main objectives are to provide the most comprehensive dataset available on the European microfinance sector, focusing on the types of businesses reached, social performance of business loans, and impact measurement approaches.  It also serves as a benchmarking reference for MFIs, a policy tool for policymakers, and offers insights into how microfinance institutions contribute to social inclusion, entrepreneurship, and local development.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "🔢 Pair 2\n",
      "❓ Question:\n",
      "How did the AccessibleEU initiative exceed its yearly objectives in promoting accessibility for persons with disabilities across Europe?\n",
      "\n",
      "✅ Reference Answer (Original ChatGPT):\n",
      "The AccessibleEU initiative surpassed its yearly objectives by significantly advancing accessibility measures across European Union member states. It achieved this through extensive stakeholder engagement, implementation of accessibility audits, development of best practice toolkits, and organization of awareness campaigns. The initiative focused on inclusive design, policy harmonization, and improved accessibility in public infrastructure, thereby contributing to a more inclusive environment for persons with disabilities throughout Europe.\n",
      "\n",
      "🤖 Generated Answer (Model 1):\n",
      "The AccessibleEU initiative exceeded its yearly objectives in promoting accessibility for persons with disabilities across Europe in several ways. One of the key achievements was the refinement and implementation of the set of indicators monitoring accessibility initiatives across all Member States, which helped to collect a large amount of information on commitments and efforts made by each Member State to implement accessibility legislation and standards. Additionally, the initiative successfully hosted 88 events, including two online courses, grew the Community of Practiceto over 3,400 members, established aMoodle forum, and kept building its digital library with 163 new Good Practices and 136 new references on accessibility. Furthermore, the inititative held a high-level event in Brussels on 3rdJuly, bringing together European institutions, national governments, leading companies, disability organisations and accessibility experts to analyse the European Accessibility Act and emphasise the need to comply with it. All these achievements demonstrate the initiative's success in promoting accessibility for persons with disabilities across Europe.\n",
      "\n",
      "🧠 Gemini Answer (Model 2):\n",
      "AccessibleEU surpassed its 2024 objectives by:\n",
      "\n",
      "* **Bringing together stakeholders:** Successfully connecting policymakers, experts, and persons with disabilities.\n",
      "* **Providing guidance:** Offering training, awareness-raising, and technical publications to implement accessibility standards.\n",
      "* **Refining and implementing monitoring indicators:**  Collecting extensive data on Member States' commitments and efforts to implement accessibility legislation and standards, accessible via the AccessibleEU - Accessibility Monitoring website.\n",
      "* **Extensive outreach and community building:** Hosting 88 events (including two online courses), growing its Community of Practice to over 3,400 members, establishing a Moodle forum, and expanding its digital library with 163 new Good Practices and 136 new references on accessibility.  \n",
      "* **High-level event in Brussels:**  Analyzing the European Accessibility Act and emphasizing compliance, bringing together European institutions, national governments, companies, disability organizations, and accessibility experts.  The event covered various sectors including ICT, transport, digital services, telecommunications, public procurement, and the built environment.\n",
      "\n",
      "🧠 Gemini Answer Prompt 2(Model 3):\n",
      "The AccessibleEU initiative surpassed its yearly objectives by bringing together policymakers, experts, and persons with disabilities; providing guidance on implementing accessibility standards through training, awareness-raising, and technical publications; refining and implementing indicators monitoring accessibility initiatives across all Member States; collecting information on Member State commitments and efforts to implement accessibility legislation and standards; hosting 88 events (including two online courses); growing its Community of Practice to over 3,400 members; establishing a Moodle forum; and building its digital library with 163 new Good Practices and 136 new references on accessibility.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "🔢 Pair 3\n",
      "❓ Question:\n",
      "What key policy development took place on July 3, 2025, involving the European Commission and its implications for AI regulation in the EU?\n",
      "\n",
      "✅ Reference Answer (Original ChatGPT):\n",
      "On July 3, 2025, the European Commission and the European Parliament reached a political agreement on the AI Act, a landmark piece of legislation aimed at regulating artificial intelligence technologies across the EU. This act sets out a risk-based framework, classifying AI systems by their potential impact on fundamental rights and safety. It introduces strict obligations for high-risk AI applications, including requirements for transparency, accountability, and human oversight. The policy aims to foster trust in AI while supporting innovation and ensuring compliance with EU values.\n",
      "\n",
      "🤖 Generated Answer (Model 1):\n",
      "It is not specified in the provided context what key policy development took place on July 3, 2025, involving the European Commission and its implications for AI regulation in the EU.\n",
      "\n",
      "🧠 Gemini Answer (Model 2):\n",
      "The provided text does not contain information about any key policy development on July 3, 2025, regarding AI regulation in the EU.  The latest date mentioned is January 24, 2024, when the Commission adopted five initiatives to strengthen the EU's economic security, including a legislative proposal for AI regulation.\n",
      "\n",
      "🧠 Gemini Answer Prompt 2(Model 3):\n",
      "The answer is not found in the provided context.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "🔢 Pair 4\n",
      "❓ Question:\n",
      "What environmentally sustainable measures has Hadag, Hamburg’s ferry service operator, implemented to modernize its fleet and reduce emissions?\n",
      "\n",
      "✅ Reference Answer (Original ChatGPT):\n",
      "Hadag, the ferry service operator in Hamburg, has undertaken a modernization program that includes the introduction of hybrid and fully electric ferries to its fleet. These environmentally sustainable measures are designed to reduce greenhouse gas emissions and improve energy efficiency in public transport. The initiative aligns with Hamburg’s broader climate strategy and emphasizes the shift towards low-emission urban mobility solutions in maritime transportation.\n",
      "\n",
      "🤖 Generated Answer (Model 1):\n",
      "Hadag, Hamburg's ferry service operator, has implemented an AI-powered platform to optimize the scheduling and operation of its electric ferry fleet. The platform addresses key challenges in electric ferry operations, such as battery range and fluctuating energy costs. The initiative supports the city's goal to reduce transport-related emissions significantly. As a member of the Carbon Neutral Cities Alliance, Hamburg aims to cut emissions by 55% by 2030 and by 95% by 2050. Electric ferries play a key role in reaching these targets. The AI system will support daily operations by tailoring charging strategies, managing energy demand, and enhancing communication with passengers via digital displays and mobile apps. With three electric ferries already delivered in 2024 and more on the way, Hadag is on track to operate a fully zero-emissions fleet - demonstrating how advanced technology can drive real climate action in public transport.\n",
      "\n",
      "🧠 Gemini Answer (Model 2):\n",
      "Hadag has implemented an AI-powered platform to optimize the scheduling and operation of its electric ferry fleet.  This system streamlines ferry timetables and charging routines, addresses challenges like battery range and fluctuating energy costs, and improves passenger information.  The initiative is part of Hamburg's broader goal to reduce transport-related emissions, aiming for a 55% reduction by 2030 and 95% by 2050.  Hadag is transitioning to a fully zero-emissions fleet, with three electric ferries already delivered in 2024 and more planned.\n",
      "\n",
      "🧠 Gemini Answer Prompt 2(Model 3):\n",
      "Hadag is adopting an AI-powered platform to optimize the scheduling and operation of its electric ferry fleet.  This system will streamline ferry timetables and charging routines, and improve passenger information.  Three electric ferries were delivered in 2024, with more to come, aiming for a fully zero-emissions fleet.  The AI system will tailor charging strategies, manage energy demand, and enhance communication with passengers.  These actions support Hamburg's goal to significantly reduce transport-related emissions, aiming for a 55% reduction by 2030 and a 95% reduction by 2050.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "🔢 Pair 5\n",
      "❓ Question:\n",
      "What actions did the Israeli government undertake following recent Cabinet resolutions concerning the security situation in the Gaza Strip?\n",
      "\n",
      "✅ Reference Answer (Original ChatGPT):\n",
      "Following recent Cabinet resolutions, the Israeli government implemented heightened security operations targeting militant infrastructure in the Gaza Strip. These actions included airstrikes on weapon manufacturing sites and border surveillance enhancements. The operations were positioned as a response to ongoing threats and aimed at restoring stability and deterring further escalation in the region.\n",
      "\n",
      "🤖 Generated Answer (Model 1):\n",
      "The Israeli government undertook several actions following recent Cabinet resolutions concerning the security situation in the Gaza Strip. These actions include, among other things, a substantial increase of daily trucks for food and non-food items to enter Gaza, the opening of several other crossing points in both the northern and southern areas; the reopening of the Jordanian and Egyptian aid routes; enabling the distribution of food supplies through bakeries and public kitchens throughout the Gaza strip; the resumption of fuel deliveries for use by humanitarian facilities, up to an operational level; the protection of aid workers;  the repair and facilitation of works on vital infrastructure like the resumption of the power supply to the water desalination facility. Additionally, the EU stands ready to coordinate with all relevant humanitarian stakeholders, UN agencies and NGOs on the ground, to ensure swift implementation of those urgent steps.\n",
      "\n",
      "🧠 Gemini Answer (Model 2):\n",
      "Following cabinet resolutions and dialogue with the EU, the Israeli government agreed to several measures to improve the humanitarian situation in Gaza.  These include substantially increasing the daily number of trucks carrying food and non-food items into Gaza, opening additional crossing points in the north and south, reopening Jordanian and Egyptian aid routes, enabling food distribution via bakeries and public kitchens, resuming fuel deliveries for humanitarian facilities, protecting aid workers, and repairing vital infrastructure such as the water desalination plant's power supply.\n",
      "\n",
      "🧠 Gemini Answer Prompt 2(Model 3):\n",
      "Following Cabinet resolutions and dialogue with the EU, Israel agreed to several actions to improve Gaza's humanitarian situation.  These include substantially increasing daily trucks for food and non-food items, opening more crossing points in northern and southern areas, reopening Jordanian and Egyptian aid routes, enabling food distribution through bakeries and public kitchens, resuming fuel deliveries for humanitarian facilities, protecting aid workers, and repairing vital infrastructure like the water desalination plant's power supply.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "🔢 Pair 6\n",
      "❓ Question:\n",
      "What is the purpose of the EU Flight Emissions Label (FEL) initiative involving the European Union Aviation Safety Agency (EASA) and Air France-KLM, and how does it aim to enhance transparency and sustainability in European aviation?\n",
      "\n",
      "✅ Reference Answer (Original ChatGPT):\n",
      "The EU Flight Emissions Label (FEL) initiative is a collaborative effort between the European Union Aviation Safety Agency (EASA) and Air France-KLM aimed at enhancing transparency in flight emissions reporting. Under a Memorandum of Cooperation signed in 2024, Air France and KLM will pilot the FEL scheme to provide reliable, harmonized data on flight emissions based on factors like aircraft type, passenger load, freight volume, and fuel type. The goal is to allow passengers to make more informed choices, protect them from greenwashing, and promote the adoption of sustainable aviation fuels, thereby reinforcing decarbonization and fair competition within the EU aviation market.\n",
      "\n",
      "🤖 Generated Answer (Model 1):\n",
      "The purpose of the EU Flight Emissions Label (FEL) initiative involving the European Union Aviation Safety Agency (EASA) and Air France-KLM is to enhance transparency and sustainability in European aviation by providing a trustworthy framework for airlines to share flight emissions data with passengers. The initiative aims to provide a reliable and harmonised methodology for estimating flight emissions, considering operational factors like aircraft type, passenger numbers, and freight volume on board, as well as the amount and type of aviation fuels uplifted per airport. This will help passengers make more informed decisions when comparing and booking flights among competitors and protect them from misleading greenwashing claims. Ultimately, this initiative will strengthen consumer protection laws within the EU. The agreement marks a major advance in making the Flight Emissions Label more widely available, setting out a reliable and harmonised methodology for estimating flight emissions.\n",
      "\n",
      "🧠 Gemini Answer (Model 2):\n",
      "The EU Flight Emissions Label (FEL) initiative, involving EASA and Air France-KLM, aims to create a trustworthy framework for airlines to share flight emissions data with passengers.  This enhances transparency by allowing passengers to see the emissions associated with their flights, enabling more informed decisions when booking and comparing flights.  It also combats misleading greenwashing claims.  The initiative promotes sustainability by accelerating the uptake of sustainable aviation fuels (SAFs) and other emissions offsets, as airlines can use the emission estimates to offer these options to passengers.  The voluntary participation of airlines, initially exemplified by Air France-KLM, allows for a pre-assessment of emission values and testing of digital tools, ultimately contributing to a more sustainable and transparent future for European aviation.  Furthermore,  FEL provides a harmonized methodology for estimating flight emissions, considering factors like aircraft type, passenger numbers, freight volume, and fuel type.\n",
      "\n",
      "🧠 Gemini Answer Prompt 2(Model 3):\n",
      "The EU Flight Emissions Label (FEL) provides a trustworthy framework for airlines to share flight emissions data with passengers.  It aims to pre-assess emission values, estimate implementation efforts, and test digital tools like the EASA Sustainability Portal.  Airlines can voluntarily join FEL, and only those who do can display emissions to passengers in the EU (Article 6(5) of the Regulation).  The initiative promotes transparency by allowing passengers to make informed decisions when comparing flights and prevents misleading greenwashing.  It also helps accelerate the uptake of sustainable aviation fuels to reduce emissions, strengthening consumer protection laws within the EU.  The collaboration with Air France-KLM, the first to join, is a key milestone in the rollout of FEL and ReFuelEU Aviation.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "🔢 Pair 7\n",
      "❓ Question:\n",
      "What were the key findings and contributions of the two-year research initiative on circular economy monitoring, and how many indicators were ultimately selected and tested?\n",
      "\n",
      "✅ Reference Answer (Original ChatGPT):\n",
      "The two-year research initiative on circular economy monitoring resulted in a comprehensive set of options for tracking circularity across multiple sectors, including batteries, vehicles, electronics, bioeconomy, food, construction, plastics, and textiles. Out of over 730 potential indicators catalogued, 60 were selected and tested through 19 detailed case studies. These indicators support policy development by offering scalable monitoring tools at regional, national, and international levels, and across households, companies, and industries. The project aimed to strengthen understanding and implementation of circular economy policies and tools to address resource use and climate challenges.\n",
      "\n",
      "🤖 Generated Answer (Model 1):\n",
      "The two-year research initiative on circular economy monitoring concluded by presenting a comprehensive set of options for monitoring circularity across critical policy areas and sectors. The project catalogued over 730 potential indicators, and after taking existing monitoring efforts and focusing on complementary and expanding aspects such as product-service systems, 60 indicators were ultimately selected and tested via 19 detailed case studies. The project advanced our understanding of effective circular economy policies and support tools, laying the foundation for future efforts to monitor and drive the circular economy. It tackles resource use and climate change challenges, fostering the transition to a more sustainable and equitable society.\n",
      "\n",
      "The project's key findings and contributions include:\n",
      "\n",
      "1. Cataloging over 730 potential indicators accessible through the Circular Economy Indicators Tool, which provides a comprehensive resource for monitoring circularity.\n",
      "2. Taking existing monitoring efforts, such as the Circular Economy Monitoring Framework, into consideration and focusing on complementary and expanding aspects such as product-service systems.\n",
      "3. Selecting and testing 60 indicators via 19 detailed case studies, which provides valuable insights into effective circular economy policies and support tools.\n",
      "4.\n",
      "\n",
      "🧠 Gemini Answer (Model 2):\n",
      "The two-year research initiative developed a comprehensive set of indicators for monitoring circularity across various sectors (batteries, vehicles, electronics, bioeconomy, food, construction, plastics, packaging, textiles) and levels (regional, national, international, household, company, sectoral).  It built upon existing frameworks like the Circular Economy Monitoring Framework, adding complementary aspects such as product-service systems.  The project started with over 730 potential indicators, ultimately selecting and testing 60 of them through 19 detailed case studies.  The initiative advances understanding of effective circular economy policies and tools, contributing to a more sustainable and equitable society by addressing resource use and climate change challenges.\n",
      "\n",
      "🧠 Gemini Answer Prompt 2(Model 3):\n",
      "The two-year research initiative developed a comprehensive set of options for monitoring circularity across various policy areas and sectors.  It advanced understanding of effective circular economy policies and support tools, laying the foundation for future monitoring efforts.  The project considered existing monitoring frameworks and expanded on aspects like product-service systems.  Of 730 potential indicators, 60 were selected and tested via 19 detailed case studies.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "🔢 Pair 8\n",
      "❓ Question:\n",
      "Why does the European Union continue to commemorate the genocide in Srebrenica, and what is its stance on historical denial and reconciliation in the Western Balkans?\n",
      "\n",
      "✅ Reference Answer (Original ChatGPT):\n",
      "The European Union commemorates the genocide in Srebrenica to honor the memory of over 8,300 Bosniak men and boys who were killed in July 1995 and to support the survivors and families still affected by the tragedy. The EU emphasizes the importance of preserving historical truth, recognizing its past responsibility in failing to prevent the atrocity, and firmly condemns any denial, distortion, or glorification of war crimes. The EU urges political leaders in Bosnia and Herzegovina and the Western Balkans to acknowledge established facts, support genuine reconciliation, and work towards a peaceful and unified future. It reaffirms its commitment to Bosnia and Herzegovina's path toward EU membership.\n",
      "\n",
      "🤖 Generated Answer (Model 1):\n",
      "The European Union continues to commemorate the genocide in Srebrenica because it stands among the darkest chapters in Europe's collective memory. The Union acknowledges that victims were systematically executed and buried in mass graves within the UN-designated 'safe area' of Srebrenica, and that this was a failure in preventing and stopping the genocide. The Union firmly rejects and condemns any denial, distortion, or minimisation of the Srebrenica genocide, as well as the glorification of war criminals. Political leaders in Bosnia and Herzegovina and across the Western Balkans have a great responsibility in acknowledging the established facts and genuinely honouring the victims. They must actively work towards reconciliation by confronting the roots of hatred that led to these atrocities. This is the only way towards a brighter and prosperous future for the next generation. The European Union stands with the people of Bosnia and Herzegovina, and is fully committed to supporting its country on its path towards EU membership.\n",
      "\n",
      "🧠 Gemini Answer (Model 2):\n",
      "The European Union commemorates the Srebrenica genocide to preserve the truth for future generations, ensuring they understand the systematic execution and burial of victims in a UN-designated safe area during the summer of 1995.  The EU acknowledges its past failure to prevent the genocide and recognizes its responsibility.\n",
      "\n",
      "The EU firmly rejects and condemns any denial, distortion, or minimization of the Srebrenica genocide, as well as the glorification of war criminals.  It believes that political leaders in Bosnia and Herzegovina and the Western Balkans must acknowledge the established facts, honor the victims, and actively work towards reconciliation by addressing the roots of hatred.  This, the EU believes, is the path to a brighter future.\n",
      "\n",
      "🧠 Gemini Answer Prompt 2(Model 3):\n",
      "The European Union commemorates the Srebrenica genocide to preserve the truth for future generations, acknowledging its past responsibility for failing to prevent it.  It firmly rejects and condemns any denial, distortion, or minimization of the genocide, as well as the glorification of war criminals.  The EU believes that political leaders in Bosnia and Herzegovina and the Western Balkans must lead by example in acknowledging established facts, honoring victims, and working towards reconciliation by confronting the roots of hatred.  This is considered the only path to a brighter future.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "🔢 Pair 9\n",
      "❓ Question:\n",
      "What changes were introduced in the European Commission's 'quick fix' amendments to the European Sustainability Reporting Standards (ESRS), and how do these impact wave one companies reporting for financial years 2025 and 2026?\n",
      "\n",
      "✅ Reference Answer (Original ChatGPT):\n",
      "The European Commission introduced 'quick fix' amendments to the European Sustainability Reporting Standards (ESRS) to ease the burden on 'wave one' companies that began reporting in financial year 2024. The amendments allow these companies to omit certain disclosures, such as anticipated financial impacts of sustainability-related risks, not only for 2024 but also for 2025 and 2026. The changes align wave one companies with the phase-in provisions granted to smaller firms and address the fact that they were excluded from the 'stop-the-clock' Directive that deferred requirements for 'wave two' and 'wave three' companies. A broader ESRS revision is underway, aiming for simplification and consistency by 2027.\n",
      "\n",
      "🤖 Generated Answer (Model 1):\n",
      "The European Commission introduced targeted \"quick fix\" amendments to the first set of European Sustainability Reporting Standards (ESRS). These changes aim to reduce the burden and increase certainty for wave one companies that had to start reporting for financial year 2024. According to the current ESRS, companies reporting on financial year 2024 can omit information on, amongst other things, the anticipated financial effects of certain sustainability-related risks. The \"quick fix\" amendment, which applies from financial year 2025, allows wave one companies to omit that same information for financial years 2025 and 2026. This means wave one companies will not have to report additional information compared to financial year 2024. Moreover, for financial years 2025 and 2026, wave one companies with more than 750 employees will benefit from most of the same phase-in provisions that currently apply to companies with up to 750 employees. These changes were necessary because wave one companies were not captured by the \"stop-the-clock\" Directive, which delayed by two years the sustainability reporting requirements for companies that report\n",
      "\n",
      "🧠 Gemini Answer (Model 2):\n",
      "The European Commission's \"quick fix\" amendments to the ESRS allow wave one companies (those reporting for financial year 2024) to omit information on the anticipated financial effects of certain sustainability-related risks for financial years 2025 and 2026, maintaining the same reporting requirements as in 2024.  Additionally, for 2025 and 2026, wave one companies with over 750 employees will receive most of the same phase-in provisions currently available to companies with up to 750 employees.  This means these larger wave one companies will face a reduced reporting burden compared to what might have been expected without the amendment.\n",
      "\n",
      "🧠 Gemini Answer Prompt 2(Model 3):\n",
      "The “quick fix” amendment, applying from financial year 2025, allows wave one companies to omit information on the anticipated financial effects of certain sustainability-related risks for financial years 2025 and 2026—the same as for 2024.  Additionally, for financial years 2025 and 2026, wave one companies with over 750 employees will receive most of the same phase-in provisions currently available to companies with up to 750 employees.  This means wave one companies will not have to report additional information compared to financial year 2024.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "🔢 Pair 10\n",
      "❓ Question:\n",
      "What led to the imposition of anti-dumping duties on lysine imports from China, and what are the expected effects on the EU industry and environmental sustainability?\n",
      "\n",
      "✅ Reference Answer (Original ChatGPT):\n",
      "The EU imposed anti-dumping duties on lysine imports from China after an investigation revealed that these imports were being sold at unfairly low prices, harming European producers. Lysine, a critical amino acid used in animal feed, pharmaceuticals, and dietary supplements, plays an essential role in improving animal nutrition and reducing environmental impacts such as nitrogen pollution. The duties are expected to level the playing field for EU lysine manufacturers and promote fair competition while supporting environmentally sustainable agricultural practices in the EU.\n",
      "\n",
      "🤖 Generated Answer (Model 1):\n",
      "The imposition of anti-dumping duties on lysine imports from China was due to an investigation that showed that dumped imports of lysine were harming EU industry. The expected effects on the EU industry and environmental sustainability are that the duties imposed today should help EU lysine makers to compete on a more equal footing with their Chinese counterparts. The measures aim to protect EU producers of lysine from unfair trading practices, which enables farmers to meet their animals’ nutritional requirements more accurately, saving land and reducing nitrogen pollution – such as nitrates and ammonia – on livestock farms.\n",
      "\n",
      "🧠 Gemini Answer (Model 2):\n",
      "The imposition of anti-dumping duties on lysine imports from China stemmed from an investigation revealing that dumped imports were harming the EU lysine industry.  The expected effect is that EU lysine producers will be able to compete more equally with their Chinese counterparts.  Regarding environmental sustainability, the text notes that using synthetic lysine in animal feed allows for more accurate meeting of animals' nutritional needs, thus saving land and reducing nitrogen pollution (nitrates and ammonia) on livestock farms.  The anti-dumping duties are therefore indirectly expected to support this environmental benefit by protecting the EU lysine industry.\n",
      "\n",
      "🧠 Gemini Answer Prompt 2(Model 3):\n",
      "An investigation showed that dumped imports of lysine from the People’s Republic of China were harming EU industry.  The imposed duties should help EU lysine makers compete on a more equal footing with their Chinese counterparts.  Additionally,  the use of synthetic lysine in animal feed enables farmers to meet their animals’ nutritional requirements more accurately, saving land and reducing nitrogen pollution (nitrates and ammonia) on livestock farms.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i, (original, model3,gemini,gemini_prompt2) in enumerate(zip(data,model_qa_v3 , gemini_qa,gemini_qa_prompt2)):\n",
    "    print(f\"🔢 Pair {i+1}\")\n",
    "    print(f\"❓ Question:\\n{original['question']}\\n\")\n",
    "    print(f\"✅ Reference Answer (Original ChatGPT):\\n{original['answer']}\\n\")\n",
    "    print(f\"🤖 Generated Answer (Model 1):\\n{model3['answer']}\\n\")\n",
    "    print(f\"🧠 Gemini Answer (Model 2):\\n{gemini['answer']}\")\n",
    "    print(f\"🧠 Gemini Answer Prompt 2(Model 3):\\n{gemini_prompt2['answer']}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    if i == 9:\n",
    "        break  # Show only the top 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbff12e-2867-4785-8074-0d5f8b5c01b6",
   "metadata": {},
   "source": [
    "#### Last one the langchain splitting (simplest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "66ea8566-1eda-4a46-8ef0-df6a5e553754",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"all_chunks_flat.pkl\", \"rb\") as f:\n",
    "    langchain = pickle.load(f)\n",
    "\n",
    "all_chunks_langchain = langchain[\"chunks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c36ac385-c8e2-41a3-8c96-48d6fbd671c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_langchain = faiss.read_index(\"chunk_index_langchain.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e9a5dfe3-1ca0-45ce-9c0a-253c56c367b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gemini_qa_langchain = []\n",
    "# i = 0\n",
    "# for item in tqdm.tqdm(data):\n",
    "#     query = item[\"question\"]\n",
    "#     # Step 1: Get relevant chunks\n",
    "#     top_chunks = retrieve_relevant_chunks_cosine(query,model_2,index_langchain ,all_chunks_langchain, top_k=5)\n",
    "\n",
    "#     # Step 2: Build prompt\n",
    "#     prompt = build_prompt_2(query, top_chunks)\n",
    "\n",
    "#     response = gemini_model.generate_content(prompt)\n",
    "#     answer = response.text\n",
    "\n",
    "#     # Step 5: Save updated QA\n",
    "#     gemini_qa_langchain.append({\n",
    "#         \"question\": query,\n",
    "#         \"answer\": answer\n",
    "#     })\n",
    "#     i+=1\n",
    "#     if(i>9):\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7ed003-84db-418a-bc14-c1c9e31d1d09",
   "metadata": {},
   "source": [
    "The quota reached its limit lets use the old LLM to geenrate the ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6a7960c9-ac35-4a38-86b0-247c7fb7d6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/297 [00:00<?, ?it/s]Llama.generate: 11 prefix-match hit, remaining 487 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time = 8677788.02 ms /   488 tokens (17782.35 ms per token,     0.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2213.93 ms /    11 runs   (  201.27 ms per token,     4.97 tokens per second)\n",
      "llama_perf_context_print:       total time =   42363.90 ms /   499 tokens\n",
      "  0%|▎                                                                               | 1/297 [00:42<3:29:37, 42.49s/it]Llama.generate: 55 prefix-match hit, remaining 501 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   40200.30 ms /   501 tokens (   80.24 ms per token,    12.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =   41535.63 ms /   187 runs   (  222.12 ms per token,     4.50 tokens per second)\n",
      "llama_perf_context_print:       total time =   81889.05 ms /   688 tokens\n",
      "  1%|▌                                                                               | 2/297 [02:04<5:23:09, 65.73s/it]Llama.generate: 55 prefix-match hit, remaining 477 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   37193.86 ms /   477 tokens (   77.97 ms per token,    12.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =   11139.83 ms /    54 runs   (  206.29 ms per token,     4.85 tokens per second)\n",
      "llama_perf_context_print:       total time =   48363.59 ms /   531 tokens\n",
      "  1%|▊                                                                               | 3/297 [02:52<4:43:26, 57.85s/it]Llama.generate: 55 prefix-match hit, remaining 478 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   37539.46 ms /   478 tokens (   78.53 ms per token,    12.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =   32066.50 ms /   161 runs   (  199.17 ms per token,     5.02 tokens per second)\n",
      "llama_perf_context_print:       total time =   69714.52 ms /   639 tokens\n",
      "  1%|█                                                                               | 4/297 [04:02<5:05:34, 62.57s/it]Llama.generate: 55 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   35534.50 ms /   454 tokens (   78.27 ms per token,    12.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =   38811.49 ms /   191 runs   (  203.20 ms per token,     4.92 tokens per second)\n",
      "llama_perf_context_print:       total time =   74482.63 ms /   645 tokens\n",
      "  2%|█▎                                                                              | 5/297 [05:17<5:25:36, 66.90s/it]Llama.generate: 55 prefix-match hit, remaining 560 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   44998.33 ms /   560 tokens (   80.35 ms per token,    12.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =   49146.68 ms /   241 runs   (  203.93 ms per token,     4.90 tokens per second)\n",
      "llama_perf_context_print:       total time =   94331.28 ms /   801 tokens\n",
      "  2%|█▌                                                                              | 6/297 [06:51<6:09:54, 76.27s/it]Llama.generate: 55 prefix-match hit, remaining 318 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   24398.49 ms /   318 tokens (   76.72 ms per token,    13.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =   17718.56 ms /    91 runs   (  194.71 ms per token,     5.14 tokens per second)\n",
      "llama_perf_context_print:       total time =   42165.57 ms /   409 tokens\n",
      "  2%|█▉                                                                              | 7/297 [07:34<5:15:05, 65.19s/it]Llama.generate: 55 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   35852.48 ms /   455 tokens (   78.80 ms per token,    12.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =   37994.42 ms /   195 runs   (  194.84 ms per token,     5.13 tokens per second)\n",
      "llama_perf_context_print:       total time =   73976.10 ms /   650 tokens\n",
      "  3%|██▏                                                                             | 8/297 [08:48<5:27:37, 68.02s/it]Llama.generate: 55 prefix-match hit, remaining 520 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   40991.72 ms /   520 tokens (   78.83 ms per token,    12.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =   27314.57 ms /   128 runs   (  213.40 ms per token,     4.69 tokens per second)\n",
      "llama_perf_context_print:       total time =   68385.58 ms /   648 tokens\n",
      "  3%|██▍                                                                             | 9/297 [09:56<5:27:13, 68.17s/it]Llama.generate: 55 prefix-match hit, remaining 427 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   33370.37 ms /   427 tokens (   78.15 ms per token,    12.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =   47918.16 ms /   231 runs   (  207.44 ms per token,     4.82 tokens per second)\n",
      "llama_perf_context_print:       total time =   81459.59 ms /   658 tokens\n",
      "  3%|██▍                                                                             | 9/297 [11:18<6:01:46, 75.37s/it]\n"
     ]
    }
   ],
   "source": [
    "mistral_qa_langchain = []\n",
    "i = 0\n",
    "for item in tqdm.tqdm(data):\n",
    "    query = item[\"question\"]\n",
    "    # Step 1: Get relevant chunks\n",
    "    top_chunks = retrieve_relevant_chunks_cosine(query,model_2,index_langchain ,all_chunks_langchain, top_k=5)\n",
    "\n",
    "    # Step 2: Build prompt\n",
    "    prompt = build_prompt_2(query, top_chunks)\n",
    "\n",
    "    # Step 3: Token count (optional)\n",
    "    # token_ids = llm.tokenize(prompt.encode(\"utf-8\"))\n",
    "    # print(f\"Token count for question: '{query[:50]}...':\", len(token_ids))\n",
    "\n",
    "    # Step 4: Run inference\n",
    "    response = llm(prompt, max_tokens=256, temperature=0.7, stop=[\"###\"])\n",
    "    answer = response[\"choices\"][0][\"text\"].strip()\n",
    "    # Step 5: Save updated QA\n",
    "    \n",
    "    mistral_qa_langchain.append({\n",
    "        \"question\": query,\n",
    "        \"answer\": answer\n",
    "    })\n",
    "    i+=1\n",
    "    if(i>9):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ea96e8-27e2-4bec-9431-b0bc60cb95d1",
   "metadata": {},
   "source": [
    "Let's fetch 50 in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fa792784-4c7b-4f36-aa28-05e7c0fcfc6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/287 [00:00<?, ?it/s]Llama.generate: 55 prefix-match hit, remaining 617 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   50439.35 ms /   617 tokens (   81.75 ms per token,    12.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =   41016.11 ms /   189 runs   (  217.02 ms per token,     4.61 tokens per second)\n",
      "llama_perf_context_print:       total time =   91568.93 ms /   806 tokens\n",
      "  0%|▎                                                                               | 1/287 [01:31<7:17:00, 91.68s/it]Llama.generate: 55 prefix-match hit, remaining 1301 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =  125302.18 ms /  1301 tokens (   96.31 ms per token,    10.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =   27801.35 ms /   118 runs   (  235.60 ms per token,     4.24 tokens per second)\n",
      "llama_perf_context_print:       total time =  153165.34 ms /  1419 tokens\n",
      "  1%|▌                                                                             | 2/287 [04:04<10:07:32, 127.90s/it]Llama.generate: 55 prefix-match hit, remaining 542 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   53477.97 ms /   542 tokens (   98.67 ms per token,    10.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5936.89 ms /    30 runs   (  197.90 ms per token,     5.05 tokens per second)\n",
      "llama_perf_context_print:       total time =   59428.55 ms /   572 tokens\n",
      "  1%|▊                                                                               | 3/287 [05:04<7:37:37, 96.68s/it]Llama.generate: 55 prefix-match hit, remaining 482 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   42399.78 ms /   482 tokens (   87.97 ms per token,    11.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =   28007.85 ms /   150 runs   (  186.72 ms per token,     5.36 tokens per second)\n",
      "llama_perf_context_print:       total time =   70493.31 ms /   632 tokens\n",
      "  1%|█                                                                               | 4/287 [06:15<6:47:23, 86.37s/it]Llama.generate: 55 prefix-match hit, remaining 488 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   40957.15 ms /   488 tokens (   83.93 ms per token,    11.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =   23666.39 ms /   125 runs   (  189.33 ms per token,     5.28 tokens per second)\n",
      "llama_perf_context_print:       total time =   64687.84 ms /   613 tokens\n",
      "  2%|█▍                                                                              | 5/287 [07:19<6:09:20, 78.58s/it]Llama.generate: 55 prefix-match hit, remaining 531 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   45355.84 ms /   531 tokens (   85.42 ms per token,    11.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =   28171.00 ms /   133 runs   (  211.81 ms per token,     4.72 tokens per second)\n",
      "llama_perf_context_print:       total time =   73611.93 ms /   664 tokens\n",
      "  2%|█▋                                                                              | 6/287 [08:33<6:00:14, 76.92s/it]Llama.generate: 56 prefix-match hit, remaining 540 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   46976.92 ms /   540 tokens (   86.99 ms per token,    11.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =   34162.60 ms /   137 runs   (  249.36 ms per token,     4.01 tokens per second)\n",
      "llama_perf_context_print:       total time =   81241.77 ms /   677 tokens\n",
      "  2%|█▉                                                                              | 7/287 [09:54<6:05:40, 78.36s/it]Llama.generate: 56 prefix-match hit, remaining 502 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   40475.75 ms /   502 tokens (   80.63 ms per token,    12.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =   67961.23 ms /   255 runs   (  266.51 ms per token,     3.75 tokens per second)\n",
      "llama_perf_context_print:       total time =  108697.05 ms /   757 tokens\n",
      "  3%|██▏                                                                             | 8/287 [11:43<6:49:25, 88.05s/it]Llama.generate: 55 prefix-match hit, remaining 568 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   45433.96 ms /   568 tokens (   79.99 ms per token,    12.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =   66493.44 ms /   255 runs   (  260.76 ms per token,     3.83 tokens per second)\n",
      "llama_perf_context_print:       total time =  112187.45 ms /   823 tokens\n",
      "  3%|██▌                                                                             | 9/287 [13:35<7:23:03, 95.62s/it]Llama.generate: 55 prefix-match hit, remaining 486 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   38467.58 ms /   486 tokens (   79.15 ms per token,    12.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =   37845.40 ms /   146 runs   (  259.22 ms per token,     3.86 tokens per second)\n",
      "llama_perf_context_print:       total time =   76425.48 ms /   632 tokens\n",
      "  3%|██▊                                                                            | 10/287 [14:52<6:54:12, 89.72s/it]Llama.generate: 55 prefix-match hit, remaining 487 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   39537.99 ms /   487 tokens (   81.19 ms per token,    12.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =   36429.06 ms /   140 runs   (  260.21 ms per token,     3.84 tokens per second)\n",
      "llama_perf_context_print:       total time =   76075.43 ms /   627 tokens\n",
      "  4%|███                                                                            | 11/287 [16:08<6:33:37, 85.57s/it]Llama.generate: 55 prefix-match hit, remaining 493 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   39525.15 ms /   493 tokens (   80.17 ms per token,    12.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =   47858.86 ms /   177 runs   (  270.39 ms per token,     3.70 tokens per second)\n",
      "llama_perf_context_print:       total time =   87549.07 ms /   670 tokens\n",
      "  4%|███▎                                                                           | 12/287 [17:36<6:35:05, 86.20s/it]Llama.generate: 56 prefix-match hit, remaining 504 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   40090.40 ms /   504 tokens (   79.54 ms per token,    12.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =   49662.90 ms /   193 runs   (  257.32 ms per token,     3.89 tokens per second)\n",
      "llama_perf_context_print:       total time =   89925.70 ms /   697 tokens\n",
      "  5%|███▌                                                                           | 13/287 [19:06<6:38:55, 87.35s/it]Llama.generate: 55 prefix-match hit, remaining 426 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   33692.41 ms /   426 tokens (   79.09 ms per token,    12.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =   39677.52 ms /   155 runs   (  255.98 ms per token,     3.91 tokens per second)\n",
      "llama_perf_context_print:       total time =   73491.30 ms /   581 tokens\n",
      "  5%|███▊                                                                           | 14/287 [20:19<6:18:32, 83.19s/it]Llama.generate: 55 prefix-match hit, remaining 273 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   21374.16 ms /   273 tokens (   78.29 ms per token,    12.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =   23656.28 ms /    98 runs   (  241.39 ms per token,     4.14 tokens per second)\n",
      "llama_perf_context_print:       total time =   45091.23 ms /   371 tokens\n",
      "  5%|████▏                                                                          | 15/287 [21:04<5:25:10, 71.73s/it]Llama.generate: 55 prefix-match hit, remaining 362 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   29074.48 ms /   362 tokens (   80.32 ms per token,    12.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =   23475.19 ms /    94 runs   (  249.74 ms per token,     4.00 tokens per second)\n",
      "llama_perf_context_print:       total time =   52609.59 ms /   456 tokens\n",
      "  6%|████▍                                                                          | 16/287 [21:57<4:58:05, 66.00s/it]Llama.generate: 55 prefix-match hit, remaining 500 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   40677.55 ms /   500 tokens (   81.36 ms per token,    12.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =   44879.72 ms /   167 runs   (  268.74 ms per token,     3.72 tokens per second)\n",
      "llama_perf_context_print:       total time =   85705.95 ms /   667 tokens\n",
      "  6%|████▋                                                                          | 17/287 [23:23<5:23:47, 71.95s/it]Llama.generate: 55 prefix-match hit, remaining 392 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   30683.22 ms /   392 tokens (   78.27 ms per token,    12.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =   26242.20 ms /   101 runs   (  259.82 ms per token,     3.85 tokens per second)\n",
      "llama_perf_context_print:       total time =   56993.74 ms /   493 tokens\n",
      "  6%|████▉                                                                          | 18/287 [24:20<5:02:33, 67.48s/it]Llama.generate: 55 prefix-match hit, remaining 412 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   33306.44 ms /   412 tokens (   80.84 ms per token,    12.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2597.51 ms /    11 runs   (  236.14 ms per token,     4.23 tokens per second)\n",
      "llama_perf_context_print:       total time =   35910.13 ms /   423 tokens\n",
      "  7%|█████▏                                                                         | 19/287 [24:56<4:19:11, 58.03s/it]Llama.generate: 141 prefix-match hit, remaining 256 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   21420.58 ms /   256 tokens (   83.67 ms per token,    11.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =   36529.14 ms /   134 runs   (  272.61 ms per token,     3.67 tokens per second)\n",
      "llama_perf_context_print:       total time =   58057.62 ms /   390 tokens\n",
      "  7%|█████▌                                                                         | 20/287 [25:54<4:18:23, 58.07s/it]Llama.generate: 55 prefix-match hit, remaining 590 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   48150.89 ms /   590 tokens (   81.61 ms per token,    12.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =   28488.08 ms /   108 runs   (  263.78 ms per token,     3.79 tokens per second)\n",
      "llama_perf_context_print:       total time =   76710.68 ms /   698 tokens\n",
      "  7%|█████▊                                                                         | 21/287 [27:11<4:42:21, 63.69s/it]Llama.generate: 55 prefix-match hit, remaining 492 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   39501.15 ms /   492 tokens (   80.29 ms per token,    12.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =   38550.40 ms /   139 runs   (  277.34 ms per token,     3.61 tokens per second)\n",
      "llama_perf_context_print:       total time =   78162.66 ms /   631 tokens\n",
      "  8%|██████                                                                         | 22/287 [28:29<5:00:35, 68.06s/it]Llama.generate: 55 prefix-match hit, remaining 456 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   35911.03 ms /   456 tokens (   78.75 ms per token,    12.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =   66163.26 ms /   255 runs   (  259.46 ms per token,     3.85 tokens per second)\n",
      "llama_perf_context_print:       total time =  102342.53 ms /   711 tokens\n",
      "  8%|██████▎                                                                        | 23/287 [30:12<5:44:49, 78.37s/it]Llama.generate: 55 prefix-match hit, remaining 535 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   42631.87 ms /   535 tokens (   79.69 ms per token,    12.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =   67623.52 ms /   255 runs   (  265.19 ms per token,     3.77 tokens per second)\n",
      "llama_perf_context_print:       total time =  110521.38 ms /   790 tokens\n",
      "  8%|██████▌                                                                        | 24/287 [32:02<6:25:53, 88.03s/it]Llama.generate: 55 prefix-match hit, remaining 437 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   34264.61 ms /   437 tokens (   78.41 ms per token,    12.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =   45327.60 ms /   171 runs   (  265.07 ms per token,     3.77 tokens per second)\n",
      "llama_perf_context_print:       total time =   79736.43 ms /   608 tokens\n",
      "  9%|██████▉                                                                        | 25/287 [33:22<6:13:39, 85.57s/it]Llama.generate: 55 prefix-match hit, remaining 602 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   49831.17 ms /   602 tokens (   82.78 ms per token,    12.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =   21673.40 ms /    87 runs   (  249.12 ms per token,     4.01 tokens per second)\n",
      "llama_perf_context_print:       total time =   71559.19 ms /   689 tokens\n",
      "  9%|███████▏                                                                       | 26/287 [34:34<5:54:01, 81.38s/it]Llama.generate: 55 prefix-match hit, remaining 514 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   42687.33 ms /   514 tokens (   83.05 ms per token,    12.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =   16759.83 ms /    72 runs   (  232.78 ms per token,     4.30 tokens per second)\n",
      "llama_perf_context_print:       total time =   59490.62 ms /   586 tokens\n",
      "  9%|███████▍                                                                       | 27/287 [35:33<5:24:17, 74.83s/it]Llama.generate: 55 prefix-match hit, remaining 515 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   43781.92 ms /   515 tokens (   85.01 ms per token,    11.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =   25562.67 ms /   101 runs   (  253.10 ms per token,     3.95 tokens per second)\n",
      "llama_perf_context_print:       total time =   69413.61 ms /   616 tokens\n",
      " 10%|███████▋                                                                       | 28/287 [36:43<5:16:06, 73.23s/it]Llama.generate: 55 prefix-match hit, remaining 518 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   42231.46 ms /   518 tokens (   81.53 ms per token,    12.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =   17658.92 ms /    70 runs   (  252.27 ms per token,     3.96 tokens per second)\n",
      "llama_perf_context_print:       total time =   59934.73 ms /   588 tokens\n",
      " 10%|███████▉                                                                       | 29/287 [37:43<4:57:50, 69.26s/it]Llama.generate: 55 prefix-match hit, remaining 428 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   34745.30 ms /   428 tokens (   81.18 ms per token,    12.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =   57810.41 ms /   220 runs   (  262.77 ms per token,     3.81 tokens per second)\n",
      "llama_perf_context_print:       total time =   92762.57 ms /   648 tokens\n",
      " 10%|████████▎                                                                      | 30/287 [39:16<5:26:58, 76.34s/it]Llama.generate: 55 prefix-match hit, remaining 493 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   39291.15 ms /   493 tokens (   79.70 ms per token,    12.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =   65035.13 ms /   255 runs   (  255.04 ms per token,     3.92 tokens per second)\n",
      "llama_perf_context_print:       total time =  104567.67 ms /   748 tokens\n",
      " 11%|████████▌                                                                      | 31/287 [41:00<6:01:55, 84.83s/it]Llama.generate: 55 prefix-match hit, remaining 411 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   32362.21 ms /   411 tokens (   78.74 ms per token,    12.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =   52427.52 ms /   198 runs   (  264.79 ms per token,     3.78 tokens per second)\n",
      "llama_perf_context_print:       total time =   84965.08 ms /   609 tokens\n",
      " 11%|████████▊                                                                      | 32/287 [42:25<6:00:47, 84.89s/it]Llama.generate: 55 prefix-match hit, remaining 616 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   50325.97 ms /   616 tokens (   81.70 ms per token,    12.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =   62544.45 ms /   243 runs   (  257.38 ms per token,     3.89 tokens per second)\n",
      "llama_perf_context_print:       total time =  113103.98 ms /   859 tokens\n",
      " 11%|█████████                                                                      | 33/287 [44:18<6:35:17, 93.38s/it]Llama.generate: 55 prefix-match hit, remaining 548 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   43745.27 ms /   548 tokens (   79.83 ms per token,    12.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3162.96 ms /    12 runs   (  263.58 ms per token,     3.79 tokens per second)\n",
      "llama_perf_context_print:       total time =   46915.74 ms /   560 tokens\n",
      " 12%|█████████▎                                                                     | 34/287 [45:05<5:35:05, 79.47s/it]Llama.generate: 55 prefix-match hit, remaining 1371 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =  125230.91 ms /  1371 tokens (   91.34 ms per token,    10.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =   40665.59 ms /   146 runs   (  278.53 ms per token,     3.59 tokens per second)\n",
      "llama_perf_context_print:       total time =  166002.50 ms /  1517 tokens\n",
      " 12%|█████████▌                                                                    | 35/287 [47:51<7:22:52, 105.45s/it]Llama.generate: 55 prefix-match hit, remaining 535 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   44083.78 ms /   535 tokens (   82.40 ms per token,    12.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =   68275.08 ms /   255 runs   (  267.75 ms per token,     3.73 tokens per second)\n",
      "llama_perf_context_print:       total time =  112625.36 ms /   790 tokens\n",
      " 13%|█████████▊                                                                    | 36/287 [49:44<7:30:13, 107.62s/it]Llama.generate: 55 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   35561.81 ms /   454 tokens (   78.33 ms per token,    12.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =   51323.26 ms /   195 runs   (  263.20 ms per token,     3.80 tokens per second)\n",
      "llama_perf_context_print:       total time =   87062.53 ms /   649 tokens\n",
      " 13%|██████████                                                                    | 37/287 [51:11<7:02:49, 101.48s/it]Llama.generate: 55 prefix-match hit, remaining 432 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   34008.94 ms /   432 tokens (   78.72 ms per token,    12.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =   58363.51 ms /   221 runs   (  264.09 ms per token,     3.79 tokens per second)\n",
      "llama_perf_context_print:       total time =   92587.42 ms /   653 tokens\n",
      " 13%|██████████▍                                                                    | 38/287 [52:44<6:50:09, 98.83s/it]Llama.generate: 55 prefix-match hit, remaining 522 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   41363.65 ms /   522 tokens (   79.24 ms per token,    12.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =   40486.85 ms /   153 runs   (  264.62 ms per token,     3.78 tokens per second)\n",
      "llama_perf_context_print:       total time =   81977.36 ms /   675 tokens\n",
      " 14%|██████████▋                                                                    | 39/287 [54:06<6:27:41, 93.80s/it]Llama.generate: 55 prefix-match hit, remaining 354 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   27688.70 ms /   354 tokens (   78.22 ms per token,    12.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =   40220.40 ms /   156 runs   (  257.82 ms per token,     3.88 tokens per second)\n",
      "llama_perf_context_print:       total time =   68032.57 ms /   510 tokens\n",
      " 14%|███████████                                                                    | 40/287 [55:14<5:54:24, 86.09s/it]Llama.generate: 55 prefix-match hit, remaining 542 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   81083.44 ms\n",
      "llama_perf_context_print: prompt eval time =   43229.74 ms /   542 tokens (   79.76 ms per token,    12.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =   50855.46 ms /   193 runs   (  263.50 ms per token,     3.80 tokens per second)\n",
      "llama_perf_context_print:       total time =   94261.54 ms /   735 tokens\n",
      " 14%|███████████                                                                    | 40/287 [56:49<5:50:50, 85.23s/it]\n"
     ]
    }
   ],
   "source": [
    "i = 10\n",
    "for item in tqdm.tqdm(data[10:]):\n",
    "    query = item[\"question\"]\n",
    "    # Step 1: Get relevant chunks\n",
    "    top_chunks = retrieve_relevant_chunks_cosine(query,model_2,index_langchain ,all_chunks_langchain, top_k=5)\n",
    "\n",
    "    # Step 2: Build prompt\n",
    "    prompt = build_prompt_2(query, top_chunks)\n",
    "\n",
    "    # Step 3: Token count (optional)\n",
    "    # token_ids = llm.tokenize(prompt.encode(\"utf-8\"))\n",
    "    # print(f\"Token count for question: '{query[:50]}...':\", len(token_ids))\n",
    "\n",
    "    # Step 4: Run inference\n",
    "    response = llm(prompt, max_tokens=256, temperature=0.7, stop=[\"###\"])\n",
    "    answer = response[\"choices\"][0][\"text\"].strip()\n",
    "    # Step 5: Save updated QA\n",
    "    \n",
    "    mistral_qa_langchain.append({\n",
    "        \"question\": query,\n",
    "        \"answer\": answer\n",
    "    })\n",
    "    i+=1\n",
    "    if(i>50):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7baed0cd-2b9f-4907-8763-c9b5d74172ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_data = data[:51] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8211d5e4-73d9-40cc-be51-6a0d05cd5eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:05,  9.78it/s]                                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Average BLEU Score (1-gram): 0.2761\n",
      "🔹 Average Semantic Similarity: 0.7907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "assert len(subset_data) == len(mistral_qa_langchain), \"Mismatch in number of QA pairs\"\n",
    "\n",
    "bleu_scores = []\n",
    "similarity_scores = []\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "for gpt_item, my_item in tqdm.tqdm(zip(subset_data, mistral_qa_langchain), total=10):\n",
    "    reference = gpt_item['answer'].strip()\n",
    "    hypothesis = my_item['answer'].strip()\n",
    "\n",
    "    # BLEU\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    bleu = sentence_bleu([reference.split()], hypothesis.split(), weights=(1, 0, 0, 0), smoothing_function=smoothie)\n",
    "    bleu_scores.append(bleu)\n",
    "\n",
    "    # Semantic similarity\n",
    "    emb_ref = model.encode(reference, convert_to_tensor=True)\n",
    "    emb_hyp = model.encode(hypothesis, convert_to_tensor=True)\n",
    "    sim_score = util.pytorch_cos_sim(emb_ref, emb_hyp).item()\n",
    "    similarity_scores.append(sim_score)\n",
    "\n",
    "# Averages\n",
    "average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "average_similarity = sum(similarity_scores) / len(similarity_scores)\n",
    "\n",
    "print(f\"🔹 Average BLEU Score (1-gram): {average_bleu:.4f}\")\n",
    "print(f\"🔹 Average Semantic Similarity: {average_similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "27ce6ab5-6067-4d3e-afa0-0c0331531e84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mistral_qa_langchain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e389490c-f368-4c15-9e02-afc1f4f70645",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 51/51 [00:01<00:00, 48.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Average METEOR: 0.3231\n",
      "🔹 Average ROUGE-1: 0.4263\n",
      "🔹 Average ROUGE-2: 0.1692\n",
      "🔹 Average ROUGE-L: 0.2706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "meteor_scores = []\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougeL_scores = []\n",
    "\n",
    "for gpt, mine in tqdm.tqdm(zip(subset_data, mistral_qa_langchain), total=len(subset_data)):\n",
    "    ref = gpt[\"answer\"].strip()\n",
    "    hyp = mine[\"answer\"].strip()\n",
    "\n",
    "    # METEOR\n",
    "    meteor = single_meteor_score(word_tokenize(ref), word_tokenize(hyp))\n",
    "    meteor_scores.append(meteor)\n",
    "\n",
    "    # ROUGE\n",
    "    scores = rouge.score(ref, hyp)\n",
    "    rouge1_scores.append(scores[\"rouge1\"].fmeasure)\n",
    "    rouge2_scores.append(scores[\"rouge2\"].fmeasure)\n",
    "    rougeL_scores.append(scores[\"rougeL\"].fmeasure)\n",
    "\n",
    "print(\"🔹 Average METEOR:\", round(sum(meteor_scores) / len(meteor_scores), 4))\n",
    "print(\"🔹 Average ROUGE-1:\", round(sum(rouge1_scores) / len(rouge1_scores), 4))\n",
    "print(\"🔹 Average ROUGE-2:\", round(sum(rouge2_scores) / len(rouge2_scores), 4))\n",
    "print(\"🔹 Average ROUGE-L:\", round(sum(rougeL_scores) / len(rougeL_scores), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ff4fa352-ab7e-47fe-950d-7d1b4e1f3147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answers generated and saved to 'mistral_qa_langchain_51.json'.\n"
     ]
    }
   ],
   "source": [
    "with open(\"mistral_qa_langchain_51.json\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "    json.dump(mistral_qa_langchain, out_file, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Answers generated and saved to 'mistral_qa_langchain_51.json'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a5f922-bb17-4877-9b3e-6130bfccc0b9",
   "metadata": {},
   "source": [
    "This matches the performance of the Gemini 1.5 flash with the earlier index. But in this strategy the chunks are very small and consistent hence the latency of the LLM has improved. If the gemini is out of quota I will finalise this strategy for the final chatboat.\n",
    "Let's go for 50 QA to be sure and save some time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edd6c22-43bc-467c-bdb1-dda2516629a6",
   "metadata": {},
   "source": [
    "This is good, I will use this one as my final chatboat option."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2894ea-f1bb-4821-84cd-a785b8de9782",
   "metadata": {},
   "source": [
    "Let's use langchain to maintain the memory. The two ways we can maintain a memory into this,\n",
    "1. First is to provide few set of QA previously asked along with current question in the prompt. The problem with this is that it may fill the context and increase the latency\n",
    "2. Second one is to provide a summary of n previous conversations and append this in the prompt before the context and question and get the answer. This maybe better we will try both and retain the best one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9357371-e4bb-45ae-a949-aa0a6482ccf1",
   "metadata": {},
   "source": [
    "Few pairs of QA in Prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "57286405-6612-4cb1-ba0e-6d4e88fc84bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from langchain-community) (0.3.71)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from langchain-community) (0.3.26)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from langchain-community) (2.0.34)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from langchain-community) (6.0.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from langchain-community) (3.10.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from langchain-community) (8.2.3)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: langsmith>=0.1.125 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from langchain-community) (0.4.8)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.11.0)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (2.8.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (4.14.1)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (24.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (2.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (2.20.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.21.0)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain-community) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain-community) (2025.7.9)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from langsmith>=0.1.125->langchain-community) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from langsmith>=0.1.125->langchain-community) (3.11.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\shri\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\shri\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\shri\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\shri\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.14.0)\n",
      "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.5 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.5/2.5 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 0.8/2.5 MB 1.3 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 1.0/2.5 MB 1.2 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 1.3/2.5 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.6/2.5 MB 1.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.8/2.5 MB 1.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 2.1/2.5 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.4/2.5 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 MB 1.3 MB/s eta 0:00:00\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-inspection, typing-inspect, marshmallow, httpx-sse, dataclasses-json, pydantic-settings, langchain-community\n",
      "\n",
      "   ---------------------- ----------------- 4/7 [dataclasses-json]\n",
      "   ---------------------------- ----------- 5/7 [pydantic-settings]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------- ----- 6/7 [langchain-community]\n",
      "   ---------------------------------------- 7/7 [langchain-community]\n",
      "\n",
      "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.1 langchain-community-0.3.27 marshmallow-3.26.1 pydantic-settings-2.10.1 typing-inspect-0.9.0 typing-inspection-0.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85ef5790-6dd0-4431-b75a-6f66a9610f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from typing import List\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from pydantic import Field\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c119133b-f93d-4093-933a-fd56e5c962af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from C:\\Users\\shri\\Data_Science\\Text Mining\\mistral-7b-instruct-v0.1.Q2_K.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 10\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q2_K:   65 tensors\n",
      "llama_model_loader: - type q3_K:  160 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q2_K - Medium\n",
      "print_info: file size   = 2.87 GiB (3.41 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q2_K) (and 290 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  2939.57 MiB\n",
      "..................................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 8192\n",
      "llama_context: n_ctx_per_seq = 8192\n",
      "llama_context: n_batch       = 64\n",
      "llama_context: n_ubatch      = 8\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (8192) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.12 MiB\n",
      "create_memory: n_ctx = 8192 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified: layer  28: dev = CPU\n",
      "llama_kv_cache_unified: layer  29: dev = CPU\n",
      "llama_kv_cache_unified: layer  30: dev = CPU\n",
      "llama_kv_cache_unified: layer  31: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =  1024.00 MiB\n",
      "llama_kv_cache_unified: size = 1024.00 MiB (  8192 cells,  32 layers,  1 seqs), K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 8, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    8, n_seqs =  1, n_outputs =    8\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    8, n_seqs =  1, n_outputs =    8\n",
      "llama_context:        CPU compute buffer size =    10.50 MiB\n",
      "llama_context: graph nodes  = 1158\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '10', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "# mistral_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "# pipe = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-Instruct-v0.1\", device=0)\n",
    "\n",
    "mistral_llm = LlamaCpp(\n",
    "    model_path=r\"C:\\Users\\shri\\Data_Science\\Text Mining\\mistral-7b-instruct-v0.1.Q2_K.gguf\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=512,\n",
    "    top_p=1,\n",
    "    n_ctx=8192,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a97ff70c-a9b5-44d3-8da5-9d64bb7336e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaCpp(client=<llama_cpp.llama.Llama object at 0x000001AC0A8AA1E0>, model_path='C:\\\\Users\\\\shri\\\\Data_Science\\\\Text Mining\\\\mistral-7b-instruct-v0.1.Q2_K.gguf', n_ctx=8192, max_tokens=512, temperature=0.7, top_p=1.0, model_kwargs={})"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ec36e979-2f7f-4dc9-958e-51c8f6cae6c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shri\\AppData\\Local\\Temp\\ipykernel_18324\\1656931266.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n"
     ]
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "3eb7406b-be9a-4367-bb40-ddcb39475503",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shri\\anaconda3\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:404: UserWarning: <built-in function any> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "class CustomRetriever(BaseRetriever):\n",
    "    model: any = Field()\n",
    "    index: any = Field()\n",
    "    chunks: List[str] = Field()\n",
    "    top_k: int = Field(default=5)\n",
    "\n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        query_embedding = self.model.encode([query]).astype(\"float32\")\n",
    "\n",
    "        import faiss\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        D, I = self.index.search(query_embedding, self.top_k)\n",
    "\n",
    "        return [Document(page_content=self.chunks[i]) for i in I[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "1a6f9fda-9dc3-4952-bbda-7acd8fb64e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.load_local(\"chunk_index_langchain_2\", embeddings=embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "# Extract the FAISS index object (optional)\n",
    "faiss_index = vectorstore.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "2dbccfb8-e228-4357-964b-e8fccbeb2961",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "58c92803-2c17-4383-995f-20e79f16a2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferWindowMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    k=3  \n",
    ")\n",
    "\n",
    "my_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    ### Instruction:\n",
    "    Answer the question based only on the following context and chat history.\n",
    "\n",
    "    Chat History:\n",
    "    {chat_history}\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "\n",
    "    ### Response:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm= mistral_llm,\n",
    "    retriever = CustomRetriever(\n",
    "    model=embedding_model,\n",
    "    index=faiss_index,\n",
    "    chunks=all_chunks_langchain,\n",
    "    top_k=5\n",
    ")\n",
    ",\n",
    "    memory=memory,\n",
    "    combine_docs_chain_kwargs={\"prompt\": my_prompt}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "cdb45527-cd13-4bd7-9c0b-1c6a5c637919",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   55909.70 ms\n",
      "llama_perf_context_print: prompt eval time =   55908.41 ms /   490 tokens (  114.10 ms per token,     8.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =   39906.29 ms /   165 runs   (  241.86 ms per token,     4.13 tokens per second)\n",
      "llama_perf_context_print:       total time =   95991.68 ms /   655 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    The European Microfinance Network (EMN) and the Microfinance Centre (MFC) announced a collaborative initiative in April 2024, aimed at supporting the European microfinance sector. Their main objective is to capture data from the vast majority of European microfinance institutions, providing the most comprehensive dataset available on the sector today. This edition focuses on the types of businesses reached by microfinance and highlights the social performance of business loans, along with the impact measurement approaches adopted by MFIs. The report serves as an important policy tool, supporting evidence-based decision-making for policymakers working to strengthen financial inclusion and the social economy. It also functions as a benchmarking reference for MFIs, helping them evaluate their performance and position within the wider European landscape.\n"
     ]
    }
   ],
   "source": [
    "response = qa_chain.run(data[0]['question'])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "4f9ac97a-6c4c-4f53-87a5-7aab9d20c136",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 1 prefix-match hit, remaining 275 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   55909.70 ms\n",
      "llama_perf_context_print: prompt eval time =   67612.35 ms /   275 tokens (  245.86 ms per token,     4.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8524.91 ms /    19 runs   (  448.68 ms per token,     2.23 tokens per second)\n",
      "llama_perf_context_print:       total time =   76171.65 ms /   294 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 466 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   55909.70 ms\n",
      "llama_perf_context_print: prompt eval time =  122174.21 ms /   466 tokens (  262.18 ms per token,     3.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =   31189.98 ms /    68 runs   (  458.68 ms per token,     2.18 tokens per second)\n",
      "llama_perf_context_print:       total time =  153483.98 ms /   534 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The collaborative initiative by the European Microfinance Network (EMN) and the Microfinance Centre (MFC) aimed to capture data from a vast majority of European microfinance institutions. However, there is no information provided in the context or chat history on the types of businesses that were reached or how the data was collected.\n"
     ]
    }
   ],
   "source": [
    "next_response = qa_chain.run(\"What types of businesses were reached?\")\n",
    "print(next_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "4e738202-744f-45d6-a212-5fd0c18a5033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='What collaborative initiative was announced by the European Microfinance Network (EMN) and the Microfinance Centre (MFC) in April 2024, and what are its main objectives for supporting the European microfinance sector?', additional_kwargs={}, response_metadata={}), AIMessage(content='\\n    The European Microfinance Network (EMN) and the Microfinance Centre (MFC) announced a collaborative initiative in April 2024, aimed at supporting the European microfinance sector. Their main objective is to capture data from the vast majority of European microfinance institutions, providing the most comprehensive dataset available on the sector today. This edition focuses on the types of businesses reached by microfinance and highlights the social performance of business loans, along with the impact measurement approaches adopted by MFIs. The report serves as an important policy tool, supporting evidence-based decision-making for policymakers working to strengthen financial inclusion and the social economy. It also functions as a benchmarking reference for MFIs, helping them evaluate their performance and position within the wider European landscape.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What types of businesses were reached?', additional_kwargs={}, response_metadata={}), AIMessage(content=' The collaborative initiative by the European Microfinance Network (EMN) and the Microfinance Centre (MFC) aimed to capture data from a vast majority of European microfinance institutions. However, there is no information provided in the context or chat history on the types of businesses that were reached or how the data was collected.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "print(memory.chat_memory.messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "99482e2e-ff0f-49c6-af96-d0ce861b8172",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 1 prefix-match hit, remaining 364 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   55909.70 ms\n",
      "llama_perf_context_print: prompt eval time =   95024.38 ms /   364 tokens (  261.06 ms per token,     3.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5890.89 ms /    12 runs   (  490.91 ms per token,     2.04 tokens per second)\n",
      "llama_perf_context_print:       total time =  100932.92 ms /   376 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 814 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   55909.70 ms\n",
      "llama_perf_context_print: prompt eval time =  206059.69 ms /   814 tokens (  253.14 ms per token,     3.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =   11961.55 ms /    25 runs   (  478.46 ms per token,     2.09 tokens per second)\n",
      "llama_perf_context_print:       total time =  218062.72 ms /   839 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The full form of EMN is European Microfinance Network and the full form of MFC is Microfinance Centre.\n"
     ]
    }
   ],
   "source": [
    "next_response = qa_chain.run(\"What is the full form of EMN and MFC?\")\n",
    "print(next_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258bba0c-7a37-48ba-9637-5287ae8d1600",
   "metadata": {},
   "source": [
    "It seems working the chatboat it is giving the answers by considering the context and and previous QA pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433583e2-bde5-40ab-aeb7-99e400fe1f03",
   "metadata": {},
   "source": [
    "I will use this approach. Just for once I am trying to use a Q4 quantised version of this mistral 7B model which is documented to be faster than this Q2 quantised version of this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a096d69d-f6cc-4fe2-a587-f6401e41b0ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from C:\\Users\\shri\\Data_Science\\Text Mining\\mistral-7b-instruct-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V2\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 2 '</s>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 2 '</s>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 98 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "load_tensors:   CPU_REPACK model buffer size =  3204.00 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB\n",
      "repack: repack tensor blk.0.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.0.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.1.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.1.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.attn_v.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.24.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.24.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.27.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.27.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.28.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.28.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.28.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.28.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.28.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.29.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.29.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.30.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.30.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.30.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.30.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.30.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.31.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.31.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.31.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.31.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.31.ffn_up.weight with q4_K_8x8\n",
      "...................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 8192\n",
      "llama_context: n_ctx_per_seq = 8192\n",
      "llama_context: n_batch       = 64\n",
      "llama_context: n_ubatch      = 8\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (8192) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.12 MiB\n",
      "create_memory: n_ctx = 8192 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified: layer  28: dev = CPU\n",
      "llama_kv_cache_unified: layer  29: dev = CPU\n",
      "llama_kv_cache_unified: layer  30: dev = CPU\n",
      "llama_kv_cache_unified: layer  31: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =  1024.00 MiB\n",
      "llama_kv_cache_unified: size = 1024.00 MiB (  8192 cells,  32 layers,  1 seqs), K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 8, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    8, n_seqs =  1, n_outputs =    8\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    8, n_seqs =  1, n_outputs =    8\n",
      "llama_context:        CPU compute buffer size =    10.50 MiB\n",
      "llama_context: graph nodes  = 1158\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "mistral_Q4 = LlamaCpp(\n",
    "    model_path= r\"C:\\Users\\shri\\Data_Science\\Text Mining\\mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=512,\n",
    "    top_p=1,\n",
    "    n_ctx=8192,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "ab8f13b9-3f69-4b0b-b6bd-a16dee9beaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain_2 = ConversationalRetrievalChain.from_llm(\n",
    "    llm= mistral_Q4,\n",
    "    retriever = CustomRetriever(\n",
    "    model=embedding_model,\n",
    "    index=faiss_index,\n",
    "    chunks=all_chunks_langchain,\n",
    "    top_k=5\n",
    ")\n",
    ",\n",
    "    memory=memory,\n",
    "    combine_docs_chain_kwargs={\"prompt\": my_prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "01859705-c68d-4463-8ece-11aeab2ef6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 32 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   63002.41 ms\n",
      "llama_perf_context_print: prompt eval time =   66941.14 ms /   458 tokens (  146.16 ms per token,     6.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =   94425.34 ms /   197 runs   (  479.32 ms per token,     2.09 tokens per second)\n",
      "llama_perf_context_print:       total time =  161842.69 ms /   655 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    The European Microfinance Network (EMN) and the Microfinance Centre (MFC) announced a collaborative initiative in April 2024, which is the publication of the12th editionof their flagship publication:Microfinance in Europe: Survey Report. This long-standing survey remains the leading source of data and analysis on the microfinance sector in Europe.\n",
      "\n",
      "The main objectives of this collaborative initiative are to provide evidence-based decision-making for policymakers working to strengthen financial inclusion and the social economy, and to serve as a benchmarking reference for MFIs, helping them evaluate their performance and position within the wider European landscape. This edition focuses on the types of businesses reached by microfinance and highlights the social performance of business loans, along with the impact measurement approaches adopted by MFIs. It offers valuable insights into how these institutions contribute to social inclusion, entrepreneurship, and local development.\n"
     ]
    }
   ],
   "source": [
    "response = qa_chain_2.run(data[0]['question'])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "da46e8f3-a7a5-4a0c-bbe1-16dc557b01e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 1 prefix-match hit, remaining 307 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   63002.41 ms\n",
      "llama_perf_context_print: prompt eval time =   41197.35 ms /   307 tokens (  134.19 ms per token,     7.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =   15861.02 ms /    35 runs   (  453.17 ms per token,     2.21 tokens per second)\n",
      "llama_perf_context_print:       total time =   57120.99 ms /   342 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 730 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   63002.41 ms\n",
      "llama_perf_context_print: prompt eval time =  100660.39 ms /   730 tokens (  137.89 ms per token,     7.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =   64141.74 ms /   136 runs   (  471.63 ms per token,     2.12 tokens per second)\n",
      "llama_perf_context_print:       total time =  165099.07 ms /   866 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The 12th edition of the Microfinance in Europe: Survey Report highlights that the microfinance sector in Europe reaches a variety of businesses, including loans for the public sector, framework loans for the public sector, loans for the private sector, intermediated loans for SMEs, mid-caps and other priorities, microfinance equity, venture debt, investments in infrastructure and environmental funds, investments in SME and mid-cap funds, guarantees in support of SMEs, mid-caps and other objectives, advisory services, mandates and partnerships, InvestEU RRF and financial, credit enhancement for project finance, and guarantees.\n"
     ]
    }
   ],
   "source": [
    "next_response = qa_chain_2.run(\"What types of businesses were reached?\")\n",
    "print(next_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "2ce8f293-4fa8-4044-a664-a69f1bbc97ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 1 prefix-match hit, remaining 464 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   55909.70 ms\n",
      "llama_perf_context_print: prompt eval time =  125602.90 ms /   464 tokens (  270.70 ms per token,     3.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6484.48 ms /    12 runs   (  540.37 ms per token,     1.85 tokens per second)\n",
      "llama_perf_context_print:       total time =  132115.25 ms /   476 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 897 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   55909.70 ms\n",
      "llama_perf_context_print: prompt eval time =  248930.44 ms /   897 tokens (  277.51 ms per token,     3.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8899.32 ms /    19 runs   (  468.39 ms per token,     2.13 tokens per second)\n",
      "llama_perf_context_print:       total time =  257866.35 ms /   916 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The full names of EMN and MFC are not provided in the context or chat history.\n"
     ]
    }
   ],
   "source": [
    "next_response = qa_chain.run(\"What is the full form of EMN and MFC?\")\n",
    "print(next_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416362a1-3838-4639-bd86-ae34112b5849",
   "metadata": {},
   "source": [
    "okay lets be done with it. I will use the Q4 in the streamlit web app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed13f1ea-223e-44a1-a584-6182135273bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
